{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danianamir/RL_UNITY/blob/main/RL_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dependecies\n"
      ],
      "metadata": {
        "id": "Gr_kYEJIfRwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/danianamir/RL_UNITY.git\n",
        "%cd /content/RL_UNITY\n",
        "!git pull origin main"
      ],
      "metadata": {
        "id": "RWUQ_QfiUY9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWJBVGVq_RKR"
      },
      "outputs": [],
      "source": [
        "#package installs with pip_github......................................................................\n",
        "%cd /content/RL_UNITY/ml-agents-envs\n",
        "!pip install .\n",
        "!pip install gymnasium\n",
        "!pip install ray[rllib]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import.......................................................................................................\n",
        "#mlagnets\n",
        "import mlagents_envs\n",
        "from mlagents_envs.environment import UnityEnvironment\n",
        "from mlagents_envs.base_env import ActionTuple\n",
        "\n",
        "#gym\n",
        "import gymnasium as gym\n",
        "\n",
        "from gymnasium import spaces\n",
        "\n",
        "#ray[rllib]\n",
        "import ray\n",
        "from ray import air, tune\n",
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "\n",
        "from ray.rllib.algorithms.ppo.ppo import PPOConfig\n",
        "from ray.rllib.algorithms.ppo.ppo import PPO\n",
        "\n",
        "from ray.rllib.algorithms.apex_ddpg import ApexDDPGConfig\n",
        "from ray.rllib.algorithms.apex_ddpg import ApexDDPG\n",
        "\n",
        "\n",
        "from ray.rllib.examples.policy.random_policy import RandomPolicy\n",
        "\n",
        "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
        "from ray.tune.registry import register_env\n",
        "\n",
        "from ray.rllib.evaluation.rollout_worker import RolloutWorker\n",
        "from ray.rllib.evaluation.worker_set import WorkerSet\n",
        "from ray.rllib.policy.policy import PolicySpec\n",
        "\n",
        "\n",
        "#typing\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "#nympy\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import time\n",
        "import random\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "-k_yM6OAoJqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# give access to run the linux file ................................................................................\n",
        "%cd\n",
        "!chmod -R 755 /content/RL_UNITY/dn_server/dn_linux.x86_64\n",
        "!chmod -R 755 /content/RL_UNITY/dn_server/UnityPlayer.so\n",
        "!ls -l /content/RL_UNITY/dn_server/\n",
        "\n",
        "%cd\n",
        "!chmod -R 755 /content/RL_UNITY/3dball/dn_linux.x86_64\n",
        "!chmod -R 755 /content/RL_UNITY/3dball/UnityPlayer.so\n",
        "!ls -l //content/RL_UNITY/3dball/"
      ],
      "metadata": {
        "id": "ScnCEzjYoNdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train gym"
      ],
      "metadata": {
        "id": "fiDQjqBl8QOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create custom environmet for single agent setting\n",
        "class CustomEnv(gym.Env):\n",
        "\n",
        "  # first we initialize the type of action_space observation_space\n",
        "  def __init__(self):\n",
        "    #if you want to have 3 action and each of them have 5 diffrent value [0 1 2 3 4]\n",
        "    #self.action_space = MultiDiscrete([5, 5, 5])\n",
        "    self.action_space =spaces.Discrete(2)\n",
        "    self.observation_space =spaces.Box(low=-100, high=100, shape=(1,))\n",
        "    self.state = 0\n",
        "\n",
        "  # we make the transition function that change the state for given action\n",
        "  def step(self, action):\n",
        "\n",
        "    if action == 0:\n",
        "      self.state += 1\n",
        "    elif action == 1:\n",
        "      self.state -= 1\n",
        "\n",
        "\n",
        "    if self.state==10 :\n",
        "      reward =1\n",
        "    else:\n",
        "      reward=0\n",
        "\n",
        "    done = (self.state >= 10 or self.state <= -10)\n",
        "    info = {}\n",
        "    truncated=False\n",
        "\n",
        "    return np.array([self.state]), reward, done,truncated, info\n",
        "\n",
        "  #reset the episod\n",
        "  def reset(self, *, seed=None, options=None):\n",
        "    self.state = 0\n",
        "    info={}\n",
        "    return np.array([self.state]) ,info\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5zKdMfoM8Wk6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# register the env\n",
        "tune.register_env(\"my\",lambda _: CustomEnv())"
      ],
      "metadata": {
        "id": "j0-hDUNeSuzn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# set config\n",
        "config=(\n",
        "       PPOConfig()\n",
        "      .environment(\"my\")\n",
        "      .framework(\"tf\")\n",
        "      .rollouts(num_rollout_workers=1,\n",
        "                rollout_fragment_length=200,\n",
        "                )\n",
        "      .training(\n",
        "            lr=0.0003,\n",
        "            lambda_=0.95,\n",
        "            gamma=0.99,\n",
        "            sgd_minibatch_size=256,\n",
        "            train_batch_size=4000,\n",
        "            num_sgd_iter=20,\n",
        "            clip_param=0.2,\n",
        "            model={\"fcnet_hiddens\": [256, 256]},\n",
        "            _enable_learner_api=False\n",
        "            )\n",
        "      .rl_module(_enable_rl_module_api=False)\n",
        "      .checkpointing(export_native_model_files=True)\n",
        "      .resources()\n",
        ")\n",
        "\n",
        "\n",
        "#print the config\n",
        "pprint(config.to_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLm-G1on9TG5",
        "outputId": "6f3814c6-73fa-4e71-84f1-6c1e71955d78"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-09-03 14:12:09,300\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "2023-09-03 14:12:09,306\tWARNING algorithm_config.py:2572 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'_AlgorithmConfig__prior_exploration_config': None,\n",
            " '_disable_action_flattening': False,\n",
            " '_disable_execution_plan_api': True,\n",
            " '_disable_initialize_loss_from_dummy_batch': False,\n",
            " '_disable_preprocessor_api': False,\n",
            " '_enable_learner_api': False,\n",
            " '_enable_rl_module_api': False,\n",
            " '_fake_gpus': False,\n",
            " '_is_atari': None,\n",
            " '_learner_class': None,\n",
            " '_tf_policy_handles_more_than_one_loss': False,\n",
            " 'action_mask_key': 'action_mask',\n",
            " 'action_space': None,\n",
            " 'actions_in_input_normalized': False,\n",
            " 'algorithm_config_overrides_per_module': {},\n",
            " 'always_attach_evaluation_results': False,\n",
            " 'auto_wrap_old_gym_envs': True,\n",
            " 'batch_mode': 'truncate_episodes',\n",
            " 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>,\n",
            " 'checkpoint_trainable_policies_only': False,\n",
            " 'clip_actions': False,\n",
            " 'clip_param': 0.2,\n",
            " 'clip_rewards': None,\n",
            " 'compress_observations': False,\n",
            " 'count_steps_by': 'env_steps',\n",
            " 'create_env_on_driver': False,\n",
            " 'custom_eval_function': None,\n",
            " 'custom_resources_per_worker': {},\n",
            " 'delay_between_worker_restarts_s': 60.0,\n",
            " 'disable_env_checking': False,\n",
            " 'eager_max_retraces': 20,\n",
            " 'eager_tracing': True,\n",
            " 'enable_async_evaluation': False,\n",
            " 'enable_connectors': True,\n",
            " 'enable_tf1_exec_eagerly': False,\n",
            " 'entropy_coeff': 0.0,\n",
            " 'entropy_coeff_schedule': None,\n",
            " 'env': 'my',\n",
            " 'env_config': {},\n",
            " 'env_runner_cls': None,\n",
            " 'env_task_fn': None,\n",
            " 'evaluation_config': None,\n",
            " 'evaluation_duration': 10,\n",
            " 'evaluation_duration_unit': 'episodes',\n",
            " 'evaluation_interval': None,\n",
            " 'evaluation_num_workers': 0,\n",
            " 'evaluation_parallel_to_training': False,\n",
            " 'evaluation_sample_timeout_s': 180.0,\n",
            " 'exploration_config': {'type': 'StochasticSampling'},\n",
            " 'explore': True,\n",
            " 'export_native_model_files': True,\n",
            " 'extra_python_environs_for_driver': {},\n",
            " 'extra_python_environs_for_worker': {},\n",
            " 'fake_sampler': False,\n",
            " 'framework': 'tf',\n",
            " 'gamma': 0.99,\n",
            " 'grad_clip': None,\n",
            " 'grad_clip_by': 'global_norm',\n",
            " 'ignore_worker_failures': False,\n",
            " 'in_evaluation': False,\n",
            " 'input': 'sampler',\n",
            " 'input_config': {},\n",
            " 'keep_per_episode_custom_metrics': False,\n",
            " 'kl_coeff': 0.2,\n",
            " 'kl_target': 0.01,\n",
            " 'lambda': 0.95,\n",
            " 'local_gpu_idx': 0,\n",
            " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
            "                           'intra_op_parallelism_threads': 8},\n",
            " 'log_level': 'WARN',\n",
            " 'log_sys_usage': True,\n",
            " 'logger_config': None,\n",
            " 'logger_creator': None,\n",
            " 'lr': 0.0003,\n",
            " 'lr_schedule': None,\n",
            " 'max_num_worker_restarts': 1000,\n",
            " 'max_requests_in_flight_per_sampler_worker': 2,\n",
            " 'metrics_episode_collection_timeout_s': 60.0,\n",
            " 'metrics_num_episodes_for_smoothing': 100,\n",
            " 'min_sample_timesteps_per_iteration': 0,\n",
            " 'min_time_s_per_iteration': None,\n",
            " 'min_train_timesteps_per_iteration': 0,\n",
            " 'model': {'_disable_action_flattening': False,\n",
            "           '_disable_preprocessor_api': False,\n",
            "           '_time_major': False,\n",
            "           '_use_default_native_models': -1,\n",
            "           'always_check_shapes': False,\n",
            "           'attention_dim': 64,\n",
            "           'attention_head_dim': 32,\n",
            "           'attention_init_gru_gate_bias': 2.0,\n",
            "           'attention_memory_inference': 50,\n",
            "           'attention_memory_training': 50,\n",
            "           'attention_num_heads': 1,\n",
            "           'attention_num_transformer_units': 1,\n",
            "           'attention_position_wise_mlp_dim': 32,\n",
            "           'attention_use_n_prev_actions': 0,\n",
            "           'attention_use_n_prev_rewards': 0,\n",
            "           'conv_activation': 'relu',\n",
            "           'conv_filters': None,\n",
            "           'custom_action_dist': None,\n",
            "           'custom_model': None,\n",
            "           'custom_model_config': {},\n",
            "           'custom_preprocessor': None,\n",
            "           'dim': 84,\n",
            "           'encoder_latent_dim': None,\n",
            "           'fcnet_activation': 'tanh',\n",
            "           'fcnet_hiddens': [256, 256],\n",
            "           'framestack': True,\n",
            "           'free_log_std': False,\n",
            "           'grayscale': False,\n",
            "           'lstm_cell_size': 256,\n",
            "           'lstm_use_prev_action': False,\n",
            "           'lstm_use_prev_action_reward': -1,\n",
            "           'lstm_use_prev_reward': False,\n",
            "           'max_seq_len': 20,\n",
            "           'no_final_linear': False,\n",
            "           'post_fcnet_activation': 'relu',\n",
            "           'post_fcnet_hiddens': [],\n",
            "           'use_attention': False,\n",
            "           'use_lstm': False,\n",
            "           'vf_share_layers': False,\n",
            "           'zero_mean': True},\n",
            " 'normalize_actions': True,\n",
            " 'num_consecutive_worker_failures_tolerance': 100,\n",
            " 'num_cpus_for_driver': 1,\n",
            " 'num_cpus_per_learner_worker': 1,\n",
            " 'num_cpus_per_worker': 1,\n",
            " 'num_envs_per_worker': 1,\n",
            " 'num_gpus': 0,\n",
            " 'num_gpus_per_learner_worker': 0,\n",
            " 'num_gpus_per_worker': 0,\n",
            " 'num_learner_workers': 0,\n",
            " 'num_sgd_iter': 20,\n",
            " 'num_workers': 1,\n",
            " 'observation_filter': 'NoFilter',\n",
            " 'observation_fn': None,\n",
            " 'observation_space': None,\n",
            " 'off_policy_estimation_methods': {},\n",
            " 'offline_sampling': False,\n",
            " 'ope_split_batch_by_episode': True,\n",
            " 'optimizer': {},\n",
            " 'output': None,\n",
            " 'output_compress_columns': ['obs', 'new_obs'],\n",
            " 'output_config': {},\n",
            " 'output_max_file_size': 67108864,\n",
            " 'placement_strategy': 'PACK',\n",
            " 'policies': {'default_policy': (None, None, None, None)},\n",
            " 'policies_to_train': None,\n",
            " 'policy_map_cache': -1,\n",
            " 'policy_map_capacity': 100,\n",
            " 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x78f59bc564d0>,\n",
            " 'policy_states_are_swappable': False,\n",
            " 'postprocess_inputs': False,\n",
            " 'preprocessor_pref': 'deepmind',\n",
            " 'recreate_failed_workers': False,\n",
            " 'remote_env_batch_wait_ms': 0,\n",
            " 'remote_worker_envs': False,\n",
            " 'render_env': False,\n",
            " 'replay_sequence_length': None,\n",
            " 'restart_failed_sub_environments': False,\n",
            " 'rl_module_spec': None,\n",
            " 'rollout_fragment_length': 200,\n",
            " 'sample_async': False,\n",
            " 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
            " 'sampler_perf_stats_ema_coef': None,\n",
            " 'seed': None,\n",
            " 'sgd_minibatch_size': 256,\n",
            " 'shuffle_buffer_size': 0,\n",
            " 'shuffle_sequences': True,\n",
            " 'simple_optimizer': -1,\n",
            " 'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
            " 'synchronize_filters': -1,\n",
            " 'tf_session_args': {'allow_soft_placement': True,\n",
            "                     'device_count': {'CPU': 1},\n",
            "                     'gpu_options': {'allow_growth': True},\n",
            "                     'inter_op_parallelism_threads': 2,\n",
            "                     'intra_op_parallelism_threads': 2,\n",
            "                     'log_device_placement': False},\n",
            " 'torch_compile_learner': False,\n",
            " 'torch_compile_learner_dynamo_backend': 'inductor',\n",
            " 'torch_compile_learner_dynamo_mode': None,\n",
            " 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
            " 'torch_compile_worker': False,\n",
            " 'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
            " 'torch_compile_worker_dynamo_mode': None,\n",
            " 'train_batch_size': 4000,\n",
            " 'update_worker_filter_stats': True,\n",
            " 'use_critic': True,\n",
            " 'use_gae': True,\n",
            " 'use_kl_loss': True,\n",
            " 'use_worker_filter_stats': True,\n",
            " 'validate_workers_after_construction': True,\n",
            " 'vf_clip_param': 10.0,\n",
            " 'vf_loss_coeff': 1.0,\n",
            " 'vf_share_layers': -1,\n",
            " 'worker_cls': -1,\n",
            " 'worker_health_probe_timeout_s': 60,\n",
            " 'worker_restore_timeout_s': 1800}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build algo\n",
        "algo=config.build()\n"
      ],
      "metadata": {
        "id": "dlZ6h_WNSrZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# train algo and print result\n",
        "results=[]\n",
        "for _ in range(6):\n",
        "            result = algo.train()\n",
        "            pprint(result)\n",
        "            results.append(result)\n",
        "            print(\"****************************************************************************************************************************************\")\n",
        "            if (\n",
        "                result['timesteps_total'] >= 100000\n",
        "                or result[\"episode_reward_mean\"] >= 100000\n",
        "                or result[\"episodes_total\"]>=10000\n",
        "            ):\n",
        "                break\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dapIN2bDG9UF",
        "outputId": "91f0b149-9472-4702-9f8b-67112290e7dd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'agent_timesteps_total': 8000,\n",
            " 'config': {'_AlgorithmConfig__prior_exploration_config': None,\n",
            "            '_disable_action_flattening': False,\n",
            "            '_disable_execution_plan_api': True,\n",
            "            '_disable_initialize_loss_from_dummy_batch': False,\n",
            "            '_disable_preprocessor_api': False,\n",
            "            '_enable_learner_api': False,\n",
            "            '_enable_rl_module_api': False,\n",
            "            '_fake_gpus': False,\n",
            "            '_is_atari': None,\n",
            "            '_learner_class': None,\n",
            "            '_tf_policy_handles_more_than_one_loss': False,\n",
            "            'action_mask_key': 'action_mask',\n",
            "            'action_space': None,\n",
            "            'actions_in_input_normalized': False,\n",
            "            'algorithm_config_overrides_per_module': {},\n",
            "            'always_attach_evaluation_results': False,\n",
            "            'auto_wrap_old_gym_envs': True,\n",
            "            'batch_mode': 'truncate_episodes',\n",
            "            'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>,\n",
            "            'checkpoint_trainable_policies_only': False,\n",
            "            'clip_actions': False,\n",
            "            'clip_param': 0.2,\n",
            "            'clip_rewards': None,\n",
            "            'compress_observations': False,\n",
            "            'count_steps_by': 'env_steps',\n",
            "            'create_env_on_driver': False,\n",
            "            'custom_eval_function': None,\n",
            "            'custom_resources_per_worker': {},\n",
            "            'delay_between_worker_restarts_s': 60.0,\n",
            "            'disable_env_checking': False,\n",
            "            'eager_max_retraces': 20,\n",
            "            'eager_tracing': True,\n",
            "            'enable_async_evaluation': False,\n",
            "            'enable_connectors': True,\n",
            "            'enable_tf1_exec_eagerly': False,\n",
            "            'entropy_coeff': 0.0,\n",
            "            'entropy_coeff_schedule': None,\n",
            "            'env': 'my',\n",
            "            'env_config': {},\n",
            "            'env_runner_cls': None,\n",
            "            'env_task_fn': None,\n",
            "            'evaluation_config': None,\n",
            "            'evaluation_duration': 10,\n",
            "            'evaluation_duration_unit': 'episodes',\n",
            "            'evaluation_interval': None,\n",
            "            'evaluation_num_workers': 0,\n",
            "            'evaluation_parallel_to_training': False,\n",
            "            'evaluation_sample_timeout_s': 180.0,\n",
            "            'exploration_config': {'type': 'StochasticSampling'},\n",
            "            'explore': True,\n",
            "            'export_native_model_files': True,\n",
            "            'extra_python_environs_for_driver': {},\n",
            "            'extra_python_environs_for_worker': {},\n",
            "            'fake_sampler': False,\n",
            "            'framework': 'torch',\n",
            "            'gamma': 0.99,\n",
            "            'grad_clip': None,\n",
            "            'grad_clip_by': 'global_norm',\n",
            "            'ignore_worker_failures': False,\n",
            "            'in_evaluation': False,\n",
            "            'input': 'sampler',\n",
            "            'input_config': {},\n",
            "            'keep_per_episode_custom_metrics': False,\n",
            "            'kl_coeff': 0.2,\n",
            "            'kl_target': 0.01,\n",
            "            'lambda': 0.95,\n",
            "            'local_gpu_idx': 0,\n",
            "            'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
            "                                      'intra_op_parallelism_threads': 8},\n",
            "            'log_level': 'WARN',\n",
            "            'log_sys_usage': True,\n",
            "            'logger_config': None,\n",
            "            'logger_creator': None,\n",
            "            'lr': 0.0003,\n",
            "            'lr_schedule': None,\n",
            "            'max_num_worker_restarts': 1000,\n",
            "            'max_requests_in_flight_per_sampler_worker': 2,\n",
            "            'metrics_episode_collection_timeout_s': 60.0,\n",
            "            'metrics_num_episodes_for_smoothing': 100,\n",
            "            'min_sample_timesteps_per_iteration': 0,\n",
            "            'min_time_s_per_iteration': None,\n",
            "            'min_train_timesteps_per_iteration': 0,\n",
            "            'model': {'_disable_action_flattening': False,\n",
            "                      '_disable_preprocessor_api': False,\n",
            "                      '_time_major': False,\n",
            "                      '_use_default_native_models': -1,\n",
            "                      'always_check_shapes': False,\n",
            "                      'attention_dim': 64,\n",
            "                      'attention_head_dim': 32,\n",
            "                      'attention_init_gru_gate_bias': 2.0,\n",
            "                      'attention_memory_inference': 50,\n",
            "                      'attention_memory_training': 50,\n",
            "                      'attention_num_heads': 1,\n",
            "                      'attention_num_transformer_units': 1,\n",
            "                      'attention_position_wise_mlp_dim': 32,\n",
            "                      'attention_use_n_prev_actions': 0,\n",
            "                      'attention_use_n_prev_rewards': 0,\n",
            "                      'conv_activation': 'relu',\n",
            "                      'conv_filters': None,\n",
            "                      'custom_action_dist': None,\n",
            "                      'custom_model': None,\n",
            "                      'custom_model_config': {},\n",
            "                      'custom_preprocessor': None,\n",
            "                      'dim': 84,\n",
            "                      'encoder_latent_dim': None,\n",
            "                      'fcnet_activation': 'tanh',\n",
            "                      'fcnet_hiddens': [256, 256],\n",
            "                      'framestack': True,\n",
            "                      'free_log_std': False,\n",
            "                      'grayscale': False,\n",
            "                      'lstm_cell_size': 256,\n",
            "                      'lstm_use_prev_action': False,\n",
            "                      'lstm_use_prev_action_reward': -1,\n",
            "                      'lstm_use_prev_reward': False,\n",
            "                      'max_seq_len': 20,\n",
            "                      'no_final_linear': False,\n",
            "                      'post_fcnet_activation': 'relu',\n",
            "                      'post_fcnet_hiddens': [],\n",
            "                      'use_attention': False,\n",
            "                      'use_lstm': False,\n",
            "                      'vf_share_layers': False,\n",
            "                      'zero_mean': True},\n",
            "            'normalize_actions': True,\n",
            "            'num_consecutive_worker_failures_tolerance': 100,\n",
            "            'num_cpus_for_driver': 1,\n",
            "            'num_cpus_per_learner_worker': 1,\n",
            "            'num_cpus_per_worker': 1,\n",
            "            'num_envs_per_worker': 1,\n",
            "            'num_gpus': 0,\n",
            "            'num_gpus_per_learner_worker': 0,\n",
            "            'num_gpus_per_worker': 0,\n",
            "            'num_learner_workers': 0,\n",
            "            'num_sgd_iter': 20,\n",
            "            'num_workers': 1,\n",
            "            'observation_filter': 'NoFilter',\n",
            "            'observation_fn': None,\n",
            "            'observation_space': None,\n",
            "            'off_policy_estimation_methods': {},\n",
            "            'offline_sampling': False,\n",
            "            'ope_split_batch_by_episode': True,\n",
            "            'optimizer': {},\n",
            "            'output': None,\n",
            "            'output_compress_columns': ['obs', 'new_obs'],\n",
            "            'output_config': {},\n",
            "            'output_max_file_size': 67108864,\n",
            "            'placement_strategy': 'PACK',\n",
            "            'policies': {'default_policy': (None, None, None, None)},\n",
            "            'policies_to_train': None,\n",
            "            'policy_map_cache': -1,\n",
            "            'policy_map_capacity': 100,\n",
            "            'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x78f59bc564d0>,\n",
            "            'policy_states_are_swappable': False,\n",
            "            'postprocess_inputs': False,\n",
            "            'preprocessor_pref': 'deepmind',\n",
            "            'recreate_failed_workers': False,\n",
            "            'remote_env_batch_wait_ms': 0,\n",
            "            'remote_worker_envs': False,\n",
            "            'render_env': False,\n",
            "            'replay_sequence_length': None,\n",
            "            'restart_failed_sub_environments': False,\n",
            "            'rl_module_spec': None,\n",
            "            'rollout_fragment_length': 200,\n",
            "            'sample_async': False,\n",
            "            'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
            "            'sampler_perf_stats_ema_coef': None,\n",
            "            'seed': None,\n",
            "            'sgd_minibatch_size': 256,\n",
            "            'shuffle_buffer_size': 0,\n",
            "            'shuffle_sequences': True,\n",
            "            'simple_optimizer': False,\n",
            "            'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
            "            'synchronize_filters': -1,\n",
            "            'tf_session_args': {'allow_soft_placement': True,\n",
            "                                'device_count': {'CPU': 1},\n",
            "                                'gpu_options': {'allow_growth': True},\n",
            "                                'inter_op_parallelism_threads': 2,\n",
            "                                'intra_op_parallelism_threads': 2,\n",
            "                                'log_device_placement': False},\n",
            "            'torch_compile_learner': False,\n",
            "            'torch_compile_learner_dynamo_backend': 'inductor',\n",
            "            'torch_compile_learner_dynamo_mode': None,\n",
            "            'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
            "            'torch_compile_worker': False,\n",
            "            'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
            "            'torch_compile_worker_dynamo_mode': None,\n",
            "            'train_batch_size': 4000,\n",
            "            'update_worker_filter_stats': True,\n",
            "            'use_critic': True,\n",
            "            'use_gae': True,\n",
            "            'use_kl_loss': True,\n",
            "            'use_worker_filter_stats': True,\n",
            "            'validate_workers_after_construction': True,\n",
            "            'vf_clip_param': 10.0,\n",
            "            'vf_loss_coeff': 1.0,\n",
            "            'vf_share_layers': -1,\n",
            "            'worker_cls': -1,\n",
            "            'worker_health_probe_timeout_s': 60,\n",
            "            'worker_restore_timeout_s': 1800},\n",
            " 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.01079330638963349,\n",
            "                       'StateBufferConnector_ms': 0.00696766133211097,\n",
            "                       'ViewRequirementAgentConnector_ms': 0.20772389003208705},\n",
            " 'counters': {'num_agent_steps_sampled': 8000,\n",
            "              'num_agent_steps_trained': 8000,\n",
            "              'num_env_steps_sampled': 8000,\n",
            "              'num_env_steps_trained': 8000},\n",
            " 'custom_metrics': {},\n",
            " 'date': '2023-09-03_14-04-06',\n",
            " 'done': False,\n",
            " 'episode_len_mean': 80.59183673469387,\n",
            " 'episode_media': {},\n",
            " 'episode_reward_max': 107.80000000000004,\n",
            " 'episode_reward_mean': 13.977551020408166,\n",
            " 'episode_reward_min': -70.0,\n",
            " 'episodes_this_iter': 72,\n",
            " 'episodes_total': 98,\n",
            " 'hist_stats': {'episode_lengths': [73,\n",
            "                                    189,\n",
            "                                    29,\n",
            "                                    599,\n",
            "                                    207,\n",
            "                                    41,\n",
            "                                    409,\n",
            "                                    139,\n",
            "                                    111,\n",
            "                                    43,\n",
            "                                    151,\n",
            "                                    37,\n",
            "                                    19,\n",
            "                                    155,\n",
            "                                    181,\n",
            "                                    225,\n",
            "                                    31,\n",
            "                                    269,\n",
            "                                    117,\n",
            "                                    195,\n",
            "                                    171,\n",
            "                                    187,\n",
            "                                    173,\n",
            "                                    65,\n",
            "                                    77,\n",
            "                                    51,\n",
            "                                    125,\n",
            "                                    57,\n",
            "                                    61,\n",
            "                                    65,\n",
            "                                    47,\n",
            "                                    83,\n",
            "                                    33,\n",
            "                                    101,\n",
            "                                    29,\n",
            "                                    19,\n",
            "                                    47,\n",
            "                                    39,\n",
            "                                    95,\n",
            "                                    101,\n",
            "                                    63,\n",
            "                                    81,\n",
            "                                    37,\n",
            "                                    31,\n",
            "                                    29,\n",
            "                                    49,\n",
            "                                    27,\n",
            "                                    15,\n",
            "                                    43,\n",
            "                                    19,\n",
            "                                    39,\n",
            "                                    59,\n",
            "                                    21,\n",
            "                                    19,\n",
            "                                    37,\n",
            "                                    45,\n",
            "                                    45,\n",
            "                                    115,\n",
            "                                    53,\n",
            "                                    61,\n",
            "                                    55,\n",
            "                                    47,\n",
            "                                    63,\n",
            "                                    27,\n",
            "                                    39,\n",
            "                                    121,\n",
            "                                    23,\n",
            "                                    43,\n",
            "                                    113,\n",
            "                                    31,\n",
            "                                    43,\n",
            "                                    41,\n",
            "                                    39,\n",
            "                                    103,\n",
            "                                    29,\n",
            "                                    15,\n",
            "                                    43,\n",
            "                                    27,\n",
            "                                    15,\n",
            "                                    41,\n",
            "                                    107,\n",
            "                                    23,\n",
            "                                    89,\n",
            "                                    75,\n",
            "                                    31,\n",
            "                                    51,\n",
            "                                    35,\n",
            "                                    125,\n",
            "                                    45,\n",
            "                                    103,\n",
            "                                    63,\n",
            "                                    55,\n",
            "                                    73,\n",
            "                                    57,\n",
            "                                    19,\n",
            "                                    167,\n",
            "                                    41,\n",
            "                                    47],\n",
            "                'episode_reward': [-5.7,\n",
            "                                   17.1,\n",
            "                                   -15.1,\n",
            "                                   107.80000000000004,\n",
            "                                   -70.0,\n",
            "                                   -16.700000000000003,\n",
            "                                   -23.100000000000023,\n",
            "                                   -14.400000000000006,\n",
            "                                   13.6,\n",
            "                                   12.2,\n",
            "                                   -42.19999999999998,\n",
            "                                   -17.900000000000002,\n",
            "                                   -7.799999999999999,\n",
            "                                   -61.79999999999999,\n",
            "                                   -29.09999999999993,\n",
            "                                   30.699999999999996,\n",
            "                                   -14.200000000000001,\n",
            "                                   -15.300000000000034,\n",
            "                                   26.699999999999967,\n",
            "                                   -30.799999999999997,\n",
            "                                   51.6,\n",
            "                                   -29.0,\n",
            "                                   -25.70000000000001,\n",
            "                                   -34.9,\n",
            "                                   -45.49999999999999,\n",
            "                                   22.400000000000002,\n",
            "                                   46.499999999999986,\n",
            "                                   26.1,\n",
            "                                   9.299999999999999,\n",
            "                                   34.9,\n",
            "                                   26.800000000000004,\n",
            "                                   30.8,\n",
            "                                   12.5,\n",
            "                                   14.899999999999997,\n",
            "                                   16.1,\n",
            "                                   8.0,\n",
            "                                   15.600000000000001,\n",
            "                                   21.8,\n",
            "                                   40.0,\n",
            "                                   19.100000000000005,\n",
            "                                   26.800000000000004,\n",
            "                                   34.300000000000004,\n",
            "                                   15.700000000000003,\n",
            "                                   18.8,\n",
            "                                   16.500000000000004,\n",
            "                                   26.900000000000002,\n",
            "                                   13.200000000000001,\n",
            "                                   7.799999999999999,\n",
            "                                   23.199999999999996,\n",
            "                                   11.6,\n",
            "                                   14.6,\n",
            "                                   30.0,\n",
            "                                   9.899999999999999,\n",
            "                                   9.0,\n",
            "                                   21.9,\n",
            "                                   16.1,\n",
            "                                   27.700000000000003,\n",
            "                                   34.800000000000004,\n",
            "                                   25.5,\n",
            "                                   12.499999999999996,\n",
            "                                   36.199999999999996,\n",
            "                                   14.799999999999997,\n",
            "                                   30.4,\n",
            "                                   9.0,\n",
            "                                   21.2,\n",
            "                                   69.50000000000004,\n",
            "                                   10.4,\n",
            "                                   20.4,\n",
            "                                   47.3,\n",
            "                                   10.599999999999998,\n",
            "                                   8.2,\n",
            "                                   11.900000000000002,\n",
            "                                   17.800000000000004,\n",
            "                                   6.6,\n",
            "                                   18.099999999999998,\n",
            "                                   8.8,\n",
            "                                   8.8,\n",
            "                                   13.4,\n",
            "                                   9.0,\n",
            "                                   22.7,\n",
            "                                   67.60000000000001,\n",
            "                                   9.6,\n",
            "                                   40.5,\n",
            "                                   38.6,\n",
            "                                   17.400000000000002,\n",
            "                                   8.4,\n",
            "                                   14.200000000000001,\n",
            "                                   17.5,\n",
            "                                   28.9,\n",
            "                                   51.600000000000016,\n",
            "                                   18.000000000000004,\n",
            "                                   7.399999999999999,\n",
            "                                   16.5,\n",
            "                                   35.3,\n",
            "                                   10.999999999999998,\n",
            "                                   65.80000000000001,\n",
            "                                   12.900000000000002,\n",
            "                                   11.4]},\n",
            " 'hostname': '9e348d898222',\n",
            " 'info': {'learner': {'default_policy': {'custom_metrics': {},\n",
            "                                         'diff_num_grad_updates_vs_sampler_policy': 149.5,\n",
            "                                         'learner_stats': {'allreduce_latency': 0.0,\n",
            "                                                           'cur_kl_coeff': 0.19999999999999998,\n",
            "                                                           'cur_lr': 0.0003,\n",
            "                                                           'entropy': 0.650128623843193,\n",
            "                                                           'entropy_coeff': 0.0,\n",
            "                                                           'grad_gnorm': 5.336692201197147,\n",
            "                                                           'kl': 0.016329921516253308,\n",
            "                                                           'policy_loss': -0.045187165585036076,\n",
            "                                                           'total_loss': 6.491514069239298,\n",
            "                                                           'vf_explained_var': -0.3137036728858948,\n",
            "                                                           'vf_loss': 6.533435306549072},\n",
            "                                         'model': {},\n",
            "                                         'num_agent_steps_trained': 256.0,\n",
            "                                         'num_grad_updates_lifetime': 450.5}},\n",
            "          'num_agent_steps_sampled': 8000,\n",
            "          'num_agent_steps_trained': 8000,\n",
            "          'num_env_steps_sampled': 8000,\n",
            "          'num_env_steps_trained': 8000},\n",
            " 'iterations_since_restore': 2,\n",
            " 'node_ip': '172.28.0.12',\n",
            " 'num_agent_steps_sampled': 8000,\n",
            " 'num_agent_steps_trained': 8000,\n",
            " 'num_env_steps_sampled': 8000,\n",
            " 'num_env_steps_sampled_this_iter': 4000,\n",
            " 'num_env_steps_sampled_throughput_per_sec': 263.6190018630334,\n",
            " 'num_env_steps_trained': 8000,\n",
            " 'num_env_steps_trained_this_iter': 4000,\n",
            " 'num_env_steps_trained_throughput_per_sec': 263.6190018630334,\n",
            " 'num_faulty_episodes': 0,\n",
            " 'num_healthy_workers': 1,\n",
            " 'num_in_flight_async_reqs': 0,\n",
            " 'num_remote_worker_restarts': 0,\n",
            " 'num_steps_trained_this_iter': 4000,\n",
            " 'perf': {'cpu_util_percent': 28.23504273504274,\n",
            "          'ram_util_percent': 18.595299145299148},\n",
            " 'pid': 401,\n",
            " 'policy_reward_max': {},\n",
            " 'policy_reward_mean': {},\n",
            " 'policy_reward_min': {},\n",
            " 'sampler_perf': {'mean_action_processing_ms': 0.22207926718029788,\n",
            "                  'mean_env_render_ms': 0.0,\n",
            "                  'mean_env_wait_ms': 0.06620356446233085,\n",
            "                  'mean_inference_ms': 1.820606127523691,\n",
            "                  'mean_raw_obs_processing_ms': 0.5427654063093263},\n",
            " 'sampler_results': {'connector_metrics': {'ObsPreprocessorConnector_ms': 0.01079330638963349,\n",
            "                                           'StateBufferConnector_ms': 0.00696766133211097,\n",
            "                                           'ViewRequirementAgentConnector_ms': 0.20772389003208705},\n",
            "                     'custom_metrics': {},\n",
            "                     'episode_len_mean': 80.59183673469387,\n",
            "                     'episode_media': {},\n",
            "                     'episode_reward_max': 107.80000000000004,\n",
            "                     'episode_reward_mean': 13.977551020408166,\n",
            "                     'episode_reward_min': -70.0,\n",
            "                     'episodes_this_iter': 72,\n",
            "                     'hist_stats': {'episode_lengths': [73,\n",
            "                                                        189,\n",
            "                                                        29,\n",
            "                                                        599,\n",
            "                                                        207,\n",
            "                                                        41,\n",
            "                                                        409,\n",
            "                                                        139,\n",
            "                                                        111,\n",
            "                                                        43,\n",
            "                                                        151,\n",
            "                                                        37,\n",
            "                                                        19,\n",
            "                                                        155,\n",
            "                                                        181,\n",
            "                                                        225,\n",
            "                                                        31,\n",
            "                                                        269,\n",
            "                                                        117,\n",
            "                                                        195,\n",
            "                                                        171,\n",
            "                                                        187,\n",
            "                                                        173,\n",
            "                                                        65,\n",
            "                                                        77,\n",
            "                                                        51,\n",
            "                                                        125,\n",
            "                                                        57,\n",
            "                                                        61,\n",
            "                                                        65,\n",
            "                                                        47,\n",
            "                                                        83,\n",
            "                                                        33,\n",
            "                                                        101,\n",
            "                                                        29,\n",
            "                                                        19,\n",
            "                                                        47,\n",
            "                                                        39,\n",
            "                                                        95,\n",
            "                                                        101,\n",
            "                                                        63,\n",
            "                                                        81,\n",
            "                                                        37,\n",
            "                                                        31,\n",
            "                                                        29,\n",
            "                                                        49,\n",
            "                                                        27,\n",
            "                                                        15,\n",
            "                                                        43,\n",
            "                                                        19,\n",
            "                                                        39,\n",
            "                                                        59,\n",
            "                                                        21,\n",
            "                                                        19,\n",
            "                                                        37,\n",
            "                                                        45,\n",
            "                                                        45,\n",
            "                                                        115,\n",
            "                                                        53,\n",
            "                                                        61,\n",
            "                                                        55,\n",
            "                                                        47,\n",
            "                                                        63,\n",
            "                                                        27,\n",
            "                                                        39,\n",
            "                                                        121,\n",
            "                                                        23,\n",
            "                                                        43,\n",
            "                                                        113,\n",
            "                                                        31,\n",
            "                                                        43,\n",
            "                                                        41,\n",
            "                                                        39,\n",
            "                                                        103,\n",
            "                                                        29,\n",
            "                                                        15,\n",
            "                                                        43,\n",
            "                                                        27,\n",
            "                                                        15,\n",
            "                                                        41,\n",
            "                                                        107,\n",
            "                                                        23,\n",
            "                                                        89,\n",
            "                                                        75,\n",
            "                                                        31,\n",
            "                                                        51,\n",
            "                                                        35,\n",
            "                                                        125,\n",
            "                                                        45,\n",
            "                                                        103,\n",
            "                                                        63,\n",
            "                                                        55,\n",
            "                                                        73,\n",
            "                                                        57,\n",
            "                                                        19,\n",
            "                                                        167,\n",
            "                                                        41,\n",
            "                                                        47],\n",
            "                                    'episode_reward': [-5.7,\n",
            "                                                       17.1,\n",
            "                                                       -15.1,\n",
            "                                                       107.80000000000004,\n",
            "                                                       -70.0,\n",
            "                                                       -16.700000000000003,\n",
            "                                                       -23.100000000000023,\n",
            "                                                       -14.400000000000006,\n",
            "                                                       13.6,\n",
            "                                                       12.2,\n",
            "                                                       -42.19999999999998,\n",
            "                                                       -17.900000000000002,\n",
            "                                                       -7.799999999999999,\n",
            "                                                       -61.79999999999999,\n",
            "                                                       -29.09999999999993,\n",
            "                                                       30.699999999999996,\n",
            "                                                       -14.200000000000001,\n",
            "                                                       -15.300000000000034,\n",
            "                                                       26.699999999999967,\n",
            "                                                       -30.799999999999997,\n",
            "                                                       51.6,\n",
            "                                                       -29.0,\n",
            "                                                       -25.70000000000001,\n",
            "                                                       -34.9,\n",
            "                                                       -45.49999999999999,\n",
            "                                                       22.400000000000002,\n",
            "                                                       46.499999999999986,\n",
            "                                                       26.1,\n",
            "                                                       9.299999999999999,\n",
            "                                                       34.9,\n",
            "                                                       26.800000000000004,\n",
            "                                                       30.8,\n",
            "                                                       12.5,\n",
            "                                                       14.899999999999997,\n",
            "                                                       16.1,\n",
            "                                                       8.0,\n",
            "                                                       15.600000000000001,\n",
            "                                                       21.8,\n",
            "                                                       40.0,\n",
            "                                                       19.100000000000005,\n",
            "                                                       26.800000000000004,\n",
            "                                                       34.300000000000004,\n",
            "                                                       15.700000000000003,\n",
            "                                                       18.8,\n",
            "                                                       16.500000000000004,\n",
            "                                                       26.900000000000002,\n",
            "                                                       13.200000000000001,\n",
            "                                                       7.799999999999999,\n",
            "                                                       23.199999999999996,\n",
            "                                                       11.6,\n",
            "                                                       14.6,\n",
            "                                                       30.0,\n",
            "                                                       9.899999999999999,\n",
            "                                                       9.0,\n",
            "                                                       21.9,\n",
            "                                                       16.1,\n",
            "                                                       27.700000000000003,\n",
            "                                                       34.800000000000004,\n",
            "                                                       25.5,\n",
            "                                                       12.499999999999996,\n",
            "                                                       36.199999999999996,\n",
            "                                                       14.799999999999997,\n",
            "                                                       30.4,\n",
            "                                                       9.0,\n",
            "                                                       21.2,\n",
            "                                                       69.50000000000004,\n",
            "                                                       10.4,\n",
            "                                                       20.4,\n",
            "                                                       47.3,\n",
            "                                                       10.599999999999998,\n",
            "                                                       8.2,\n",
            "                                                       11.900000000000002,\n",
            "                                                       17.800000000000004,\n",
            "                                                       6.6,\n",
            "                                                       18.099999999999998,\n",
            "                                                       8.8,\n",
            "                                                       8.8,\n",
            "                                                       13.4,\n",
            "                                                       9.0,\n",
            "                                                       22.7,\n",
            "                                                       67.60000000000001,\n",
            "                                                       9.6,\n",
            "                                                       40.5,\n",
            "                                                       38.6,\n",
            "                                                       17.400000000000002,\n",
            "                                                       8.4,\n",
            "                                                       14.200000000000001,\n",
            "                                                       17.5,\n",
            "                                                       28.9,\n",
            "                                                       51.600000000000016,\n",
            "                                                       18.000000000000004,\n",
            "                                                       7.399999999999999,\n",
            "                                                       16.5,\n",
            "                                                       35.3,\n",
            "                                                       10.999999999999998,\n",
            "                                                       65.80000000000001,\n",
            "                                                       12.900000000000002,\n",
            "                                                       11.4]},\n",
            "                     'num_faulty_episodes': 0,\n",
            "                     'policy_reward_max': {},\n",
            "                     'policy_reward_mean': {},\n",
            "                     'policy_reward_min': {},\n",
            "                     'sampler_perf': {'mean_action_processing_ms': 0.22207926718029788,\n",
            "                                      'mean_env_render_ms': 0.0,\n",
            "                                      'mean_env_wait_ms': 0.06620356446233085,\n",
            "                                      'mean_inference_ms': 1.820606127523691,\n",
            "                                      'mean_raw_obs_processing_ms': 0.5427654063093263}},\n",
            " 'time_since_restore': 30.164165019989014,\n",
            " 'time_this_iter_s': 15.182566404342651,\n",
            " 'time_total_s': 30.164165019989014,\n",
            " 'timers': {'learn_throughput': 942.698,\n",
            "            'learn_time_ms': 4243.142,\n",
            "            'load_throughput': 4943198.586,\n",
            "            'load_time_ms': 0.809,\n",
            "            'sample_time_ms': 10824.058,\n",
            "            'synch_weights_time_ms': 3.65,\n",
            "            'training_iteration_time_ms': 15074.083},\n",
            " 'timestamp': 1693749846,\n",
            " 'timesteps_total': 8000,\n",
            " 'training_iteration': 2,\n",
            " 'trial_id': 'default'}\n",
            "****************************************************************************************************************************************\n",
            "{'agent_timesteps_total': 12000,\n",
            " 'config': {'_AlgorithmConfig__prior_exploration_config': None,\n",
            "            '_disable_action_flattening': False,\n",
            "            '_disable_execution_plan_api': True,\n",
            "            '_disable_initialize_loss_from_dummy_batch': False,\n",
            "            '_disable_preprocessor_api': False,\n",
            "            '_enable_learner_api': False,\n",
            "            '_enable_rl_module_api': False,\n",
            "            '_fake_gpus': False,\n",
            "            '_is_atari': None,\n",
            "            '_learner_class': None,\n",
            "            '_tf_policy_handles_more_than_one_loss': False,\n",
            "            'action_mask_key': 'action_mask',\n",
            "            'action_space': None,\n",
            "            'actions_in_input_normalized': False,\n",
            "            'algorithm_config_overrides_per_module': {},\n",
            "            'always_attach_evaluation_results': False,\n",
            "            'auto_wrap_old_gym_envs': True,\n",
            "            'batch_mode': 'truncate_episodes',\n",
            "            'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>,\n",
            "            'checkpoint_trainable_policies_only': False,\n",
            "            'clip_actions': False,\n",
            "            'clip_param': 0.2,\n",
            "            'clip_rewards': None,\n",
            "            'compress_observations': False,\n",
            "            'count_steps_by': 'env_steps',\n",
            "            'create_env_on_driver': False,\n",
            "            'custom_eval_function': None,\n",
            "            'custom_resources_per_worker': {},\n",
            "            'delay_between_worker_restarts_s': 60.0,\n",
            "            'disable_env_checking': False,\n",
            "            'eager_max_retraces': 20,\n",
            "            'eager_tracing': True,\n",
            "            'enable_async_evaluation': False,\n",
            "            'enable_connectors': True,\n",
            "            'enable_tf1_exec_eagerly': False,\n",
            "            'entropy_coeff': 0.0,\n",
            "            'entropy_coeff_schedule': None,\n",
            "            'env': 'my',\n",
            "            'env_config': {},\n",
            "            'env_runner_cls': None,\n",
            "            'env_task_fn': None,\n",
            "            'evaluation_config': None,\n",
            "            'evaluation_duration': 10,\n",
            "            'evaluation_duration_unit': 'episodes',\n",
            "            'evaluation_interval': None,\n",
            "            'evaluation_num_workers': 0,\n",
            "            'evaluation_parallel_to_training': False,\n",
            "            'evaluation_sample_timeout_s': 180.0,\n",
            "            'exploration_config': {'type': 'StochasticSampling'},\n",
            "            'explore': True,\n",
            "            'export_native_model_files': True,\n",
            "            'extra_python_environs_for_driver': {},\n",
            "            'extra_python_environs_for_worker': {},\n",
            "            'fake_sampler': False,\n",
            "            'framework': 'torch',\n",
            "            'gamma': 0.99,\n",
            "            'grad_clip': None,\n",
            "            'grad_clip_by': 'global_norm',\n",
            "            'ignore_worker_failures': False,\n",
            "            'in_evaluation': False,\n",
            "            'input': 'sampler',\n",
            "            'input_config': {},\n",
            "            'keep_per_episode_custom_metrics': False,\n",
            "            'kl_coeff': 0.2,\n",
            "            'kl_target': 0.01,\n",
            "            'lambda': 0.95,\n",
            "            'local_gpu_idx': 0,\n",
            "            'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
            "                                      'intra_op_parallelism_threads': 8},\n",
            "            'log_level': 'WARN',\n",
            "            'log_sys_usage': True,\n",
            "            'logger_config': None,\n",
            "            'logger_creator': None,\n",
            "            'lr': 0.0003,\n",
            "            'lr_schedule': None,\n",
            "            'max_num_worker_restarts': 1000,\n",
            "            'max_requests_in_flight_per_sampler_worker': 2,\n",
            "            'metrics_episode_collection_timeout_s': 60.0,\n",
            "            'metrics_num_episodes_for_smoothing': 100,\n",
            "            'min_sample_timesteps_per_iteration': 0,\n",
            "            'min_time_s_per_iteration': None,\n",
            "            'min_train_timesteps_per_iteration': 0,\n",
            "            'model': {'_disable_action_flattening': False,\n",
            "                      '_disable_preprocessor_api': False,\n",
            "                      '_time_major': False,\n",
            "                      '_use_default_native_models': -1,\n",
            "                      'always_check_shapes': False,\n",
            "                      'attention_dim': 64,\n",
            "                      'attention_head_dim': 32,\n",
            "                      'attention_init_gru_gate_bias': 2.0,\n",
            "                      'attention_memory_inference': 50,\n",
            "                      'attention_memory_training': 50,\n",
            "                      'attention_num_heads': 1,\n",
            "                      'attention_num_transformer_units': 1,\n",
            "                      'attention_position_wise_mlp_dim': 32,\n",
            "                      'attention_use_n_prev_actions': 0,\n",
            "                      'attention_use_n_prev_rewards': 0,\n",
            "                      'conv_activation': 'relu',\n",
            "                      'conv_filters': None,\n",
            "                      'custom_action_dist': None,\n",
            "                      'custom_model': None,\n",
            "                      'custom_model_config': {},\n",
            "                      'custom_preprocessor': None,\n",
            "                      'dim': 84,\n",
            "                      'encoder_latent_dim': None,\n",
            "                      'fcnet_activation': 'tanh',\n",
            "                      'fcnet_hiddens': [256, 256],\n",
            "                      'framestack': True,\n",
            "                      'free_log_std': False,\n",
            "                      'grayscale': False,\n",
            "                      'lstm_cell_size': 256,\n",
            "                      'lstm_use_prev_action': False,\n",
            "                      'lstm_use_prev_action_reward': -1,\n",
            "                      'lstm_use_prev_reward': False,\n",
            "                      'max_seq_len': 20,\n",
            "                      'no_final_linear': False,\n",
            "                      'post_fcnet_activation': 'relu',\n",
            "                      'post_fcnet_hiddens': [],\n",
            "                      'use_attention': False,\n",
            "                      'use_lstm': False,\n",
            "                      'vf_share_layers': False,\n",
            "                      'zero_mean': True},\n",
            "            'normalize_actions': True,\n",
            "            'num_consecutive_worker_failures_tolerance': 100,\n",
            "            'num_cpus_for_driver': 1,\n",
            "            'num_cpus_per_learner_worker': 1,\n",
            "            'num_cpus_per_worker': 1,\n",
            "            'num_envs_per_worker': 1,\n",
            "            'num_gpus': 0,\n",
            "            'num_gpus_per_learner_worker': 0,\n",
            "            'num_gpus_per_worker': 0,\n",
            "            'num_learner_workers': 0,\n",
            "            'num_sgd_iter': 20,\n",
            "            'num_workers': 1,\n",
            "            'observation_filter': 'NoFilter',\n",
            "            'observation_fn': None,\n",
            "            'observation_space': None,\n",
            "            'off_policy_estimation_methods': {},\n",
            "            'offline_sampling': False,\n",
            "            'ope_split_batch_by_episode': True,\n",
            "            'optimizer': {},\n",
            "            'output': None,\n",
            "            'output_compress_columns': ['obs', 'new_obs'],\n",
            "            'output_config': {},\n",
            "            'output_max_file_size': 67108864,\n",
            "            'placement_strategy': 'PACK',\n",
            "            'policies': {'default_policy': (None, None, None, None)},\n",
            "            'policies_to_train': None,\n",
            "            'policy_map_cache': -1,\n",
            "            'policy_map_capacity': 100,\n",
            "            'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x78f59bc564d0>,\n",
            "            'policy_states_are_swappable': False,\n",
            "            'postprocess_inputs': False,\n",
            "            'preprocessor_pref': 'deepmind',\n",
            "            'recreate_failed_workers': False,\n",
            "            'remote_env_batch_wait_ms': 0,\n",
            "            'remote_worker_envs': False,\n",
            "            'render_env': False,\n",
            "            'replay_sequence_length': None,\n",
            "            'restart_failed_sub_environments': False,\n",
            "            'rl_module_spec': None,\n",
            "            'rollout_fragment_length': 200,\n",
            "            'sample_async': False,\n",
            "            'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
            "            'sampler_perf_stats_ema_coef': None,\n",
            "            'seed': None,\n",
            "            'sgd_minibatch_size': 256,\n",
            "            'shuffle_buffer_size': 0,\n",
            "            'shuffle_sequences': True,\n",
            "            'simple_optimizer': False,\n",
            "            'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
            "            'synchronize_filters': -1,\n",
            "            'tf_session_args': {'allow_soft_placement': True,\n",
            "                                'device_count': {'CPU': 1},\n",
            "                                'gpu_options': {'allow_growth': True},\n",
            "                                'inter_op_parallelism_threads': 2,\n",
            "                                'intra_op_parallelism_threads': 2,\n",
            "                                'log_device_placement': False},\n",
            "            'torch_compile_learner': False,\n",
            "            'torch_compile_learner_dynamo_backend': 'inductor',\n",
            "            'torch_compile_learner_dynamo_mode': None,\n",
            "            'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
            "            'torch_compile_worker': False,\n",
            "            'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
            "            'torch_compile_worker_dynamo_mode': None,\n",
            "            'train_batch_size': 4000,\n",
            "            'update_worker_filter_stats': True,\n",
            "            'use_critic': True,\n",
            "            'use_gae': True,\n",
            "            'use_kl_loss': True,\n",
            "            'use_worker_filter_stats': True,\n",
            "            'validate_workers_after_construction': True,\n",
            "            'vf_clip_param': 10.0,\n",
            "            'vf_loss_coeff': 1.0,\n",
            "            'vf_share_layers': -1,\n",
            "            'worker_cls': -1,\n",
            "            'worker_health_probe_timeout_s': 60,\n",
            "            'worker_restore_timeout_s': 1800},\n",
            " 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.01177978515625,\n",
            "                       'StateBufferConnector_ms': 0.006964683532714844,\n",
            "                       'ViewRequirementAgentConnector_ms': 0.21586155891418457},\n",
            " 'counters': {'num_agent_steps_sampled': 12000,\n",
            "              'num_agent_steps_trained': 12000,\n",
            "              'num_env_steps_sampled': 12000,\n",
            "              'num_env_steps_trained': 12000},\n",
            " 'custom_metrics': {},\n",
            " 'date': '2023-09-03_14-04-21',\n",
            " 'done': False,\n",
            " 'episode_len_mean': 59.14,\n",
            " 'episode_media': {},\n",
            " 'episode_reward_max': 164.19999999999996,\n",
            " 'episode_reward_mean': 28.322999999999997,\n",
            " 'episode_reward_min': 6.6,\n",
            " 'episodes_this_iter': 68,\n",
            " 'episodes_total': 166,\n",
            " 'hist_stats': {'episode_lengths': [23,\n",
            "                                    43,\n",
            "                                    113,\n",
            "                                    31,\n",
            "                                    43,\n",
            "                                    41,\n",
            "                                    39,\n",
            "                                    103,\n",
            "                                    29,\n",
            "                                    15,\n",
            "                                    43,\n",
            "                                    27,\n",
            "                                    15,\n",
            "                                    41,\n",
            "                                    107,\n",
            "                                    23,\n",
            "                                    89,\n",
            "                                    75,\n",
            "                                    31,\n",
            "                                    51,\n",
            "                                    35,\n",
            "                                    125,\n",
            "                                    45,\n",
            "                                    103,\n",
            "                                    63,\n",
            "                                    55,\n",
            "                                    73,\n",
            "                                    57,\n",
            "                                    19,\n",
            "                                    167,\n",
            "                                    41,\n",
            "                                    47,\n",
            "                                    319,\n",
            "                                    65,\n",
            "                                    113,\n",
            "                                    43,\n",
            "                                    27,\n",
            "                                    85,\n",
            "                                    41,\n",
            "                                    47,\n",
            "                                    41,\n",
            "                                    189,\n",
            "                                    79,\n",
            "                                    99,\n",
            "                                    29,\n",
            "                                    55,\n",
            "                                    127,\n",
            "                                    39,\n",
            "                                    21,\n",
            "                                    49,\n",
            "                                    59,\n",
            "                                    113,\n",
            "                                    75,\n",
            "                                    21,\n",
            "                                    33,\n",
            "                                    39,\n",
            "                                    35,\n",
            "                                    65,\n",
            "                                    25,\n",
            "                                    31,\n",
            "                                    65,\n",
            "                                    151,\n",
            "                                    79,\n",
            "                                    35,\n",
            "                                    47,\n",
            "                                    109,\n",
            "                                    185,\n",
            "                                    43,\n",
            "                                    17,\n",
            "                                    87,\n",
            "                                    39,\n",
            "                                    79,\n",
            "                                    33,\n",
            "                                    31,\n",
            "                                    35,\n",
            "                                    59,\n",
            "                                    99,\n",
            "                                    29,\n",
            "                                    29,\n",
            "                                    41,\n",
            "                                    61,\n",
            "                                    59,\n",
            "                                    71,\n",
            "                                    65,\n",
            "                                    23,\n",
            "                                    25,\n",
            "                                    35,\n",
            "                                    29,\n",
            "                                    69,\n",
            "                                    35,\n",
            "                                    49,\n",
            "                                    27,\n",
            "                                    35,\n",
            "                                    29,\n",
            "                                    61,\n",
            "                                    27,\n",
            "                                    11,\n",
            "                                    43,\n",
            "                                    55,\n",
            "                                    37],\n",
            "                'episode_reward': [10.4,\n",
            "                                   20.4,\n",
            "                                   47.3,\n",
            "                                   10.599999999999998,\n",
            "                                   8.2,\n",
            "                                   11.900000000000002,\n",
            "                                   17.800000000000004,\n",
            "                                   6.6,\n",
            "                                   18.099999999999998,\n",
            "                                   8.8,\n",
            "                                   8.8,\n",
            "                                   13.4,\n",
            "                                   9.0,\n",
            "                                   22.7,\n",
            "                                   67.60000000000001,\n",
            "                                   9.6,\n",
            "                                   40.5,\n",
            "                                   38.6,\n",
            "                                   17.400000000000002,\n",
            "                                   8.4,\n",
            "                                   14.200000000000001,\n",
            "                                   17.5,\n",
            "                                   28.9,\n",
            "                                   51.600000000000016,\n",
            "                                   18.000000000000004,\n",
            "                                   7.399999999999999,\n",
            "                                   16.5,\n",
            "                                   35.3,\n",
            "                                   10.999999999999998,\n",
            "                                   65.80000000000001,\n",
            "                                   12.900000000000002,\n",
            "                                   11.4,\n",
            "                                   164.19999999999996,\n",
            "                                   35.49999999999999,\n",
            "                                   63.7,\n",
            "                                   15.000000000000002,\n",
            "                                   12.6,\n",
            "                                   46.099999999999994,\n",
            "                                   20.900000000000002,\n",
            "                                   20.6,\n",
            "                                   19.699999999999996,\n",
            "                                   100.30000000000004,\n",
            "                                   42.199999999999996,\n",
            "                                   58.40000000000003,\n",
            "                                   19.5,\n",
            "                                   32.99999999999999,\n",
            "                                   81.80000000000001,\n",
            "                                   28.799999999999997,\n",
            "                                   14.5,\n",
            "                                   18.100000000000005,\n",
            "                                   31.999999999999996,\n",
            "                                   42.7,\n",
            "                                   34.6,\n",
            "                                   9.899999999999999,\n",
            "                                   22.099999999999994,\n",
            "                                   21.999999999999996,\n",
            "                                   14.2,\n",
            "                                   32.3,\n",
            "                                   10.7,\n",
            "                                   18.2,\n",
            "                                   39.49999999999999,\n",
            "                                   80.59999999999997,\n",
            "                                   32.4,\n",
            "                                   21.6,\n",
            "                                   22.2,\n",
            "                                   56.099999999999994,\n",
            "                                   97.50000000000004,\n",
            "                                   24.2,\n",
            "                                   11.5,\n",
            "                                   50.40000000000001,\n",
            "                                   17.599999999999998,\n",
            "                                   42.20000000000001,\n",
            "                                   16.7,\n",
            "                                   13.600000000000003,\n",
            "                                   17.2,\n",
            "                                   31.400000000000002,\n",
            "                                   58.99999999999999,\n",
            "                                   14.299999999999999,\n",
            "                                   14.9,\n",
            "                                   21.900000000000002,\n",
            "                                   22.5,\n",
            "                                   24.8,\n",
            "                                   35.4,\n",
            "                                   46.49999999999998,\n",
            "                                   14.800000000000002,\n",
            "                                   13.3,\n",
            "                                   8.4,\n",
            "                                   18.099999999999998,\n",
            "                                   34.699999999999996,\n",
            "                                   16.0,\n",
            "                                   24.900000000000006,\n",
            "                                   15.0,\n",
            "                                   19.2,\n",
            "                                   13.3,\n",
            "                                   26.9,\n",
            "                                   16.600000000000005,\n",
            "                                   6.6,\n",
            "                                   22.0,\n",
            "                                   28.200000000000003,\n",
            "                                   20.1]},\n",
            " 'hostname': '9e348d898222',\n",
            " 'info': {'learner': {'default_policy': {'custom_metrics': {},\n",
            "                                         'diff_num_grad_updates_vs_sampler_policy': 149.5,\n",
            "                                         'learner_stats': {'allreduce_latency': 0.0,\n",
            "                                                           'cur_kl_coeff': 0.19999999999999998,\n",
            "                                                           'cur_lr': 0.0003,\n",
            "                                                           'entropy': 0.6255539212624232,\n",
            "                                                           'entropy_coeff': 0.0,\n",
            "                                                           'grad_gnorm': 8.015252068291108,\n",
            "                                                           'kl': 0.01368677364000759,\n",
            "                                                           'policy_loss': -0.04837601842358708,\n",
            "                                                           'total_loss': 6.227677507400513,\n",
            "                                                           'vf_explained_var': -0.3186047257979711,\n",
            "                                                           'vf_loss': 6.273316241900126},\n",
            "                                         'model': {},\n",
            "                                         'num_agent_steps_trained': 256.0,\n",
            "                                         'num_grad_updates_lifetime': 750.5}},\n",
            "          'num_agent_steps_sampled': 12000,\n",
            "          'num_agent_steps_trained': 12000,\n",
            "          'num_env_steps_sampled': 12000,\n",
            "          'num_env_steps_trained': 12000},\n",
            " 'iterations_since_restore': 3,\n",
            " 'node_ip': '172.28.0.12',\n",
            " 'num_agent_steps_sampled': 12000,\n",
            " 'num_agent_steps_trained': 12000,\n",
            " 'num_env_steps_sampled': 12000,\n",
            " 'num_env_steps_sampled_this_iter': 4000,\n",
            " 'num_env_steps_sampled_throughput_per_sec': 275.03300042042116,\n",
            " 'num_env_steps_trained': 12000,\n",
            " 'num_env_steps_trained_this_iter': 4000,\n",
            " 'num_env_steps_trained_throughput_per_sec': 275.03300042042116,\n",
            " 'num_faulty_episodes': 0,\n",
            " 'num_healthy_workers': 1,\n",
            " 'num_in_flight_async_reqs': 0,\n",
            " 'num_remote_worker_restarts': 0,\n",
            " 'num_steps_trained_this_iter': 4000,\n",
            " 'perf': {'cpu_util_percent': 74.7, 'ram_util_percent': 18.399999999999995},\n",
            " 'pid': 401,\n",
            " 'policy_reward_max': {},\n",
            " 'policy_reward_mean': {},\n",
            " 'policy_reward_min': {},\n",
            " 'sampler_perf': {'mean_action_processing_ms': 0.2198180901538332,\n",
            "                  'mean_env_render_ms': 0.0,\n",
            "                  'mean_env_wait_ms': 0.06550865215321773,\n",
            "                  'mean_inference_ms': 1.8101872358550697,\n",
            "                  'mean_raw_obs_processing_ms': 0.5456348557032277},\n",
            " 'sampler_results': {'connector_metrics': {'ObsPreprocessorConnector_ms': 0.01177978515625,\n",
            "                                           'StateBufferConnector_ms': 0.006964683532714844,\n",
            "                                           'ViewRequirementAgentConnector_ms': 0.21586155891418457},\n",
            "                     'custom_metrics': {},\n",
            "                     'episode_len_mean': 59.14,\n",
            "                     'episode_media': {},\n",
            "                     'episode_reward_max': 164.19999999999996,\n",
            "                     'episode_reward_mean': 28.322999999999997,\n",
            "                     'episode_reward_min': 6.6,\n",
            "                     'episodes_this_iter': 68,\n",
            "                     'hist_stats': {'episode_lengths': [23,\n",
            "                                                        43,\n",
            "                                                        113,\n",
            "                                                        31,\n",
            "                                                        43,\n",
            "                                                        41,\n",
            "                                                        39,\n",
            "                                                        103,\n",
            "                                                        29,\n",
            "                                                        15,\n",
            "                                                        43,\n",
            "                                                        27,\n",
            "                                                        15,\n",
            "                                                        41,\n",
            "                                                        107,\n",
            "                                                        23,\n",
            "                                                        89,\n",
            "                                                        75,\n",
            "                                                        31,\n",
            "                                                        51,\n",
            "                                                        35,\n",
            "                                                        125,\n",
            "                                                        45,\n",
            "                                                        103,\n",
            "                                                        63,\n",
            "                                                        55,\n",
            "                                                        73,\n",
            "                                                        57,\n",
            "                                                        19,\n",
            "                                                        167,\n",
            "                                                        41,\n",
            "                                                        47,\n",
            "                                                        319,\n",
            "                                                        65,\n",
            "                                                        113,\n",
            "                                                        43,\n",
            "                                                        27,\n",
            "                                                        85,\n",
            "                                                        41,\n",
            "                                                        47,\n",
            "                                                        41,\n",
            "                                                        189,\n",
            "                                                        79,\n",
            "                                                        99,\n",
            "                                                        29,\n",
            "                                                        55,\n",
            "                                                        127,\n",
            "                                                        39,\n",
            "                                                        21,\n",
            "                                                        49,\n",
            "                                                        59,\n",
            "                                                        113,\n",
            "                                                        75,\n",
            "                                                        21,\n",
            "                                                        33,\n",
            "                                                        39,\n",
            "                                                        35,\n",
            "                                                        65,\n",
            "                                                        25,\n",
            "                                                        31,\n",
            "                                                        65,\n",
            "                                                        151,\n",
            "                                                        79,\n",
            "                                                        35,\n",
            "                                                        47,\n",
            "                                                        109,\n",
            "                                                        185,\n",
            "                                                        43,\n",
            "                                                        17,\n",
            "                                                        87,\n",
            "                                                        39,\n",
            "                                                        79,\n",
            "                                                        33,\n",
            "                                                        31,\n",
            "                                                        35,\n",
            "                                                        59,\n",
            "                                                        99,\n",
            "                                                        29,\n",
            "                                                        29,\n",
            "                                                        41,\n",
            "                                                        61,\n",
            "                                                        59,\n",
            "                                                        71,\n",
            "                                                        65,\n",
            "                                                        23,\n",
            "                                                        25,\n",
            "                                                        35,\n",
            "                                                        29,\n",
            "                                                        69,\n",
            "                                                        35,\n",
            "                                                        49,\n",
            "                                                        27,\n",
            "                                                        35,\n",
            "                                                        29,\n",
            "                                                        61,\n",
            "                                                        27,\n",
            "                                                        11,\n",
            "                                                        43,\n",
            "                                                        55,\n",
            "                                                        37],\n",
            "                                    'episode_reward': [10.4,\n",
            "                                                       20.4,\n",
            "                                                       47.3,\n",
            "                                                       10.599999999999998,\n",
            "                                                       8.2,\n",
            "                                                       11.900000000000002,\n",
            "                                                       17.800000000000004,\n",
            "                                                       6.6,\n",
            "                                                       18.099999999999998,\n",
            "                                                       8.8,\n",
            "                                                       8.8,\n",
            "                                                       13.4,\n",
            "                                                       9.0,\n",
            "                                                       22.7,\n",
            "                                                       67.60000000000001,\n",
            "                                                       9.6,\n",
            "                                                       40.5,\n",
            "                                                       38.6,\n",
            "                                                       17.400000000000002,\n",
            "                                                       8.4,\n",
            "                                                       14.200000000000001,\n",
            "                                                       17.5,\n",
            "                                                       28.9,\n",
            "                                                       51.600000000000016,\n",
            "                                                       18.000000000000004,\n",
            "                                                       7.399999999999999,\n",
            "                                                       16.5,\n",
            "                                                       35.3,\n",
            "                                                       10.999999999999998,\n",
            "                                                       65.80000000000001,\n",
            "                                                       12.900000000000002,\n",
            "                                                       11.4,\n",
            "                                                       164.19999999999996,\n",
            "                                                       35.49999999999999,\n",
            "                                                       63.7,\n",
            "                                                       15.000000000000002,\n",
            "                                                       12.6,\n",
            "                                                       46.099999999999994,\n",
            "                                                       20.900000000000002,\n",
            "                                                       20.6,\n",
            "                                                       19.699999999999996,\n",
            "                                                       100.30000000000004,\n",
            "                                                       42.199999999999996,\n",
            "                                                       58.40000000000003,\n",
            "                                                       19.5,\n",
            "                                                       32.99999999999999,\n",
            "                                                       81.80000000000001,\n",
            "                                                       28.799999999999997,\n",
            "                                                       14.5,\n",
            "                                                       18.100000000000005,\n",
            "                                                       31.999999999999996,\n",
            "                                                       42.7,\n",
            "                                                       34.6,\n",
            "                                                       9.899999999999999,\n",
            "                                                       22.099999999999994,\n",
            "                                                       21.999999999999996,\n",
            "                                                       14.2,\n",
            "                                                       32.3,\n",
            "                                                       10.7,\n",
            "                                                       18.2,\n",
            "                                                       39.49999999999999,\n",
            "                                                       80.59999999999997,\n",
            "                                                       32.4,\n",
            "                                                       21.6,\n",
            "                                                       22.2,\n",
            "                                                       56.099999999999994,\n",
            "                                                       97.50000000000004,\n",
            "                                                       24.2,\n",
            "                                                       11.5,\n",
            "                                                       50.40000000000001,\n",
            "                                                       17.599999999999998,\n",
            "                                                       42.20000000000001,\n",
            "                                                       16.7,\n",
            "                                                       13.600000000000003,\n",
            "                                                       17.2,\n",
            "                                                       31.400000000000002,\n",
            "                                                       58.99999999999999,\n",
            "                                                       14.299999999999999,\n",
            "                                                       14.9,\n",
            "                                                       21.900000000000002,\n",
            "                                                       22.5,\n",
            "                                                       24.8,\n",
            "                                                       35.4,\n",
            "                                                       46.49999999999998,\n",
            "                                                       14.800000000000002,\n",
            "                                                       13.3,\n",
            "                                                       8.4,\n",
            "                                                       18.099999999999998,\n",
            "                                                       34.699999999999996,\n",
            "                                                       16.0,\n",
            "                                                       24.900000000000006,\n",
            "                                                       15.0,\n",
            "                                                       19.2,\n",
            "                                                       13.3,\n",
            "                                                       26.9,\n",
            "                                                       16.600000000000005,\n",
            "                                                       6.6,\n",
            "                                                       22.0,\n",
            "                                                       28.200000000000003,\n",
            "                                                       20.1]},\n",
            "                     'num_faulty_episodes': 0,\n",
            "                     'policy_reward_max': {},\n",
            "                     'policy_reward_mean': {},\n",
            "                     'policy_reward_min': {},\n",
            "                     'sampler_perf': {'mean_action_processing_ms': 0.2198180901538332,\n",
            "                                      'mean_env_render_ms': 0.0,\n",
            "                                      'mean_env_wait_ms': 0.06550865215321773,\n",
            "                                      'mean_inference_ms': 1.8101872358550697,\n",
            "                                      'mean_raw_obs_processing_ms': 0.5456348557032277}},\n",
            " 'time_since_restore': 44.716203451156616,\n",
            " 'time_this_iter_s': 14.552038431167603,\n",
            " 'time_total_s': 44.716203451156616,\n",
            " 'timers': {'learn_throughput': 964.407,\n",
            "            'learn_time_ms': 4147.627,\n",
            "            'load_throughput': 5403290.177,\n",
            "            'load_time_ms': 0.74,\n",
            "            'sample_time_ms': 10743.408,\n",
            "            'synch_weights_time_ms': 3.517,\n",
            "            'training_iteration_time_ms': 14897.27},\n",
            " 'timestamp': 1693749861,\n",
            " 'timesteps_total': 12000,\n",
            " 'training_iteration': 3,\n",
            " 'trial_id': 'default'}\n",
            "****************************************************************************************************************************************\n",
            "{'agent_timesteps_total': 16000,\n",
            " 'config': {'_AlgorithmConfig__prior_exploration_config': None,\n",
            "            '_disable_action_flattening': False,\n",
            "            '_disable_execution_plan_api': True,\n",
            "            '_disable_initialize_loss_from_dummy_batch': False,\n",
            "            '_disable_preprocessor_api': False,\n",
            "            '_enable_learner_api': False,\n",
            "            '_enable_rl_module_api': False,\n",
            "            '_fake_gpus': False,\n",
            "            '_is_atari': None,\n",
            "            '_learner_class': None,\n",
            "            '_tf_policy_handles_more_than_one_loss': False,\n",
            "            'action_mask_key': 'action_mask',\n",
            "            'action_space': None,\n",
            "            'actions_in_input_normalized': False,\n",
            "            'algorithm_config_overrides_per_module': {},\n",
            "            'always_attach_evaluation_results': False,\n",
            "            'auto_wrap_old_gym_envs': True,\n",
            "            'batch_mode': 'truncate_episodes',\n",
            "            'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>,\n",
            "            'checkpoint_trainable_policies_only': False,\n",
            "            'clip_actions': False,\n",
            "            'clip_param': 0.2,\n",
            "            'clip_rewards': None,\n",
            "            'compress_observations': False,\n",
            "            'count_steps_by': 'env_steps',\n",
            "            'create_env_on_driver': False,\n",
            "            'custom_eval_function': None,\n",
            "            'custom_resources_per_worker': {},\n",
            "            'delay_between_worker_restarts_s': 60.0,\n",
            "            'disable_env_checking': False,\n",
            "            'eager_max_retraces': 20,\n",
            "            'eager_tracing': True,\n",
            "            'enable_async_evaluation': False,\n",
            "            'enable_connectors': True,\n",
            "            'enable_tf1_exec_eagerly': False,\n",
            "            'entropy_coeff': 0.0,\n",
            "            'entropy_coeff_schedule': None,\n",
            "            'env': 'my',\n",
            "            'env_config': {},\n",
            "            'env_runner_cls': None,\n",
            "            'env_task_fn': None,\n",
            "            'evaluation_config': None,\n",
            "            'evaluation_duration': 10,\n",
            "            'evaluation_duration_unit': 'episodes',\n",
            "            'evaluation_interval': None,\n",
            "            'evaluation_num_workers': 0,\n",
            "            'evaluation_parallel_to_training': False,\n",
            "            'evaluation_sample_timeout_s': 180.0,\n",
            "            'exploration_config': {'type': 'StochasticSampling'},\n",
            "            'explore': True,\n",
            "            'export_native_model_files': True,\n",
            "            'extra_python_environs_for_driver': {},\n",
            "            'extra_python_environs_for_worker': {},\n",
            "            'fake_sampler': False,\n",
            "            'framework': 'torch',\n",
            "            'gamma': 0.99,\n",
            "            'grad_clip': None,\n",
            "            'grad_clip_by': 'global_norm',\n",
            "            'ignore_worker_failures': False,\n",
            "            'in_evaluation': False,\n",
            "            'input': 'sampler',\n",
            "            'input_config': {},\n",
            "            'keep_per_episode_custom_metrics': False,\n",
            "            'kl_coeff': 0.2,\n",
            "            'kl_target': 0.01,\n",
            "            'lambda': 0.95,\n",
            "            'local_gpu_idx': 0,\n",
            "            'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
            "                                      'intra_op_parallelism_threads': 8},\n",
            "            'log_level': 'WARN',\n",
            "            'log_sys_usage': True,\n",
            "            'logger_config': None,\n",
            "            'logger_creator': None,\n",
            "            'lr': 0.0003,\n",
            "            'lr_schedule': None,\n",
            "            'max_num_worker_restarts': 1000,\n",
            "            'max_requests_in_flight_per_sampler_worker': 2,\n",
            "            'metrics_episode_collection_timeout_s': 60.0,\n",
            "            'metrics_num_episodes_for_smoothing': 100,\n",
            "            'min_sample_timesteps_per_iteration': 0,\n",
            "            'min_time_s_per_iteration': None,\n",
            "            'min_train_timesteps_per_iteration': 0,\n",
            "            'model': {'_disable_action_flattening': False,\n",
            "                      '_disable_preprocessor_api': False,\n",
            "                      '_time_major': False,\n",
            "                      '_use_default_native_models': -1,\n",
            "                      'always_check_shapes': False,\n",
            "                      'attention_dim': 64,\n",
            "                      'attention_head_dim': 32,\n",
            "                      'attention_init_gru_gate_bias': 2.0,\n",
            "                      'attention_memory_inference': 50,\n",
            "                      'attention_memory_training': 50,\n",
            "                      'attention_num_heads': 1,\n",
            "                      'attention_num_transformer_units': 1,\n",
            "                      'attention_position_wise_mlp_dim': 32,\n",
            "                      'attention_use_n_prev_actions': 0,\n",
            "                      'attention_use_n_prev_rewards': 0,\n",
            "                      'conv_activation': 'relu',\n",
            "                      'conv_filters': None,\n",
            "                      'custom_action_dist': None,\n",
            "                      'custom_model': None,\n",
            "                      'custom_model_config': {},\n",
            "                      'custom_preprocessor': None,\n",
            "                      'dim': 84,\n",
            "                      'encoder_latent_dim': None,\n",
            "                      'fcnet_activation': 'tanh',\n",
            "                      'fcnet_hiddens': [256, 256],\n",
            "                      'framestack': True,\n",
            "                      'free_log_std': False,\n",
            "                      'grayscale': False,\n",
            "                      'lstm_cell_size': 256,\n",
            "                      'lstm_use_prev_action': False,\n",
            "                      'lstm_use_prev_action_reward': -1,\n",
            "                      'lstm_use_prev_reward': False,\n",
            "                      'max_seq_len': 20,\n",
            "                      'no_final_linear': False,\n",
            "                      'post_fcnet_activation': 'relu',\n",
            "                      'post_fcnet_hiddens': [],\n",
            "                      'use_attention': False,\n",
            "                      'use_lstm': False,\n",
            "                      'vf_share_layers': False,\n",
            "                      'zero_mean': True},\n",
            "            'normalize_actions': True,\n",
            "            'num_consecutive_worker_failures_tolerance': 100,\n",
            "            'num_cpus_for_driver': 1,\n",
            "            'num_cpus_per_learner_worker': 1,\n",
            "            'num_cpus_per_worker': 1,\n",
            "            'num_envs_per_worker': 1,\n",
            "            'num_gpus': 0,\n",
            "            'num_gpus_per_learner_worker': 0,\n",
            "            'num_gpus_per_worker': 0,\n",
            "            'num_learner_workers': 0,\n",
            "            'num_sgd_iter': 20,\n",
            "            'num_workers': 1,\n",
            "            'observation_filter': 'NoFilter',\n",
            "            'observation_fn': None,\n",
            "            'observation_space': None,\n",
            "            'off_policy_estimation_methods': {},\n",
            "            'offline_sampling': False,\n",
            "            'ope_split_batch_by_episode': True,\n",
            "            'optimizer': {},\n",
            "            'output': None,\n",
            "            'output_compress_columns': ['obs', 'new_obs'],\n",
            "            'output_config': {},\n",
            "            'output_max_file_size': 67108864,\n",
            "            'placement_strategy': 'PACK',\n",
            "            'policies': {'default_policy': (None, None, None, None)},\n",
            "            'policies_to_train': None,\n",
            "            'policy_map_cache': -1,\n",
            "            'policy_map_capacity': 100,\n",
            "            'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x78f59bc564d0>,\n",
            "            'policy_states_are_swappable': False,\n",
            "            'postprocess_inputs': False,\n",
            "            'preprocessor_pref': 'deepmind',\n",
            "            'recreate_failed_workers': False,\n",
            "            'remote_env_batch_wait_ms': 0,\n",
            "            'remote_worker_envs': False,\n",
            "            'render_env': False,\n",
            "            'replay_sequence_length': None,\n",
            "            'restart_failed_sub_environments': False,\n",
            "            'rl_module_spec': None,\n",
            "            'rollout_fragment_length': 200,\n",
            "            'sample_async': False,\n",
            "            'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
            "            'sampler_perf_stats_ema_coef': None,\n",
            "            'seed': None,\n",
            "            'sgd_minibatch_size': 256,\n",
            "            'shuffle_buffer_size': 0,\n",
            "            'shuffle_sequences': True,\n",
            "            'simple_optimizer': False,\n",
            "            'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
            "            'synchronize_filters': -1,\n",
            "            'tf_session_args': {'allow_soft_placement': True,\n",
            "                                'device_count': {'CPU': 1},\n",
            "                                'gpu_options': {'allow_growth': True},\n",
            "                                'inter_op_parallelism_threads': 2,\n",
            "                                'intra_op_parallelism_threads': 2,\n",
            "                                'log_device_placement': False},\n",
            "            'torch_compile_learner': False,\n",
            "            'torch_compile_learner_dynamo_backend': 'inductor',\n",
            "            'torch_compile_learner_dynamo_mode': None,\n",
            "            'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
            "            'torch_compile_worker': False,\n",
            "            'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
            "            'torch_compile_worker_dynamo_mode': None,\n",
            "            'train_batch_size': 4000,\n",
            "            'update_worker_filter_stats': True,\n",
            "            'use_critic': True,\n",
            "            'use_gae': True,\n",
            "            'use_kl_loss': True,\n",
            "            'use_worker_filter_stats': True,\n",
            "            'validate_workers_after_construction': True,\n",
            "            'vf_clip_param': 10.0,\n",
            "            'vf_loss_coeff': 1.0,\n",
            "            'vf_share_layers': -1,\n",
            "            'worker_cls': -1,\n",
            "            'worker_health_probe_timeout_s': 60,\n",
            "            'worker_restore_timeout_s': 1800},\n",
            " 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.011071443557739258,\n",
            "                       'StateBufferConnector_ms': 0.006661891937255859,\n",
            "                       'ViewRequirementAgentConnector_ms': 0.20413517951965332},\n",
            " 'counters': {'num_agent_steps_sampled': 16000,\n",
            "              'num_agent_steps_trained': 16000,\n",
            "              'num_env_steps_sampled': 16000,\n",
            "              'num_env_steps_trained': 16000},\n",
            " 'custom_metrics': {},\n",
            " 'date': '2023-09-03_14-04-35',\n",
            " 'done': False,\n",
            " 'episode_len_mean': 89.72,\n",
            " 'episode_media': {},\n",
            " 'episode_reward_max': 479.80000000000035,\n",
            " 'episode_reward_mean': 47.86600000000001,\n",
            " 'episode_reward_min': 6.6,\n",
            " 'episodes_this_iter': 19,\n",
            " 'episodes_total': 185,\n",
            " 'hist_stats': {'episode_lengths': [51,\n",
            "                                    35,\n",
            "                                    125,\n",
            "                                    45,\n",
            "                                    103,\n",
            "                                    63,\n",
            "                                    55,\n",
            "                                    73,\n",
            "                                    57,\n",
            "                                    19,\n",
            "                                    167,\n",
            "                                    41,\n",
            "                                    47,\n",
            "                                    319,\n",
            "                                    65,\n",
            "                                    113,\n",
            "                                    43,\n",
            "                                    27,\n",
            "                                    85,\n",
            "                                    41,\n",
            "                                    47,\n",
            "                                    41,\n",
            "                                    189,\n",
            "                                    79,\n",
            "                                    99,\n",
            "                                    29,\n",
            "                                    55,\n",
            "                                    127,\n",
            "                                    39,\n",
            "                                    21,\n",
            "                                    49,\n",
            "                                    59,\n",
            "                                    113,\n",
            "                                    75,\n",
            "                                    21,\n",
            "                                    33,\n",
            "                                    39,\n",
            "                                    35,\n",
            "                                    65,\n",
            "                                    25,\n",
            "                                    31,\n",
            "                                    65,\n",
            "                                    151,\n",
            "                                    79,\n",
            "                                    35,\n",
            "                                    47,\n",
            "                                    109,\n",
            "                                    185,\n",
            "                                    43,\n",
            "                                    17,\n",
            "                                    87,\n",
            "                                    39,\n",
            "                                    79,\n",
            "                                    33,\n",
            "                                    31,\n",
            "                                    35,\n",
            "                                    59,\n",
            "                                    99,\n",
            "                                    29,\n",
            "                                    29,\n",
            "                                    41,\n",
            "                                    61,\n",
            "                                    59,\n",
            "                                    71,\n",
            "                                    65,\n",
            "                                    23,\n",
            "                                    25,\n",
            "                                    35,\n",
            "                                    29,\n",
            "                                    69,\n",
            "                                    35,\n",
            "                                    49,\n",
            "                                    27,\n",
            "                                    35,\n",
            "                                    29,\n",
            "                                    61,\n",
            "                                    27,\n",
            "                                    11,\n",
            "                                    43,\n",
            "                                    55,\n",
            "                                    37,\n",
            "                                    71,\n",
            "                                    37,\n",
            "                                    227,\n",
            "                                    47,\n",
            "                                    363,\n",
            "                                    117,\n",
            "                                    475,\n",
            "                                    237,\n",
            "                                    159,\n",
            "                                    155,\n",
            "                                    245,\n",
            "                                    105,\n",
            "                                    167,\n",
            "                                    75,\n",
            "                                    67,\n",
            "                                    103,\n",
            "                                    385,\n",
            "                                    159,\n",
            "                                    795],\n",
            "                'episode_reward': [8.4,\n",
            "                                   14.200000000000001,\n",
            "                                   17.5,\n",
            "                                   28.9,\n",
            "                                   51.600000000000016,\n",
            "                                   18.000000000000004,\n",
            "                                   7.399999999999999,\n",
            "                                   16.5,\n",
            "                                   35.3,\n",
            "                                   10.999999999999998,\n",
            "                                   65.80000000000001,\n",
            "                                   12.900000000000002,\n",
            "                                   11.4,\n",
            "                                   164.19999999999996,\n",
            "                                   35.49999999999999,\n",
            "                                   63.7,\n",
            "                                   15.000000000000002,\n",
            "                                   12.6,\n",
            "                                   46.099999999999994,\n",
            "                                   20.900000000000002,\n",
            "                                   20.6,\n",
            "                                   19.699999999999996,\n",
            "                                   100.30000000000004,\n",
            "                                   42.199999999999996,\n",
            "                                   58.40000000000003,\n",
            "                                   19.5,\n",
            "                                   32.99999999999999,\n",
            "                                   81.80000000000001,\n",
            "                                   28.799999999999997,\n",
            "                                   14.5,\n",
            "                                   18.100000000000005,\n",
            "                                   31.999999999999996,\n",
            "                                   42.7,\n",
            "                                   34.6,\n",
            "                                   9.899999999999999,\n",
            "                                   22.099999999999994,\n",
            "                                   21.999999999999996,\n",
            "                                   14.2,\n",
            "                                   32.3,\n",
            "                                   10.7,\n",
            "                                   18.2,\n",
            "                                   39.49999999999999,\n",
            "                                   80.59999999999997,\n",
            "                                   32.4,\n",
            "                                   21.6,\n",
            "                                   22.2,\n",
            "                                   56.099999999999994,\n",
            "                                   97.50000000000004,\n",
            "                                   24.2,\n",
            "                                   11.5,\n",
            "                                   50.40000000000001,\n",
            "                                   17.599999999999998,\n",
            "                                   42.20000000000001,\n",
            "                                   16.7,\n",
            "                                   13.600000000000003,\n",
            "                                   17.2,\n",
            "                                   31.400000000000002,\n",
            "                                   58.99999999999999,\n",
            "                                   14.299999999999999,\n",
            "                                   14.9,\n",
            "                                   21.900000000000002,\n",
            "                                   22.5,\n",
            "                                   24.8,\n",
            "                                   35.4,\n",
            "                                   46.49999999999998,\n",
            "                                   14.800000000000002,\n",
            "                                   13.3,\n",
            "                                   8.4,\n",
            "                                   18.099999999999998,\n",
            "                                   34.699999999999996,\n",
            "                                   16.0,\n",
            "                                   24.900000000000006,\n",
            "                                   15.0,\n",
            "                                   19.2,\n",
            "                                   13.3,\n",
            "                                   26.9,\n",
            "                                   16.600000000000005,\n",
            "                                   6.6,\n",
            "                                   22.0,\n",
            "                                   28.200000000000003,\n",
            "                                   20.1,\n",
            "                                   43.59999999999999,\n",
            "                                   19.7,\n",
            "                                   132.59999999999994,\n",
            "                                   26.800000000000004,\n",
            "                                   223.39999999999978,\n",
            "                                   58.30000000000001,\n",
            "                                   271.7999999999999,\n",
            "                                   141.70000000000005,\n",
            "                                   96.39999999999999,\n",
            "                                   97.80000000000001,\n",
            "                                   144.9,\n",
            "                                   55.50000000000001,\n",
            "                                   92.0,\n",
            "                                   45.600000000000016,\n",
            "                                   41.800000000000004,\n",
            "                                   52.599999999999966,\n",
            "                                   221.89999999999998,\n",
            "                                   95.80000000000003,\n",
            "                                   479.80000000000035]},\n",
            " 'hostname': '9e348d898222',\n",
            " 'info': {'learner': {'default_policy': {'custom_metrics': {},\n",
            "                                         'diff_num_grad_updates_vs_sampler_policy': 149.5,\n",
            "                                         'learner_stats': {'allreduce_latency': 0.0,\n",
            "                                                           'cur_kl_coeff': 0.19999999999999998,\n",
            "                                                           'cur_lr': 0.0003,\n",
            "                                                           'entropy': 0.630328081647555,\n",
            "                                                           'entropy_coeff': 0.0,\n",
            "                                                           'grad_gnorm': 10.309274217188358,\n",
            "                                                           'kl': 0.007427633518238405,\n",
            "                                                           'policy_loss': -0.026263168522467215,\n",
            "                                                           'total_loss': 5.012276553312938,\n",
            "                                                           'vf_explained_var': -0.05847146153450012,\n",
            "                                                           'vf_loss': 5.037054195404052},\n",
            "                                         'model': {},\n",
            "                                         'num_agent_steps_trained': 256.0,\n",
            "                                         'num_grad_updates_lifetime': 1050.5}},\n",
            "          'num_agent_steps_sampled': 16000,\n",
            "          'num_agent_steps_trained': 16000,\n",
            "          'num_env_steps_sampled': 16000,\n",
            "          'num_env_steps_trained': 16000},\n",
            " 'iterations_since_restore': 4,\n",
            " 'node_ip': '172.28.0.12',\n",
            " 'num_agent_steps_sampled': 16000,\n",
            " 'num_agent_steps_trained': 16000,\n",
            " 'num_env_steps_sampled': 16000,\n",
            " 'num_env_steps_sampled_this_iter': 4000,\n",
            " 'num_env_steps_sampled_throughput_per_sec': 278.68775884638274,\n",
            " 'num_env_steps_trained': 16000,\n",
            " 'num_env_steps_trained_this_iter': 4000,\n",
            " 'num_env_steps_trained_throughput_per_sec': 278.68775884638274,\n",
            " 'num_faulty_episodes': 0,\n",
            " 'num_healthy_workers': 1,\n",
            " 'num_in_flight_async_reqs': 0,\n",
            " 'num_remote_worker_restarts': 0,\n",
            " 'num_steps_trained_this_iter': 4000,\n",
            " 'perf': {'cpu_util_percent': 75.1952380952381,\n",
            "          'ram_util_percent': 18.461904761904762},\n",
            " 'pid': 401,\n",
            " 'policy_reward_max': {},\n",
            " 'policy_reward_mean': {},\n",
            " 'policy_reward_min': {},\n",
            " 'sampler_perf': {'mean_action_processing_ms': 0.21844305928543817,\n",
            "                  'mean_env_render_ms': 0.0,\n",
            "                  'mean_env_wait_ms': 0.06510801869866098,\n",
            "                  'mean_inference_ms': 1.8005513702062559,\n",
            "                  'mean_raw_obs_processing_ms': 0.5423624273176176},\n",
            " 'sampler_results': {'connector_metrics': {'ObsPreprocessorConnector_ms': 0.011071443557739258,\n",
            "                                           'StateBufferConnector_ms': 0.006661891937255859,\n",
            "                                           'ViewRequirementAgentConnector_ms': 0.20413517951965332},\n",
            "                     'custom_metrics': {},\n",
            "                     'episode_len_mean': 89.72,\n",
            "                     'episode_media': {},\n",
            "                     'episode_reward_max': 479.80000000000035,\n",
            "                     'episode_reward_mean': 47.86600000000001,\n",
            "                     'episode_reward_min': 6.6,\n",
            "                     'episodes_this_iter': 19,\n",
            "                     'hist_stats': {'episode_lengths': [51,\n",
            "                                                        35,\n",
            "                                                        125,\n",
            "                                                        45,\n",
            "                                                        103,\n",
            "                                                        63,\n",
            "                                                        55,\n",
            "                                                        73,\n",
            "                                                        57,\n",
            "                                                        19,\n",
            "                                                        167,\n",
            "                                                        41,\n",
            "                                                        47,\n",
            "                                                        319,\n",
            "                                                        65,\n",
            "                                                        113,\n",
            "                                                        43,\n",
            "                                                        27,\n",
            "                                                        85,\n",
            "                                                        41,\n",
            "                                                        47,\n",
            "                                                        41,\n",
            "                                                        189,\n",
            "                                                        79,\n",
            "                                                        99,\n",
            "                                                        29,\n",
            "                                                        55,\n",
            "                                                        127,\n",
            "                                                        39,\n",
            "                                                        21,\n",
            "                                                        49,\n",
            "                                                        59,\n",
            "                                                        113,\n",
            "                                                        75,\n",
            "                                                        21,\n",
            "                                                        33,\n",
            "                                                        39,\n",
            "                                                        35,\n",
            "                                                        65,\n",
            "                                                        25,\n",
            "                                                        31,\n",
            "                                                        65,\n",
            "                                                        151,\n",
            "                                                        79,\n",
            "                                                        35,\n",
            "                                                        47,\n",
            "                                                        109,\n",
            "                                                        185,\n",
            "                                                        43,\n",
            "                                                        17,\n",
            "                                                        87,\n",
            "                                                        39,\n",
            "                                                        79,\n",
            "                                                        33,\n",
            "                                                        31,\n",
            "                                                        35,\n",
            "                                                        59,\n",
            "                                                        99,\n",
            "                                                        29,\n",
            "                                                        29,\n",
            "                                                        41,\n",
            "                                                        61,\n",
            "                                                        59,\n",
            "                                                        71,\n",
            "                                                        65,\n",
            "                                                        23,\n",
            "                                                        25,\n",
            "                                                        35,\n",
            "                                                        29,\n",
            "                                                        69,\n",
            "                                                        35,\n",
            "                                                        49,\n",
            "                                                        27,\n",
            "                                                        35,\n",
            "                                                        29,\n",
            "                                                        61,\n",
            "                                                        27,\n",
            "                                                        11,\n",
            "                                                        43,\n",
            "                                                        55,\n",
            "                                                        37,\n",
            "                                                        71,\n",
            "                                                        37,\n",
            "                                                        227,\n",
            "                                                        47,\n",
            "                                                        363,\n",
            "                                                        117,\n",
            "                                                        475,\n",
            "                                                        237,\n",
            "                                                        159,\n",
            "                                                        155,\n",
            "                                                        245,\n",
            "                                                        105,\n",
            "                                                        167,\n",
            "                                                        75,\n",
            "                                                        67,\n",
            "                                                        103,\n",
            "                                                        385,\n",
            "                                                        159,\n",
            "                                                        795],\n",
            "                                    'episode_reward': [8.4,\n",
            "                                                       14.200000000000001,\n",
            "                                                       17.5,\n",
            "                                                       28.9,\n",
            "                                                       51.600000000000016,\n",
            "                                                       18.000000000000004,\n",
            "                                                       7.399999999999999,\n",
            "                                                       16.5,\n",
            "                                                       35.3,\n",
            "                                                       10.999999999999998,\n",
            "                                                       65.80000000000001,\n",
            "                                                       12.900000000000002,\n",
            "                                                       11.4,\n",
            "                                                       164.19999999999996,\n",
            "                                                       35.49999999999999,\n",
            "                                                       63.7,\n",
            "                                                       15.000000000000002,\n",
            "                                                       12.6,\n",
            "                                                       46.099999999999994,\n",
            "                                                       20.900000000000002,\n",
            "                                                       20.6,\n",
            "                                                       19.699999999999996,\n",
            "                                                       100.30000000000004,\n",
            "                                                       42.199999999999996,\n",
            "                                                       58.40000000000003,\n",
            "                                                       19.5,\n",
            "                                                       32.99999999999999,\n",
            "                                                       81.80000000000001,\n",
            "                                                       28.799999999999997,\n",
            "                                                       14.5,\n",
            "                                                       18.100000000000005,\n",
            "                                                       31.999999999999996,\n",
            "                                                       42.7,\n",
            "                                                       34.6,\n",
            "                                                       9.899999999999999,\n",
            "                                                       22.099999999999994,\n",
            "                                                       21.999999999999996,\n",
            "                                                       14.2,\n",
            "                                                       32.3,\n",
            "                                                       10.7,\n",
            "                                                       18.2,\n",
            "                                                       39.49999999999999,\n",
            "                                                       80.59999999999997,\n",
            "                                                       32.4,\n",
            "                                                       21.6,\n",
            "                                                       22.2,\n",
            "                                                       56.099999999999994,\n",
            "                                                       97.50000000000004,\n",
            "                                                       24.2,\n",
            "                                                       11.5,\n",
            "                                                       50.40000000000001,\n",
            "                                                       17.599999999999998,\n",
            "                                                       42.20000000000001,\n",
            "                                                       16.7,\n",
            "                                                       13.600000000000003,\n",
            "                                                       17.2,\n",
            "                                                       31.400000000000002,\n",
            "                                                       58.99999999999999,\n",
            "                                                       14.299999999999999,\n",
            "                                                       14.9,\n",
            "                                                       21.900000000000002,\n",
            "                                                       22.5,\n",
            "                                                       24.8,\n",
            "                                                       35.4,\n",
            "                                                       46.49999999999998,\n",
            "                                                       14.800000000000002,\n",
            "                                                       13.3,\n",
            "                                                       8.4,\n",
            "                                                       18.099999999999998,\n",
            "                                                       34.699999999999996,\n",
            "                                                       16.0,\n",
            "                                                       24.900000000000006,\n",
            "                                                       15.0,\n",
            "                                                       19.2,\n",
            "                                                       13.3,\n",
            "                                                       26.9,\n",
            "                                                       16.600000000000005,\n",
            "                                                       6.6,\n",
            "                                                       22.0,\n",
            "                                                       28.200000000000003,\n",
            "                                                       20.1,\n",
            "                                                       43.59999999999999,\n",
            "                                                       19.7,\n",
            "                                                       132.59999999999994,\n",
            "                                                       26.800000000000004,\n",
            "                                                       223.39999999999978,\n",
            "                                                       58.30000000000001,\n",
            "                                                       271.7999999999999,\n",
            "                                                       141.70000000000005,\n",
            "                                                       96.39999999999999,\n",
            "                                                       97.80000000000001,\n",
            "                                                       144.9,\n",
            "                                                       55.50000000000001,\n",
            "                                                       92.0,\n",
            "                                                       45.600000000000016,\n",
            "                                                       41.800000000000004,\n",
            "                                                       52.599999999999966,\n",
            "                                                       221.89999999999998,\n",
            "                                                       95.80000000000003,\n",
            "                                                       479.80000000000035]},\n",
            "                     'num_faulty_episodes': 0,\n",
            "                     'policy_reward_max': {},\n",
            "                     'policy_reward_mean': {},\n",
            "                     'policy_reward_min': {},\n",
            "                     'sampler_perf': {'mean_action_processing_ms': 0.21844305928543817,\n",
            "                                      'mean_env_render_ms': 0.0,\n",
            "                                      'mean_env_wait_ms': 0.06510801869866098,\n",
            "                                      'mean_inference_ms': 1.8005513702062559,\n",
            "                                      'mean_raw_obs_processing_ms': 0.5423624273176176}},\n",
            " 'time_since_restore': 59.07635974884033,\n",
            " 'time_this_iter_s': 14.360156297683716,\n",
            " 'time_total_s': 59.07635974884033,\n",
            " 'timers': {'learn_throughput': 943.07,\n",
            "            'learn_time_ms': 4241.467,\n",
            "            'load_throughput': 5182952.116,\n",
            "            'load_time_ms': 0.772,\n",
            "            'sample_time_ms': 10513.58,\n",
            "            'synch_weights_time_ms': 3.475,\n",
            "            'training_iteration_time_ms': 14761.184},\n",
            " 'timestamp': 1693749875,\n",
            " 'timesteps_total': 16000,\n",
            " 'training_iteration': 4,\n",
            " 'trial_id': 'default'}\n",
            "****************************************************************************************************************************************\n",
            "{'agent_timesteps_total': 20000,\n",
            " 'config': {'_AlgorithmConfig__prior_exploration_config': None,\n",
            "            '_disable_action_flattening': False,\n",
            "            '_disable_execution_plan_api': True,\n",
            "            '_disable_initialize_loss_from_dummy_batch': False,\n",
            "            '_disable_preprocessor_api': False,\n",
            "            '_enable_learner_api': False,\n",
            "            '_enable_rl_module_api': False,\n",
            "            '_fake_gpus': False,\n",
            "            '_is_atari': None,\n",
            "            '_learner_class': None,\n",
            "            '_tf_policy_handles_more_than_one_loss': False,\n",
            "            'action_mask_key': 'action_mask',\n",
            "            'action_space': None,\n",
            "            'actions_in_input_normalized': False,\n",
            "            'algorithm_config_overrides_per_module': {},\n",
            "            'always_attach_evaluation_results': False,\n",
            "            'auto_wrap_old_gym_envs': True,\n",
            "            'batch_mode': 'truncate_episodes',\n",
            "            'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>,\n",
            "            'checkpoint_trainable_policies_only': False,\n",
            "            'clip_actions': False,\n",
            "            'clip_param': 0.2,\n",
            "            'clip_rewards': None,\n",
            "            'compress_observations': False,\n",
            "            'count_steps_by': 'env_steps',\n",
            "            'create_env_on_driver': False,\n",
            "            'custom_eval_function': None,\n",
            "            'custom_resources_per_worker': {},\n",
            "            'delay_between_worker_restarts_s': 60.0,\n",
            "            'disable_env_checking': False,\n",
            "            'eager_max_retraces': 20,\n",
            "            'eager_tracing': True,\n",
            "            'enable_async_evaluation': False,\n",
            "            'enable_connectors': True,\n",
            "            'enable_tf1_exec_eagerly': False,\n",
            "            'entropy_coeff': 0.0,\n",
            "            'entropy_coeff_schedule': None,\n",
            "            'env': 'my',\n",
            "            'env_config': {},\n",
            "            'env_runner_cls': None,\n",
            "            'env_task_fn': None,\n",
            "            'evaluation_config': None,\n",
            "            'evaluation_duration': 10,\n",
            "            'evaluation_duration_unit': 'episodes',\n",
            "            'evaluation_interval': None,\n",
            "            'evaluation_num_workers': 0,\n",
            "            'evaluation_parallel_to_training': False,\n",
            "            'evaluation_sample_timeout_s': 180.0,\n",
            "            'exploration_config': {'type': 'StochasticSampling'},\n",
            "            'explore': True,\n",
            "            'export_native_model_files': True,\n",
            "            'extra_python_environs_for_driver': {},\n",
            "            'extra_python_environs_for_worker': {},\n",
            "            'fake_sampler': False,\n",
            "            'framework': 'torch',\n",
            "            'gamma': 0.99,\n",
            "            'grad_clip': None,\n",
            "            'grad_clip_by': 'global_norm',\n",
            "            'ignore_worker_failures': False,\n",
            "            'in_evaluation': False,\n",
            "            'input': 'sampler',\n",
            "            'input_config': {},\n",
            "            'keep_per_episode_custom_metrics': False,\n",
            "            'kl_coeff': 0.2,\n",
            "            'kl_target': 0.01,\n",
            "            'lambda': 0.95,\n",
            "            'local_gpu_idx': 0,\n",
            "            'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
            "                                      'intra_op_parallelism_threads': 8},\n",
            "            'log_level': 'WARN',\n",
            "            'log_sys_usage': True,\n",
            "            'logger_config': None,\n",
            "            'logger_creator': None,\n",
            "            'lr': 0.0003,\n",
            "            'lr_schedule': None,\n",
            "            'max_num_worker_restarts': 1000,\n",
            "            'max_requests_in_flight_per_sampler_worker': 2,\n",
            "            'metrics_episode_collection_timeout_s': 60.0,\n",
            "            'metrics_num_episodes_for_smoothing': 100,\n",
            "            'min_sample_timesteps_per_iteration': 0,\n",
            "            'min_time_s_per_iteration': None,\n",
            "            'min_train_timesteps_per_iteration': 0,\n",
            "            'model': {'_disable_action_flattening': False,\n",
            "                      '_disable_preprocessor_api': False,\n",
            "                      '_time_major': False,\n",
            "                      '_use_default_native_models': -1,\n",
            "                      'always_check_shapes': False,\n",
            "                      'attention_dim': 64,\n",
            "                      'attention_head_dim': 32,\n",
            "                      'attention_init_gru_gate_bias': 2.0,\n",
            "                      'attention_memory_inference': 50,\n",
            "                      'attention_memory_training': 50,\n",
            "                      'attention_num_heads': 1,\n",
            "                      'attention_num_transformer_units': 1,\n",
            "                      'attention_position_wise_mlp_dim': 32,\n",
            "                      'attention_use_n_prev_actions': 0,\n",
            "                      'attention_use_n_prev_rewards': 0,\n",
            "                      'conv_activation': 'relu',\n",
            "                      'conv_filters': None,\n",
            "                      'custom_action_dist': None,\n",
            "                      'custom_model': None,\n",
            "                      'custom_model_config': {},\n",
            "                      'custom_preprocessor': None,\n",
            "                      'dim': 84,\n",
            "                      'encoder_latent_dim': None,\n",
            "                      'fcnet_activation': 'tanh',\n",
            "                      'fcnet_hiddens': [256, 256],\n",
            "                      'framestack': True,\n",
            "                      'free_log_std': False,\n",
            "                      'grayscale': False,\n",
            "                      'lstm_cell_size': 256,\n",
            "                      'lstm_use_prev_action': False,\n",
            "                      'lstm_use_prev_action_reward': -1,\n",
            "                      'lstm_use_prev_reward': False,\n",
            "                      'max_seq_len': 20,\n",
            "                      'no_final_linear': False,\n",
            "                      'post_fcnet_activation': 'relu',\n",
            "                      'post_fcnet_hiddens': [],\n",
            "                      'use_attention': False,\n",
            "                      'use_lstm': False,\n",
            "                      'vf_share_layers': False,\n",
            "                      'zero_mean': True},\n",
            "            'normalize_actions': True,\n",
            "            'num_consecutive_worker_failures_tolerance': 100,\n",
            "            'num_cpus_for_driver': 1,\n",
            "            'num_cpus_per_learner_worker': 1,\n",
            "            'num_cpus_per_worker': 1,\n",
            "            'num_envs_per_worker': 1,\n",
            "            'num_gpus': 0,\n",
            "            'num_gpus_per_learner_worker': 0,\n",
            "            'num_gpus_per_worker': 0,\n",
            "            'num_learner_workers': 0,\n",
            "            'num_sgd_iter': 20,\n",
            "            'num_workers': 1,\n",
            "            'observation_filter': 'NoFilter',\n",
            "            'observation_fn': None,\n",
            "            'observation_space': None,\n",
            "            'off_policy_estimation_methods': {},\n",
            "            'offline_sampling': False,\n",
            "            'ope_split_batch_by_episode': True,\n",
            "            'optimizer': {},\n",
            "            'output': None,\n",
            "            'output_compress_columns': ['obs', 'new_obs'],\n",
            "            'output_config': {},\n",
            "            'output_max_file_size': 67108864,\n",
            "            'placement_strategy': 'PACK',\n",
            "            'policies': {'default_policy': (None, None, None, None)},\n",
            "            'policies_to_train': None,\n",
            "            'policy_map_cache': -1,\n",
            "            'policy_map_capacity': 100,\n",
            "            'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x78f59bc564d0>,\n",
            "            'policy_states_are_swappable': False,\n",
            "            'postprocess_inputs': False,\n",
            "            'preprocessor_pref': 'deepmind',\n",
            "            'recreate_failed_workers': False,\n",
            "            'remote_env_batch_wait_ms': 0,\n",
            "            'remote_worker_envs': False,\n",
            "            'render_env': False,\n",
            "            'replay_sequence_length': None,\n",
            "            'restart_failed_sub_environments': False,\n",
            "            'rl_module_spec': None,\n",
            "            'rollout_fragment_length': 200,\n",
            "            'sample_async': False,\n",
            "            'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
            "            'sampler_perf_stats_ema_coef': None,\n",
            "            'seed': None,\n",
            "            'sgd_minibatch_size': 256,\n",
            "            'shuffle_buffer_size': 0,\n",
            "            'shuffle_sequences': True,\n",
            "            'simple_optimizer': False,\n",
            "            'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
            "            'synchronize_filters': -1,\n",
            "            'tf_session_args': {'allow_soft_placement': True,\n",
            "                                'device_count': {'CPU': 1},\n",
            "                                'gpu_options': {'allow_growth': True},\n",
            "                                'inter_op_parallelism_threads': 2,\n",
            "                                'intra_op_parallelism_threads': 2,\n",
            "                                'log_device_placement': False},\n",
            "            'torch_compile_learner': False,\n",
            "            'torch_compile_learner_dynamo_backend': 'inductor',\n",
            "            'torch_compile_learner_dynamo_mode': None,\n",
            "            'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
            "            'torch_compile_worker': False,\n",
            "            'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
            "            'torch_compile_worker_dynamo_mode': None,\n",
            "            'train_batch_size': 4000,\n",
            "            'update_worker_filter_stats': True,\n",
            "            'use_critic': True,\n",
            "            'use_gae': True,\n",
            "            'use_kl_loss': True,\n",
            "            'use_worker_filter_stats': True,\n",
            "            'validate_workers_after_construction': True,\n",
            "            'vf_clip_param': 10.0,\n",
            "            'vf_loss_coeff': 1.0,\n",
            "            'vf_share_layers': -1,\n",
            "            'worker_cls': -1,\n",
            "            'worker_health_probe_timeout_s': 60,\n",
            "            'worker_restore_timeout_s': 1800},\n",
            " 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.011839151382446289,\n",
            "                       'StateBufferConnector_ms': 0.010642528533935547,\n",
            "                       'ViewRequirementAgentConnector_ms': 0.20080041885375977},\n",
            " 'counters': {'num_agent_steps_sampled': 20000,\n",
            "              'num_agent_steps_trained': 20000,\n",
            "              'num_env_steps_sampled': 20000,\n",
            "              'num_env_steps_trained': 20000},\n",
            " 'custom_metrics': {},\n",
            " 'date': '2023-09-03_14-04-51',\n",
            " 'done': False,\n",
            " 'episode_len_mean': 111.18,\n",
            " 'episode_media': {},\n",
            " 'episode_reward_max': 479.80000000000035,\n",
            " 'episode_reward_mean': 64.33899999999998,\n",
            " 'episode_reward_min': 6.6,\n",
            " 'episodes_this_iter': 23,\n",
            " 'episodes_total': 208,\n",
            " 'hist_stats': {'episode_lengths': [79,\n",
            "                                    99,\n",
            "                                    29,\n",
            "                                    55,\n",
            "                                    127,\n",
            "                                    39,\n",
            "                                    21,\n",
            "                                    49,\n",
            "                                    59,\n",
            "                                    113,\n",
            "                                    75,\n",
            "                                    21,\n",
            "                                    33,\n",
            "                                    39,\n",
            "                                    35,\n",
            "                                    65,\n",
            "                                    25,\n",
            "                                    31,\n",
            "                                    65,\n",
            "                                    151,\n",
            "                                    79,\n",
            "                                    35,\n",
            "                                    47,\n",
            "                                    109,\n",
            "                                    185,\n",
            "                                    43,\n",
            "                                    17,\n",
            "                                    87,\n",
            "                                    39,\n",
            "                                    79,\n",
            "                                    33,\n",
            "                                    31,\n",
            "                                    35,\n",
            "                                    59,\n",
            "                                    99,\n",
            "                                    29,\n",
            "                                    29,\n",
            "                                    41,\n",
            "                                    61,\n",
            "                                    59,\n",
            "                                    71,\n",
            "                                    65,\n",
            "                                    23,\n",
            "                                    25,\n",
            "                                    35,\n",
            "                                    29,\n",
            "                                    69,\n",
            "                                    35,\n",
            "                                    49,\n",
            "                                    27,\n",
            "                                    35,\n",
            "                                    29,\n",
            "                                    61,\n",
            "                                    27,\n",
            "                                    11,\n",
            "                                    43,\n",
            "                                    55,\n",
            "                                    37,\n",
            "                                    71,\n",
            "                                    37,\n",
            "                                    227,\n",
            "                                    47,\n",
            "                                    363,\n",
            "                                    117,\n",
            "                                    475,\n",
            "                                    237,\n",
            "                                    159,\n",
            "                                    155,\n",
            "                                    245,\n",
            "                                    105,\n",
            "                                    167,\n",
            "                                    75,\n",
            "                                    67,\n",
            "                                    103,\n",
            "                                    385,\n",
            "                                    159,\n",
            "                                    795,\n",
            "                                    47,\n",
            "                                    31,\n",
            "                                    365,\n",
            "                                    371,\n",
            "                                    49,\n",
            "                                    139,\n",
            "                                    373,\n",
            "                                    201,\n",
            "                                    275,\n",
            "                                    115,\n",
            "                                    267,\n",
            "                                    167,\n",
            "                                    37,\n",
            "                                    325,\n",
            "                                    59,\n",
            "                                    97,\n",
            "                                    105,\n",
            "                                    69,\n",
            "                                    75,\n",
            "                                    201,\n",
            "                                    219,\n",
            "                                    243,\n",
            "                                    167],\n",
            "                'episode_reward': [42.199999999999996,\n",
            "                                   58.40000000000003,\n",
            "                                   19.5,\n",
            "                                   32.99999999999999,\n",
            "                                   81.80000000000001,\n",
            "                                   28.799999999999997,\n",
            "                                   14.5,\n",
            "                                   18.100000000000005,\n",
            "                                   31.999999999999996,\n",
            "                                   42.7,\n",
            "                                   34.6,\n",
            "                                   9.899999999999999,\n",
            "                                   22.099999999999994,\n",
            "                                   21.999999999999996,\n",
            "                                   14.2,\n",
            "                                   32.3,\n",
            "                                   10.7,\n",
            "                                   18.2,\n",
            "                                   39.49999999999999,\n",
            "                                   80.59999999999997,\n",
            "                                   32.4,\n",
            "                                   21.6,\n",
            "                                   22.2,\n",
            "                                   56.099999999999994,\n",
            "                                   97.50000000000004,\n",
            "                                   24.2,\n",
            "                                   11.5,\n",
            "                                   50.40000000000001,\n",
            "                                   17.599999999999998,\n",
            "                                   42.20000000000001,\n",
            "                                   16.7,\n",
            "                                   13.600000000000003,\n",
            "                                   17.2,\n",
            "                                   31.400000000000002,\n",
            "                                   58.99999999999999,\n",
            "                                   14.299999999999999,\n",
            "                                   14.9,\n",
            "                                   21.900000000000002,\n",
            "                                   22.5,\n",
            "                                   24.8,\n",
            "                                   35.4,\n",
            "                                   46.49999999999998,\n",
            "                                   14.800000000000002,\n",
            "                                   13.3,\n",
            "                                   8.4,\n",
            "                                   18.099999999999998,\n",
            "                                   34.699999999999996,\n",
            "                                   16.0,\n",
            "                                   24.900000000000006,\n",
            "                                   15.0,\n",
            "                                   19.2,\n",
            "                                   13.3,\n",
            "                                   26.9,\n",
            "                                   16.600000000000005,\n",
            "                                   6.6,\n",
            "                                   22.0,\n",
            "                                   28.200000000000003,\n",
            "                                   20.1,\n",
            "                                   43.59999999999999,\n",
            "                                   19.7,\n",
            "                                   132.59999999999994,\n",
            "                                   26.800000000000004,\n",
            "                                   223.39999999999978,\n",
            "                                   58.30000000000001,\n",
            "                                   271.7999999999999,\n",
            "                                   141.70000000000005,\n",
            "                                   96.39999999999999,\n",
            "                                   97.80000000000001,\n",
            "                                   144.9,\n",
            "                                   55.50000000000001,\n",
            "                                   92.0,\n",
            "                                   45.600000000000016,\n",
            "                                   41.800000000000004,\n",
            "                                   52.599999999999966,\n",
            "                                   221.89999999999998,\n",
            "                                   95.80000000000003,\n",
            "                                   479.80000000000035,\n",
            "                                   21.6,\n",
            "                                   15.399999999999999,\n",
            "                                   232.89999999999972,\n",
            "                                   239.59999999999988,\n",
            "                                   28.699999999999996,\n",
            "                                   86.20000000000005,\n",
            "                                   241.29999999999993,\n",
            "                                   119.3,\n",
            "                                   164.59999999999994,\n",
            "                                   70.60000000000002,\n",
            "                                   154.7999999999998,\n",
            "                                   93.6,\n",
            "                                   25.3,\n",
            "                                   202.29999999999993,\n",
            "                                   34.60000000000001,\n",
            "                                   51.7,\n",
            "                                   59.29999999999999,\n",
            "                                   45.300000000000004,\n",
            "                                   42.6,\n",
            "                                   131.09999999999997,\n",
            "                                   135.39999999999992,\n",
            "                                   150.19999999999993,\n",
            "                                   98.40000000000002]},\n",
            " 'hostname': '9e348d898222',\n",
            " 'info': {'learner': {'default_policy': {'custom_metrics': {},\n",
            "                                         'diff_num_grad_updates_vs_sampler_policy': 149.5,\n",
            "                                         'learner_stats': {'allreduce_latency': 0.0,\n",
            "                                                           'cur_kl_coeff': 0.19999999999999998,\n",
            "                                                           'cur_lr': 0.0003,\n",
            "                                                           'entropy': 0.6090752629439036,\n",
            "                                                           'entropy_coeff': 0.0,\n",
            "                                                           'grad_gnorm': 11.760108245710532,\n",
            "                                                           'kl': 0.005841211903720248,\n",
            "                                                           'policy_loss': -0.01890338133244465,\n",
            "                                                           'total_loss': 4.976656564076742,\n",
            "                                                           'vf_explained_var': -0.013909240961074829,\n",
            "                                                           'vf_loss': 4.994391705195109},\n",
            "                                         'model': {},\n",
            "                                         'num_agent_steps_trained': 256.0,\n",
            "                                         'num_grad_updates_lifetime': 1350.5}},\n",
            "          'num_agent_steps_sampled': 20000,\n",
            "          'num_agent_steps_trained': 20000,\n",
            "          'num_env_steps_sampled': 20000,\n",
            "          'num_env_steps_trained': 20000},\n",
            " 'iterations_since_restore': 5,\n",
            " 'node_ip': '172.28.0.12',\n",
            " 'num_agent_steps_sampled': 20000,\n",
            " 'num_agent_steps_trained': 20000,\n",
            " 'num_env_steps_sampled': 20000,\n",
            " 'num_env_steps_sampled_this_iter': 4000,\n",
            " 'num_env_steps_sampled_throughput_per_sec': 267.762575075138,\n",
            " 'num_env_steps_trained': 20000,\n",
            " 'num_env_steps_trained_this_iter': 4000,\n",
            " 'num_env_steps_trained_throughput_per_sec': 267.762575075138,\n",
            " 'num_faulty_episodes': 0,\n",
            " 'num_healthy_workers': 1,\n",
            " 'num_in_flight_async_reqs': 0,\n",
            " 'num_remote_worker_restarts': 0,\n",
            " 'num_steps_trained_this_iter': 4000,\n",
            " 'perf': {'cpu_util_percent': 75.42727272727274,\n",
            "          'ram_util_percent': 18.43181818181818},\n",
            " 'pid': 401,\n",
            " 'policy_reward_max': {},\n",
            " 'policy_reward_mean': {},\n",
            " 'policy_reward_min': {},\n",
            " 'sampler_perf': {'mean_action_processing_ms': 0.2150631534305134,\n",
            "                  'mean_env_render_ms': 0.0,\n",
            "                  'mean_env_wait_ms': 0.06406101729690131,\n",
            "                  'mean_inference_ms': 1.7739551059232497,\n",
            "                  'mean_raw_obs_processing_ms': 0.5334324930982773},\n",
            " 'sampler_results': {'connector_metrics': {'ObsPreprocessorConnector_ms': 0.011839151382446289,\n",
            "                                           'StateBufferConnector_ms': 0.010642528533935547,\n",
            "                                           'ViewRequirementAgentConnector_ms': 0.20080041885375977},\n",
            "                     'custom_metrics': {},\n",
            "                     'episode_len_mean': 111.18,\n",
            "                     'episode_media': {},\n",
            "                     'episode_reward_max': 479.80000000000035,\n",
            "                     'episode_reward_mean': 64.33899999999998,\n",
            "                     'episode_reward_min': 6.6,\n",
            "                     'episodes_this_iter': 23,\n",
            "                     'hist_stats': {'episode_lengths': [79,\n",
            "                                                        99,\n",
            "                                                        29,\n",
            "                                                        55,\n",
            "                                                        127,\n",
            "                                                        39,\n",
            "                                                        21,\n",
            "                                                        49,\n",
            "                                                        59,\n",
            "                                                        113,\n",
            "                                                        75,\n",
            "                                                        21,\n",
            "                                                        33,\n",
            "                                                        39,\n",
            "                                                        35,\n",
            "                                                        65,\n",
            "                                                        25,\n",
            "                                                        31,\n",
            "                                                        65,\n",
            "                                                        151,\n",
            "                                                        79,\n",
            "                                                        35,\n",
            "                                                        47,\n",
            "                                                        109,\n",
            "                                                        185,\n",
            "                                                        43,\n",
            "                                                        17,\n",
            "                                                        87,\n",
            "                                                        39,\n",
            "                                                        79,\n",
            "                                                        33,\n",
            "                                                        31,\n",
            "                                                        35,\n",
            "                                                        59,\n",
            "                                                        99,\n",
            "                                                        29,\n",
            "                                                        29,\n",
            "                                                        41,\n",
            "                                                        61,\n",
            "                                                        59,\n",
            "                                                        71,\n",
            "                                                        65,\n",
            "                                                        23,\n",
            "                                                        25,\n",
            "                                                        35,\n",
            "                                                        29,\n",
            "                                                        69,\n",
            "                                                        35,\n",
            "                                                        49,\n",
            "                                                        27,\n",
            "                                                        35,\n",
            "                                                        29,\n",
            "                                                        61,\n",
            "                                                        27,\n",
            "                                                        11,\n",
            "                                                        43,\n",
            "                                                        55,\n",
            "                                                        37,\n",
            "                                                        71,\n",
            "                                                        37,\n",
            "                                                        227,\n",
            "                                                        47,\n",
            "                                                        363,\n",
            "                                                        117,\n",
            "                                                        475,\n",
            "                                                        237,\n",
            "                                                        159,\n",
            "                                                        155,\n",
            "                                                        245,\n",
            "                                                        105,\n",
            "                                                        167,\n",
            "                                                        75,\n",
            "                                                        67,\n",
            "                                                        103,\n",
            "                                                        385,\n",
            "                                                        159,\n",
            "                                                        795,\n",
            "                                                        47,\n",
            "                                                        31,\n",
            "                                                        365,\n",
            "                                                        371,\n",
            "                                                        49,\n",
            "                                                        139,\n",
            "                                                        373,\n",
            "                                                        201,\n",
            "                                                        275,\n",
            "                                                        115,\n",
            "                                                        267,\n",
            "                                                        167,\n",
            "                                                        37,\n",
            "                                                        325,\n",
            "                                                        59,\n",
            "                                                        97,\n",
            "                                                        105,\n",
            "                                                        69,\n",
            "                                                        75,\n",
            "                                                        201,\n",
            "                                                        219,\n",
            "                                                        243,\n",
            "                                                        167],\n",
            "                                    'episode_reward': [42.199999999999996,\n",
            "                                                       58.40000000000003,\n",
            "                                                       19.5,\n",
            "                                                       32.99999999999999,\n",
            "                                                       81.80000000000001,\n",
            "                                                       28.799999999999997,\n",
            "                                                       14.5,\n",
            "                                                       18.100000000000005,\n",
            "                                                       31.999999999999996,\n",
            "                                                       42.7,\n",
            "                                                       34.6,\n",
            "                                                       9.899999999999999,\n",
            "                                                       22.099999999999994,\n",
            "                                                       21.999999999999996,\n",
            "                                                       14.2,\n",
            "                                                       32.3,\n",
            "                                                       10.7,\n",
            "                                                       18.2,\n",
            "                                                       39.49999999999999,\n",
            "                                                       80.59999999999997,\n",
            "                                                       32.4,\n",
            "                                                       21.6,\n",
            "                                                       22.2,\n",
            "                                                       56.099999999999994,\n",
            "                                                       97.50000000000004,\n",
            "                                                       24.2,\n",
            "                                                       11.5,\n",
            "                                                       50.40000000000001,\n",
            "                                                       17.599999999999998,\n",
            "                                                       42.20000000000001,\n",
            "                                                       16.7,\n",
            "                                                       13.600000000000003,\n",
            "                                                       17.2,\n",
            "                                                       31.400000000000002,\n",
            "                                                       58.99999999999999,\n",
            "                                                       14.299999999999999,\n",
            "                                                       14.9,\n",
            "                                                       21.900000000000002,\n",
            "                                                       22.5,\n",
            "                                                       24.8,\n",
            "                                                       35.4,\n",
            "                                                       46.49999999999998,\n",
            "                                                       14.800000000000002,\n",
            "                                                       13.3,\n",
            "                                                       8.4,\n",
            "                                                       18.099999999999998,\n",
            "                                                       34.699999999999996,\n",
            "                                                       16.0,\n",
            "                                                       24.900000000000006,\n",
            "                                                       15.0,\n",
            "                                                       19.2,\n",
            "                                                       13.3,\n",
            "                                                       26.9,\n",
            "                                                       16.600000000000005,\n",
            "                                                       6.6,\n",
            "                                                       22.0,\n",
            "                                                       28.200000000000003,\n",
            "                                                       20.1,\n",
            "                                                       43.59999999999999,\n",
            "                                                       19.7,\n",
            "                                                       132.59999999999994,\n",
            "                                                       26.800000000000004,\n",
            "                                                       223.39999999999978,\n",
            "                                                       58.30000000000001,\n",
            "                                                       271.7999999999999,\n",
            "                                                       141.70000000000005,\n",
            "                                                       96.39999999999999,\n",
            "                                                       97.80000000000001,\n",
            "                                                       144.9,\n",
            "                                                       55.50000000000001,\n",
            "                                                       92.0,\n",
            "                                                       45.600000000000016,\n",
            "                                                       41.800000000000004,\n",
            "                                                       52.599999999999966,\n",
            "                                                       221.89999999999998,\n",
            "                                                       95.80000000000003,\n",
            "                                                       479.80000000000035,\n",
            "                                                       21.6,\n",
            "                                                       15.399999999999999,\n",
            "                                                       232.89999999999972,\n",
            "                                                       239.59999999999988,\n",
            "                                                       28.699999999999996,\n",
            "                                                       86.20000000000005,\n",
            "                                                       241.29999999999993,\n",
            "                                                       119.3,\n",
            "                                                       164.59999999999994,\n",
            "                                                       70.60000000000002,\n",
            "                                                       154.7999999999998,\n",
            "                                                       93.6,\n",
            "                                                       25.3,\n",
            "                                                       202.29999999999993,\n",
            "                                                       34.60000000000001,\n",
            "                                                       51.7,\n",
            "                                                       59.29999999999999,\n",
            "                                                       45.300000000000004,\n",
            "                                                       42.6,\n",
            "                                                       131.09999999999997,\n",
            "                                                       135.39999999999992,\n",
            "                                                       150.19999999999993,\n",
            "                                                       98.40000000000002]},\n",
            "                     'num_faulty_episodes': 0,\n",
            "                     'policy_reward_max': {},\n",
            "                     'policy_reward_mean': {},\n",
            "                     'policy_reward_min': {},\n",
            "                     'sampler_perf': {'mean_action_processing_ms': 0.2150631534305134,\n",
            "                                      'mean_env_render_ms': 0.0,\n",
            "                                      'mean_env_wait_ms': 0.06406101729690131,\n",
            "                                      'mean_inference_ms': 1.7739551059232497,\n",
            "                                      'mean_raw_obs_processing_ms': 0.5334324930982773}},\n",
            " 'time_since_restore': 74.0225088596344,\n",
            " 'time_this_iter_s': 14.946149110794067,\n",
            " 'time_total_s': 74.0225088596344,\n",
            " 'timers': {'learn_throughput': 849.437,\n",
            "            'learn_time_ms': 4709.001,\n",
            "            'load_throughput': 5035481.121,\n",
            "            'load_time_ms': 0.794,\n",
            "            'sample_time_ms': 10081.758,\n",
            "            'synch_weights_time_ms': 3.377,\n",
            "            'training_iteration_time_ms': 14796.658},\n",
            " 'timestamp': 1693749891,\n",
            " 'timesteps_total': 20000,\n",
            " 'training_iteration': 5,\n",
            " 'trial_id': 'default'}\n",
            "****************************************************************************************************************************************\n",
            "{'agent_timesteps_total': 24000,\n",
            " 'config': {'_AlgorithmConfig__prior_exploration_config': None,\n",
            "            '_disable_action_flattening': False,\n",
            "            '_disable_execution_plan_api': True,\n",
            "            '_disable_initialize_loss_from_dummy_batch': False,\n",
            "            '_disable_preprocessor_api': False,\n",
            "            '_enable_learner_api': False,\n",
            "            '_enable_rl_module_api': False,\n",
            "            '_fake_gpus': False,\n",
            "            '_is_atari': None,\n",
            "            '_learner_class': None,\n",
            "            '_tf_policy_handles_more_than_one_loss': False,\n",
            "            'action_mask_key': 'action_mask',\n",
            "            'action_space': None,\n",
            "            'actions_in_input_normalized': False,\n",
            "            'algorithm_config_overrides_per_module': {},\n",
            "            'always_attach_evaluation_results': False,\n",
            "            'auto_wrap_old_gym_envs': True,\n",
            "            'batch_mode': 'truncate_episodes',\n",
            "            'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>,\n",
            "            'checkpoint_trainable_policies_only': False,\n",
            "            'clip_actions': False,\n",
            "            'clip_param': 0.2,\n",
            "            'clip_rewards': None,\n",
            "            'compress_observations': False,\n",
            "            'count_steps_by': 'env_steps',\n",
            "            'create_env_on_driver': False,\n",
            "            'custom_eval_function': None,\n",
            "            'custom_resources_per_worker': {},\n",
            "            'delay_between_worker_restarts_s': 60.0,\n",
            "            'disable_env_checking': False,\n",
            "            'eager_max_retraces': 20,\n",
            "            'eager_tracing': True,\n",
            "            'enable_async_evaluation': False,\n",
            "            'enable_connectors': True,\n",
            "            'enable_tf1_exec_eagerly': False,\n",
            "            'entropy_coeff': 0.0,\n",
            "            'entropy_coeff_schedule': None,\n",
            "            'env': 'my',\n",
            "            'env_config': {},\n",
            "            'env_runner_cls': None,\n",
            "            'env_task_fn': None,\n",
            "            'evaluation_config': None,\n",
            "            'evaluation_duration': 10,\n",
            "            'evaluation_duration_unit': 'episodes',\n",
            "            'evaluation_interval': None,\n",
            "            'evaluation_num_workers': 0,\n",
            "            'evaluation_parallel_to_training': False,\n",
            "            'evaluation_sample_timeout_s': 180.0,\n",
            "            'exploration_config': {'type': 'StochasticSampling'},\n",
            "            'explore': True,\n",
            "            'export_native_model_files': True,\n",
            "            'extra_python_environs_for_driver': {},\n",
            "            'extra_python_environs_for_worker': {},\n",
            "            'fake_sampler': False,\n",
            "            'framework': 'torch',\n",
            "            'gamma': 0.99,\n",
            "            'grad_clip': None,\n",
            "            'grad_clip_by': 'global_norm',\n",
            "            'ignore_worker_failures': False,\n",
            "            'in_evaluation': False,\n",
            "            'input': 'sampler',\n",
            "            'input_config': {},\n",
            "            'keep_per_episode_custom_metrics': False,\n",
            "            'kl_coeff': 0.2,\n",
            "            'kl_target': 0.01,\n",
            "            'lambda': 0.95,\n",
            "            'local_gpu_idx': 0,\n",
            "            'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
            "                                      'intra_op_parallelism_threads': 8},\n",
            "            'log_level': 'WARN',\n",
            "            'log_sys_usage': True,\n",
            "            'logger_config': None,\n",
            "            'logger_creator': None,\n",
            "            'lr': 0.0003,\n",
            "            'lr_schedule': None,\n",
            "            'max_num_worker_restarts': 1000,\n",
            "            'max_requests_in_flight_per_sampler_worker': 2,\n",
            "            'metrics_episode_collection_timeout_s': 60.0,\n",
            "            'metrics_num_episodes_for_smoothing': 100,\n",
            "            'min_sample_timesteps_per_iteration': 0,\n",
            "            'min_time_s_per_iteration': None,\n",
            "            'min_train_timesteps_per_iteration': 0,\n",
            "            'model': {'_disable_action_flattening': False,\n",
            "                      '_disable_preprocessor_api': False,\n",
            "                      '_time_major': False,\n",
            "                      '_use_default_native_models': -1,\n",
            "                      'always_check_shapes': False,\n",
            "                      'attention_dim': 64,\n",
            "                      'attention_head_dim': 32,\n",
            "                      'attention_init_gru_gate_bias': 2.0,\n",
            "                      'attention_memory_inference': 50,\n",
            "                      'attention_memory_training': 50,\n",
            "                      'attention_num_heads': 1,\n",
            "                      'attention_num_transformer_units': 1,\n",
            "                      'attention_position_wise_mlp_dim': 32,\n",
            "                      'attention_use_n_prev_actions': 0,\n",
            "                      'attention_use_n_prev_rewards': 0,\n",
            "                      'conv_activation': 'relu',\n",
            "                      'conv_filters': None,\n",
            "                      'custom_action_dist': None,\n",
            "                      'custom_model': None,\n",
            "                      'custom_model_config': {},\n",
            "                      'custom_preprocessor': None,\n",
            "                      'dim': 84,\n",
            "                      'encoder_latent_dim': None,\n",
            "                      'fcnet_activation': 'tanh',\n",
            "                      'fcnet_hiddens': [256, 256],\n",
            "                      'framestack': True,\n",
            "                      'free_log_std': False,\n",
            "                      'grayscale': False,\n",
            "                      'lstm_cell_size': 256,\n",
            "                      'lstm_use_prev_action': False,\n",
            "                      'lstm_use_prev_action_reward': -1,\n",
            "                      'lstm_use_prev_reward': False,\n",
            "                      'max_seq_len': 20,\n",
            "                      'no_final_linear': False,\n",
            "                      'post_fcnet_activation': 'relu',\n",
            "                      'post_fcnet_hiddens': [],\n",
            "                      'use_attention': False,\n",
            "                      'use_lstm': False,\n",
            "                      'vf_share_layers': False,\n",
            "                      'zero_mean': True},\n",
            "            'normalize_actions': True,\n",
            "            'num_consecutive_worker_failures_tolerance': 100,\n",
            "            'num_cpus_for_driver': 1,\n",
            "            'num_cpus_per_learner_worker': 1,\n",
            "            'num_cpus_per_worker': 1,\n",
            "            'num_envs_per_worker': 1,\n",
            "            'num_gpus': 0,\n",
            "            'num_gpus_per_learner_worker': 0,\n",
            "            'num_gpus_per_worker': 0,\n",
            "            'num_learner_workers': 0,\n",
            "            'num_sgd_iter': 20,\n",
            "            'num_workers': 1,\n",
            "            'observation_filter': 'NoFilter',\n",
            "            'observation_fn': None,\n",
            "            'observation_space': None,\n",
            "            'off_policy_estimation_methods': {},\n",
            "            'offline_sampling': False,\n",
            "            'ope_split_batch_by_episode': True,\n",
            "            'optimizer': {},\n",
            "            'output': None,\n",
            "            'output_compress_columns': ['obs', 'new_obs'],\n",
            "            'output_config': {},\n",
            "            'output_max_file_size': 67108864,\n",
            "            'placement_strategy': 'PACK',\n",
            "            'policies': {'default_policy': (None, None, None, None)},\n",
            "            'policies_to_train': None,\n",
            "            'policy_map_cache': -1,\n",
            "            'policy_map_capacity': 100,\n",
            "            'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x78f59bc564d0>,\n",
            "            'policy_states_are_swappable': False,\n",
            "            'postprocess_inputs': False,\n",
            "            'preprocessor_pref': 'deepmind',\n",
            "            'recreate_failed_workers': False,\n",
            "            'remote_env_batch_wait_ms': 0,\n",
            "            'remote_worker_envs': False,\n",
            "            'render_env': False,\n",
            "            'replay_sequence_length': None,\n",
            "            'restart_failed_sub_environments': False,\n",
            "            'rl_module_spec': None,\n",
            "            'rollout_fragment_length': 200,\n",
            "            'sample_async': False,\n",
            "            'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
            "            'sampler_perf_stats_ema_coef': None,\n",
            "            'seed': None,\n",
            "            'sgd_minibatch_size': 256,\n",
            "            'shuffle_buffer_size': 0,\n",
            "            'shuffle_sequences': True,\n",
            "            'simple_optimizer': False,\n",
            "            'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
            "            'synchronize_filters': -1,\n",
            "            'tf_session_args': {'allow_soft_placement': True,\n",
            "                                'device_count': {'CPU': 1},\n",
            "                                'gpu_options': {'allow_growth': True},\n",
            "                                'inter_op_parallelism_threads': 2,\n",
            "                                'intra_op_parallelism_threads': 2,\n",
            "                                'log_device_placement': False},\n",
            "            'torch_compile_learner': False,\n",
            "            'torch_compile_learner_dynamo_backend': 'inductor',\n",
            "            'torch_compile_learner_dynamo_mode': None,\n",
            "            'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
            "            'torch_compile_worker': False,\n",
            "            'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
            "            'torch_compile_worker_dynamo_mode': None,\n",
            "            'train_batch_size': 4000,\n",
            "            'update_worker_filter_stats': True,\n",
            "            'use_critic': True,\n",
            "            'use_gae': True,\n",
            "            'use_kl_loss': True,\n",
            "            'use_worker_filter_stats': True,\n",
            "            'validate_workers_after_construction': True,\n",
            "            'vf_clip_param': 10.0,\n",
            "            'vf_loss_coeff': 1.0,\n",
            "            'vf_share_layers': -1,\n",
            "            'worker_cls': -1,\n",
            "            'worker_health_probe_timeout_s': 60,\n",
            "            'worker_restore_timeout_s': 1800},\n",
            " 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.011798381805419922,\n",
            "                       'StateBufferConnector_ms': 0.010599851608276367,\n",
            "                       'ViewRequirementAgentConnector_ms': 0.20080256462097168},\n",
            " 'counters': {'num_agent_steps_sampled': 24000,\n",
            "              'num_agent_steps_trained': 24000,\n",
            "              'num_env_steps_sampled': 24000,\n",
            "              'num_env_steps_trained': 24000},\n",
            " 'custom_metrics': {},\n",
            " 'date': '2023-09-03_14-05-05',\n",
            " 'done': False,\n",
            " 'episode_len_mean': 145.26,\n",
            " 'episode_media': {},\n",
            " 'episode_reward_max': 1303.8000000000043,\n",
            " 'episode_reward_mean': 85.17500000000004,\n",
            " 'episode_reward_min': 6.6,\n",
            " 'episodes_this_iter': 7,\n",
            " 'episodes_total': 215,\n",
            " 'hist_stats': {'episode_lengths': [49,\n",
            "                                    59,\n",
            "                                    113,\n",
            "                                    75,\n",
            "                                    21,\n",
            "                                    33,\n",
            "                                    39,\n",
            "                                    35,\n",
            "                                    65,\n",
            "                                    25,\n",
            "                                    31,\n",
            "                                    65,\n",
            "                                    151,\n",
            "                                    79,\n",
            "                                    35,\n",
            "                                    47,\n",
            "                                    109,\n",
            "                                    185,\n",
            "                                    43,\n",
            "                                    17,\n",
            "                                    87,\n",
            "                                    39,\n",
            "                                    79,\n",
            "                                    33,\n",
            "                                    31,\n",
            "                                    35,\n",
            "                                    59,\n",
            "                                    99,\n",
            "                                    29,\n",
            "                                    29,\n",
            "                                    41,\n",
            "                                    61,\n",
            "                                    59,\n",
            "                                    71,\n",
            "                                    65,\n",
            "                                    23,\n",
            "                                    25,\n",
            "                                    35,\n",
            "                                    29,\n",
            "                                    69,\n",
            "                                    35,\n",
            "                                    49,\n",
            "                                    27,\n",
            "                                    35,\n",
            "                                    29,\n",
            "                                    61,\n",
            "                                    27,\n",
            "                                    11,\n",
            "                                    43,\n",
            "                                    55,\n",
            "                                    37,\n",
            "                                    71,\n",
            "                                    37,\n",
            "                                    227,\n",
            "                                    47,\n",
            "                                    363,\n",
            "                                    117,\n",
            "                                    475,\n",
            "                                    237,\n",
            "                                    159,\n",
            "                                    155,\n",
            "                                    245,\n",
            "                                    105,\n",
            "                                    167,\n",
            "                                    75,\n",
            "                                    67,\n",
            "                                    103,\n",
            "                                    385,\n",
            "                                    159,\n",
            "                                    795,\n",
            "                                    47,\n",
            "                                    31,\n",
            "                                    365,\n",
            "                                    371,\n",
            "                                    49,\n",
            "                                    139,\n",
            "                                    373,\n",
            "                                    201,\n",
            "                                    275,\n",
            "                                    115,\n",
            "                                    267,\n",
            "                                    167,\n",
            "                                    37,\n",
            "                                    325,\n",
            "                                    59,\n",
            "                                    97,\n",
            "                                    105,\n",
            "                                    69,\n",
            "                                    75,\n",
            "                                    201,\n",
            "                                    219,\n",
            "                                    243,\n",
            "                                    167,\n",
            "                                    101,\n",
            "                                    2115,\n",
            "                                    251,\n",
            "                                    503,\n",
            "                                    671,\n",
            "                                    103,\n",
            "                                    113],\n",
            "                'episode_reward': [18.100000000000005,\n",
            "                                   31.999999999999996,\n",
            "                                   42.7,\n",
            "                                   34.6,\n",
            "                                   9.899999999999999,\n",
            "                                   22.099999999999994,\n",
            "                                   21.999999999999996,\n",
            "                                   14.2,\n",
            "                                   32.3,\n",
            "                                   10.7,\n",
            "                                   18.2,\n",
            "                                   39.49999999999999,\n",
            "                                   80.59999999999997,\n",
            "                                   32.4,\n",
            "                                   21.6,\n",
            "                                   22.2,\n",
            "                                   56.099999999999994,\n",
            "                                   97.50000000000004,\n",
            "                                   24.2,\n",
            "                                   11.5,\n",
            "                                   50.40000000000001,\n",
            "                                   17.599999999999998,\n",
            "                                   42.20000000000001,\n",
            "                                   16.7,\n",
            "                                   13.600000000000003,\n",
            "                                   17.2,\n",
            "                                   31.400000000000002,\n",
            "                                   58.99999999999999,\n",
            "                                   14.299999999999999,\n",
            "                                   14.9,\n",
            "                                   21.900000000000002,\n",
            "                                   22.5,\n",
            "                                   24.8,\n",
            "                                   35.4,\n",
            "                                   46.49999999999998,\n",
            "                                   14.800000000000002,\n",
            "                                   13.3,\n",
            "                                   8.4,\n",
            "                                   18.099999999999998,\n",
            "                                   34.699999999999996,\n",
            "                                   16.0,\n",
            "                                   24.900000000000006,\n",
            "                                   15.0,\n",
            "                                   19.2,\n",
            "                                   13.3,\n",
            "                                   26.9,\n",
            "                                   16.600000000000005,\n",
            "                                   6.6,\n",
            "                                   22.0,\n",
            "                                   28.200000000000003,\n",
            "                                   20.1,\n",
            "                                   43.59999999999999,\n",
            "                                   19.7,\n",
            "                                   132.59999999999994,\n",
            "                                   26.800000000000004,\n",
            "                                   223.39999999999978,\n",
            "                                   58.30000000000001,\n",
            "                                   271.7999999999999,\n",
            "                                   141.70000000000005,\n",
            "                                   96.39999999999999,\n",
            "                                   97.80000000000001,\n",
            "                                   144.9,\n",
            "                                   55.50000000000001,\n",
            "                                   92.0,\n",
            "                                   45.600000000000016,\n",
            "                                   41.800000000000004,\n",
            "                                   52.599999999999966,\n",
            "                                   221.89999999999998,\n",
            "                                   95.80000000000003,\n",
            "                                   479.80000000000035,\n",
            "                                   21.6,\n",
            "                                   15.399999999999999,\n",
            "                                   232.89999999999972,\n",
            "                                   239.59999999999988,\n",
            "                                   28.699999999999996,\n",
            "                                   86.20000000000005,\n",
            "                                   241.29999999999993,\n",
            "                                   119.3,\n",
            "                                   164.59999999999994,\n",
            "                                   70.60000000000002,\n",
            "                                   154.7999999999998,\n",
            "                                   93.6,\n",
            "                                   25.3,\n",
            "                                   202.29999999999993,\n",
            "                                   34.60000000000001,\n",
            "                                   51.7,\n",
            "                                   59.29999999999999,\n",
            "                                   45.300000000000004,\n",
            "                                   42.6,\n",
            "                                   131.09999999999997,\n",
            "                                   135.39999999999992,\n",
            "                                   150.19999999999993,\n",
            "                                   98.40000000000002,\n",
            "                                   66.30000000000003,\n",
            "                                   1303.8000000000043,\n",
            "                                   144.19999999999996,\n",
            "                                   301.1999999999996,\n",
            "                                   421.8000000000002,\n",
            "                                   58.80000000000002,\n",
            "                                   65.69999999999999]},\n",
            " 'hostname': '9e348d898222',\n",
            " 'info': {'learner': {'default_policy': {'custom_metrics': {},\n",
            "                                         'diff_num_grad_updates_vs_sampler_policy': 149.5,\n",
            "                                         'learner_stats': {'allreduce_latency': 0.0,\n",
            "                                                           'cur_kl_coeff': 0.19999999999999998,\n",
            "                                                           'cur_lr': 0.0003,\n",
            "                                                           'entropy': 0.5982900530099868,\n",
            "                                                           'entropy_coeff': 0.0,\n",
            "                                                           'grad_gnorm': 11.887184409946203,\n",
            "                                                           'kl': 0.010515402292239979,\n",
            "                                                           'policy_loss': 0.0027704871942599613,\n",
            "                                                           'total_loss': 3.2444931612412136,\n",
            "                                                           'vf_explained_var': -0.019659864902496337,\n",
            "                                                           'vf_loss': 3.239619574348132},\n",
            "                                         'model': {},\n",
            "                                         'num_agent_steps_trained': 256.0,\n",
            "                                         'num_grad_updates_lifetime': 1650.5}},\n",
            "          'num_agent_steps_sampled': 24000,\n",
            "          'num_agent_steps_trained': 24000,\n",
            "          'num_env_steps_sampled': 24000,\n",
            "          'num_env_steps_trained': 24000},\n",
            " 'iterations_since_restore': 6,\n",
            " 'node_ip': '172.28.0.12',\n",
            " 'num_agent_steps_sampled': 24000,\n",
            " 'num_agent_steps_trained': 24000,\n",
            " 'num_env_steps_sampled': 24000,\n",
            " 'num_env_steps_sampled_this_iter': 4000,\n",
            " 'num_env_steps_sampled_throughput_per_sec': 281.59955957100135,\n",
            " 'num_env_steps_trained': 24000,\n",
            " 'num_env_steps_trained_this_iter': 4000,\n",
            " 'num_env_steps_trained_throughput_per_sec': 281.59955957100135,\n",
            " 'num_faulty_episodes': 0,\n",
            " 'num_healthy_workers': 1,\n",
            " 'num_in_flight_async_reqs': 0,\n",
            " 'num_remote_worker_restarts': 0,\n",
            " 'num_steps_trained_this_iter': 4000,\n",
            " 'perf': {'cpu_util_percent': 72.07142857142857,\n",
            "          'ram_util_percent': 18.199999999999996},\n",
            " 'pid': 401,\n",
            " 'policy_reward_max': {},\n",
            " 'policy_reward_mean': {},\n",
            " 'policy_reward_min': {},\n",
            " 'sampler_perf': {'mean_action_processing_ms': 0.21367603373600294,\n",
            "                  'mean_env_render_ms': 0.0,\n",
            "                  'mean_env_wait_ms': 0.06363064767286308,\n",
            "                  'mean_inference_ms': 1.762654695156919,\n",
            "                  'mean_raw_obs_processing_ms': 0.5294406593837168},\n",
            " 'sampler_results': {'connector_metrics': {'ObsPreprocessorConnector_ms': 0.011798381805419922,\n",
            "                                           'StateBufferConnector_ms': 0.010599851608276367,\n",
            "                                           'ViewRequirementAgentConnector_ms': 0.20080256462097168},\n",
            "                     'custom_metrics': {},\n",
            "                     'episode_len_mean': 145.26,\n",
            "                     'episode_media': {},\n",
            "                     'episode_reward_max': 1303.8000000000043,\n",
            "                     'episode_reward_mean': 85.17500000000004,\n",
            "                     'episode_reward_min': 6.6,\n",
            "                     'episodes_this_iter': 7,\n",
            "                     'hist_stats': {'episode_lengths': [49,\n",
            "                                                        59,\n",
            "                                                        113,\n",
            "                                                        75,\n",
            "                                                        21,\n",
            "                                                        33,\n",
            "                                                        39,\n",
            "                                                        35,\n",
            "                                                        65,\n",
            "                                                        25,\n",
            "                                                        31,\n",
            "                                                        65,\n",
            "                                                        151,\n",
            "                                                        79,\n",
            "                                                        35,\n",
            "                                                        47,\n",
            "                                                        109,\n",
            "                                                        185,\n",
            "                                                        43,\n",
            "                                                        17,\n",
            "                                                        87,\n",
            "                                                        39,\n",
            "                                                        79,\n",
            "                                                        33,\n",
            "                                                        31,\n",
            "                                                        35,\n",
            "                                                        59,\n",
            "                                                        99,\n",
            "                                                        29,\n",
            "                                                        29,\n",
            "                                                        41,\n",
            "                                                        61,\n",
            "                                                        59,\n",
            "                                                        71,\n",
            "                                                        65,\n",
            "                                                        23,\n",
            "                                                        25,\n",
            "                                                        35,\n",
            "                                                        29,\n",
            "                                                        69,\n",
            "                                                        35,\n",
            "                                                        49,\n",
            "                                                        27,\n",
            "                                                        35,\n",
            "                                                        29,\n",
            "                                                        61,\n",
            "                                                        27,\n",
            "                                                        11,\n",
            "                                                        43,\n",
            "                                                        55,\n",
            "                                                        37,\n",
            "                                                        71,\n",
            "                                                        37,\n",
            "                                                        227,\n",
            "                                                        47,\n",
            "                                                        363,\n",
            "                                                        117,\n",
            "                                                        475,\n",
            "                                                        237,\n",
            "                                                        159,\n",
            "                                                        155,\n",
            "                                                        245,\n",
            "                                                        105,\n",
            "                                                        167,\n",
            "                                                        75,\n",
            "                                                        67,\n",
            "                                                        103,\n",
            "                                                        385,\n",
            "                                                        159,\n",
            "                                                        795,\n",
            "                                                        47,\n",
            "                                                        31,\n",
            "                                                        365,\n",
            "                                                        371,\n",
            "                                                        49,\n",
            "                                                        139,\n",
            "                                                        373,\n",
            "                                                        201,\n",
            "                                                        275,\n",
            "                                                        115,\n",
            "                                                        267,\n",
            "                                                        167,\n",
            "                                                        37,\n",
            "                                                        325,\n",
            "                                                        59,\n",
            "                                                        97,\n",
            "                                                        105,\n",
            "                                                        69,\n",
            "                                                        75,\n",
            "                                                        201,\n",
            "                                                        219,\n",
            "                                                        243,\n",
            "                                                        167,\n",
            "                                                        101,\n",
            "                                                        2115,\n",
            "                                                        251,\n",
            "                                                        503,\n",
            "                                                        671,\n",
            "                                                        103,\n",
            "                                                        113],\n",
            "                                    'episode_reward': [18.100000000000005,\n",
            "                                                       31.999999999999996,\n",
            "                                                       42.7,\n",
            "                                                       34.6,\n",
            "                                                       9.899999999999999,\n",
            "                                                       22.099999999999994,\n",
            "                                                       21.999999999999996,\n",
            "                                                       14.2,\n",
            "                                                       32.3,\n",
            "                                                       10.7,\n",
            "                                                       18.2,\n",
            "                                                       39.49999999999999,\n",
            "                                                       80.59999999999997,\n",
            "                                                       32.4,\n",
            "                                                       21.6,\n",
            "                                                       22.2,\n",
            "                                                       56.099999999999994,\n",
            "                                                       97.50000000000004,\n",
            "                                                       24.2,\n",
            "                                                       11.5,\n",
            "                                                       50.40000000000001,\n",
            "                                                       17.599999999999998,\n",
            "                                                       42.20000000000001,\n",
            "                                                       16.7,\n",
            "                                                       13.600000000000003,\n",
            "                                                       17.2,\n",
            "                                                       31.400000000000002,\n",
            "                                                       58.99999999999999,\n",
            "                                                       14.299999999999999,\n",
            "                                                       14.9,\n",
            "                                                       21.900000000000002,\n",
            "                                                       22.5,\n",
            "                                                       24.8,\n",
            "                                                       35.4,\n",
            "                                                       46.49999999999998,\n",
            "                                                       14.800000000000002,\n",
            "                                                       13.3,\n",
            "                                                       8.4,\n",
            "                                                       18.099999999999998,\n",
            "                                                       34.699999999999996,\n",
            "                                                       16.0,\n",
            "                                                       24.900000000000006,\n",
            "                                                       15.0,\n",
            "                                                       19.2,\n",
            "                                                       13.3,\n",
            "                                                       26.9,\n",
            "                                                       16.600000000000005,\n",
            "                                                       6.6,\n",
            "                                                       22.0,\n",
            "                                                       28.200000000000003,\n",
            "                                                       20.1,\n",
            "                                                       43.59999999999999,\n",
            "                                                       19.7,\n",
            "                                                       132.59999999999994,\n",
            "                                                       26.800000000000004,\n",
            "                                                       223.39999999999978,\n",
            "                                                       58.30000000000001,\n",
            "                                                       271.7999999999999,\n",
            "                                                       141.70000000000005,\n",
            "                                                       96.39999999999999,\n",
            "                                                       97.80000000000001,\n",
            "                                                       144.9,\n",
            "                                                       55.50000000000001,\n",
            "                                                       92.0,\n",
            "                                                       45.600000000000016,\n",
            "                                                       41.800000000000004,\n",
            "                                                       52.599999999999966,\n",
            "                                                       221.89999999999998,\n",
            "                                                       95.80000000000003,\n",
            "                                                       479.80000000000035,\n",
            "                                                       21.6,\n",
            "                                                       15.399999999999999,\n",
            "                                                       232.89999999999972,\n",
            "                                                       239.59999999999988,\n",
            "                                                       28.699999999999996,\n",
            "                                                       86.20000000000005,\n",
            "                                                       241.29999999999993,\n",
            "                                                       119.3,\n",
            "                                                       164.59999999999994,\n",
            "                                                       70.60000000000002,\n",
            "                                                       154.7999999999998,\n",
            "                                                       93.6,\n",
            "                                                       25.3,\n",
            "                                                       202.29999999999993,\n",
            "                                                       34.60000000000001,\n",
            "                                                       51.7,\n",
            "                                                       59.29999999999999,\n",
            "                                                       45.300000000000004,\n",
            "                                                       42.6,\n",
            "                                                       131.09999999999997,\n",
            "                                                       135.39999999999992,\n",
            "                                                       150.19999999999993,\n",
            "                                                       98.40000000000002,\n",
            "                                                       66.30000000000003,\n",
            "                                                       1303.8000000000043,\n",
            "                                                       144.19999999999996,\n",
            "                                                       301.1999999999996,\n",
            "                                                       421.8000000000002,\n",
            "                                                       58.80000000000002,\n",
            "                                                       65.69999999999999]},\n",
            "                     'num_faulty_episodes': 0,\n",
            "                     'policy_reward_max': {},\n",
            "                     'policy_reward_mean': {},\n",
            "                     'policy_reward_min': {},\n",
            "                     'sampler_perf': {'mean_action_processing_ms': 0.21367603373600294,\n",
            "                                      'mean_env_render_ms': 0.0,\n",
            "                                      'mean_env_wait_ms': 0.06363064767286308,\n",
            "                                      'mean_inference_ms': 1.762654695156919,\n",
            "                                      'mean_raw_obs_processing_ms': 0.5294406593837168}},\n",
            " 'time_since_restore': 88.23857140541077,\n",
            " 'time_this_iter_s': 14.216062545776367,\n",
            " 'time_total_s': 88.23857140541077,\n",
            " 'timers': {'learn_throughput': 808.285,\n",
            "            'learn_time_ms': 4948.752,\n",
            "            'load_throughput': 5276130.615,\n",
            "            'load_time_ms': 0.758,\n",
            "            'sample_time_ms': 9743.322,\n",
            "            'synch_weights_time_ms': 3.464,\n",
            "            'training_iteration_time_ms': 14697.966},\n",
            " 'timestamp': 1693749905,\n",
            " 'timesteps_total': 24000,\n",
            " 'training_iteration': 6,\n",
            " 'trial_id': 'default'}\n",
            "****************************************************************************************************************************************\n",
            "{'agent_timesteps_total': 28000,\n",
            " 'config': {'_AlgorithmConfig__prior_exploration_config': None,\n",
            "            '_disable_action_flattening': False,\n",
            "            '_disable_execution_plan_api': True,\n",
            "            '_disable_initialize_loss_from_dummy_batch': False,\n",
            "            '_disable_preprocessor_api': False,\n",
            "            '_enable_learner_api': False,\n",
            "            '_enable_rl_module_api': False,\n",
            "            '_fake_gpus': False,\n",
            "            '_is_atari': None,\n",
            "            '_learner_class': None,\n",
            "            '_tf_policy_handles_more_than_one_loss': False,\n",
            "            'action_mask_key': 'action_mask',\n",
            "            'action_space': None,\n",
            "            'actions_in_input_normalized': False,\n",
            "            'algorithm_config_overrides_per_module': {},\n",
            "            'always_attach_evaluation_results': False,\n",
            "            'auto_wrap_old_gym_envs': True,\n",
            "            'batch_mode': 'truncate_episodes',\n",
            "            'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>,\n",
            "            'checkpoint_trainable_policies_only': False,\n",
            "            'clip_actions': False,\n",
            "            'clip_param': 0.2,\n",
            "            'clip_rewards': None,\n",
            "            'compress_observations': False,\n",
            "            'count_steps_by': 'env_steps',\n",
            "            'create_env_on_driver': False,\n",
            "            'custom_eval_function': None,\n",
            "            'custom_resources_per_worker': {},\n",
            "            'delay_between_worker_restarts_s': 60.0,\n",
            "            'disable_env_checking': False,\n",
            "            'eager_max_retraces': 20,\n",
            "            'eager_tracing': True,\n",
            "            'enable_async_evaluation': False,\n",
            "            'enable_connectors': True,\n",
            "            'enable_tf1_exec_eagerly': False,\n",
            "            'entropy_coeff': 0.0,\n",
            "            'entropy_coeff_schedule': None,\n",
            "            'env': 'my',\n",
            "            'env_config': {},\n",
            "            'env_runner_cls': None,\n",
            "            'env_task_fn': None,\n",
            "            'evaluation_config': None,\n",
            "            'evaluation_duration': 10,\n",
            "            'evaluation_duration_unit': 'episodes',\n",
            "            'evaluation_interval': None,\n",
            "            'evaluation_num_workers': 0,\n",
            "            'evaluation_parallel_to_training': False,\n",
            "            'evaluation_sample_timeout_s': 180.0,\n",
            "            'exploration_config': {'type': 'StochasticSampling'},\n",
            "            'explore': True,\n",
            "            'export_native_model_files': True,\n",
            "            'extra_python_environs_for_driver': {},\n",
            "            'extra_python_environs_for_worker': {},\n",
            "            'fake_sampler': False,\n",
            "            'framework': 'torch',\n",
            "            'gamma': 0.99,\n",
            "            'grad_clip': None,\n",
            "            'grad_clip_by': 'global_norm',\n",
            "            'ignore_worker_failures': False,\n",
            "            'in_evaluation': False,\n",
            "            'input': 'sampler',\n",
            "            'input_config': {},\n",
            "            'keep_per_episode_custom_metrics': False,\n",
            "            'kl_coeff': 0.2,\n",
            "            'kl_target': 0.01,\n",
            "            'lambda': 0.95,\n",
            "            'local_gpu_idx': 0,\n",
            "            'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
            "                                      'intra_op_parallelism_threads': 8},\n",
            "            'log_level': 'WARN',\n",
            "            'log_sys_usage': True,\n",
            "            'logger_config': None,\n",
            "            'logger_creator': None,\n",
            "            'lr': 0.0003,\n",
            "            'lr_schedule': None,\n",
            "            'max_num_worker_restarts': 1000,\n",
            "            'max_requests_in_flight_per_sampler_worker': 2,\n",
            "            'metrics_episode_collection_timeout_s': 60.0,\n",
            "            'metrics_num_episodes_for_smoothing': 100,\n",
            "            'min_sample_timesteps_per_iteration': 0,\n",
            "            'min_time_s_per_iteration': None,\n",
            "            'min_train_timesteps_per_iteration': 0,\n",
            "            'model': {'_disable_action_flattening': False,\n",
            "                      '_disable_preprocessor_api': False,\n",
            "                      '_time_major': False,\n",
            "                      '_use_default_native_models': -1,\n",
            "                      'always_check_shapes': False,\n",
            "                      'attention_dim': 64,\n",
            "                      'attention_head_dim': 32,\n",
            "                      'attention_init_gru_gate_bias': 2.0,\n",
            "                      'attention_memory_inference': 50,\n",
            "                      'attention_memory_training': 50,\n",
            "                      'attention_num_heads': 1,\n",
            "                      'attention_num_transformer_units': 1,\n",
            "                      'attention_position_wise_mlp_dim': 32,\n",
            "                      'attention_use_n_prev_actions': 0,\n",
            "                      'attention_use_n_prev_rewards': 0,\n",
            "                      'conv_activation': 'relu',\n",
            "                      'conv_filters': None,\n",
            "                      'custom_action_dist': None,\n",
            "                      'custom_model': None,\n",
            "                      'custom_model_config': {},\n",
            "                      'custom_preprocessor': None,\n",
            "                      'dim': 84,\n",
            "                      'encoder_latent_dim': None,\n",
            "                      'fcnet_activation': 'tanh',\n",
            "                      'fcnet_hiddens': [256, 256],\n",
            "                      'framestack': True,\n",
            "                      'free_log_std': False,\n",
            "                      'grayscale': False,\n",
            "                      'lstm_cell_size': 256,\n",
            "                      'lstm_use_prev_action': False,\n",
            "                      'lstm_use_prev_action_reward': -1,\n",
            "                      'lstm_use_prev_reward': False,\n",
            "                      'max_seq_len': 20,\n",
            "                      'no_final_linear': False,\n",
            "                      'post_fcnet_activation': 'relu',\n",
            "                      'post_fcnet_hiddens': [],\n",
            "                      'use_attention': False,\n",
            "                      'use_lstm': False,\n",
            "                      'vf_share_layers': False,\n",
            "                      'zero_mean': True},\n",
            "            'normalize_actions': True,\n",
            "            'num_consecutive_worker_failures_tolerance': 100,\n",
            "            'num_cpus_for_driver': 1,\n",
            "            'num_cpus_per_learner_worker': 1,\n",
            "            'num_cpus_per_worker': 1,\n",
            "            'num_envs_per_worker': 1,\n",
            "            'num_gpus': 0,\n",
            "            'num_gpus_per_learner_worker': 0,\n",
            "            'num_gpus_per_worker': 0,\n",
            "            'num_learner_workers': 0,\n",
            "            'num_sgd_iter': 20,\n",
            "            'num_workers': 1,\n",
            "            'observation_filter': 'NoFilter',\n",
            "            'observation_fn': None,\n",
            "            'observation_space': None,\n",
            "            'off_policy_estimation_methods': {},\n",
            "            'offline_sampling': False,\n",
            "            'ope_split_batch_by_episode': True,\n",
            "            'optimizer': {},\n",
            "            'output': None,\n",
            "            'output_compress_columns': ['obs', 'new_obs'],\n",
            "            'output_config': {},\n",
            "            'output_max_file_size': 67108864,\n",
            "            'placement_strategy': 'PACK',\n",
            "            'policies': {'default_policy': (None, None, None, None)},\n",
            "            'policies_to_train': None,\n",
            "            'policy_map_cache': -1,\n",
            "            'policy_map_capacity': 100,\n",
            "            'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x78f59bc564d0>,\n",
            "            'policy_states_are_swappable': False,\n",
            "            'postprocess_inputs': False,\n",
            "            'preprocessor_pref': 'deepmind',\n",
            "            'recreate_failed_workers': False,\n",
            "            'remote_env_batch_wait_ms': 0,\n",
            "            'remote_worker_envs': False,\n",
            "            'render_env': False,\n",
            "            'replay_sequence_length': None,\n",
            "            'restart_failed_sub_environments': False,\n",
            "            'rl_module_spec': None,\n",
            "            'rollout_fragment_length': 200,\n",
            "            'sample_async': False,\n",
            "            'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
            "            'sampler_perf_stats_ema_coef': None,\n",
            "            'seed': None,\n",
            "            'sgd_minibatch_size': 256,\n",
            "            'shuffle_buffer_size': 0,\n",
            "            'shuffle_sequences': True,\n",
            "            'simple_optimizer': False,\n",
            "            'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
            "            'synchronize_filters': -1,\n",
            "            'tf_session_args': {'allow_soft_placement': True,\n",
            "                                'device_count': {'CPU': 1},\n",
            "                                'gpu_options': {'allow_growth': True},\n",
            "                                'inter_op_parallelism_threads': 2,\n",
            "                                'intra_op_parallelism_threads': 2,\n",
            "                                'log_device_placement': False},\n",
            "            'torch_compile_learner': False,\n",
            "            'torch_compile_learner_dynamo_backend': 'inductor',\n",
            "            'torch_compile_learner_dynamo_mode': None,\n",
            "            'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
            "            'torch_compile_worker': False,\n",
            "            'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
            "            'torch_compile_worker_dynamo_mode': None,\n",
            "            'train_batch_size': 4000,\n",
            "            'update_worker_filter_stats': True,\n",
            "            'use_critic': True,\n",
            "            'use_gae': True,\n",
            "            'use_kl_loss': True,\n",
            "            'use_worker_filter_stats': True,\n",
            "            'validate_workers_after_construction': True,\n",
            "            'vf_clip_param': 10.0,\n",
            "            'vf_loss_coeff': 1.0,\n",
            "            'vf_share_layers': -1,\n",
            "            'worker_cls': -1,\n",
            "            'worker_health_probe_timeout_s': 60,\n",
            "            'worker_restore_timeout_s': 1800},\n",
            " 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.011791706085205078,\n",
            "                       'StateBufferConnector_ms': 0.0106353759765625,\n",
            "                       'ViewRequirementAgentConnector_ms': 0.20078730583190918},\n",
            " 'counters': {'num_agent_steps_sampled': 28000,\n",
            "              'num_agent_steps_trained': 28000,\n",
            "              'num_env_steps_sampled': 28000,\n",
            "              'num_env_steps_trained': 28000},\n",
            " 'custom_metrics': {},\n",
            " 'date': '2023-09-03_14-05-18',\n",
            " 'done': False,\n",
            " 'episode_len_mean': 168.0,\n",
            " 'episode_media': {},\n",
            " 'episode_reward_max': 1303.8000000000043,\n",
            " 'episode_reward_mean': 99.64000000000011,\n",
            " 'episode_reward_min': 6.6,\n",
            " 'episodes_this_iter': 2,\n",
            " 'episodes_total': 217,\n",
            " 'hist_stats': {'episode_lengths': [113,\n",
            "                                    75,\n",
            "                                    21,\n",
            "                                    33,\n",
            "                                    39,\n",
            "                                    35,\n",
            "                                    65,\n",
            "                                    25,\n",
            "                                    31,\n",
            "                                    65,\n",
            "                                    151,\n",
            "                                    79,\n",
            "                                    35,\n",
            "                                    47,\n",
            "                                    109,\n",
            "                                    185,\n",
            "                                    43,\n",
            "                                    17,\n",
            "                                    87,\n",
            "                                    39,\n",
            "                                    79,\n",
            "                                    33,\n",
            "                                    31,\n",
            "                                    35,\n",
            "                                    59,\n",
            "                                    99,\n",
            "                                    29,\n",
            "                                    29,\n",
            "                                    41,\n",
            "                                    61,\n",
            "                                    59,\n",
            "                                    71,\n",
            "                                    65,\n",
            "                                    23,\n",
            "                                    25,\n",
            "                                    35,\n",
            "                                    29,\n",
            "                                    69,\n",
            "                                    35,\n",
            "                                    49,\n",
            "                                    27,\n",
            "                                    35,\n",
            "                                    29,\n",
            "                                    61,\n",
            "                                    27,\n",
            "                                    11,\n",
            "                                    43,\n",
            "                                    55,\n",
            "                                    37,\n",
            "                                    71,\n",
            "                                    37,\n",
            "                                    227,\n",
            "                                    47,\n",
            "                                    363,\n",
            "                                    117,\n",
            "                                    475,\n",
            "                                    237,\n",
            "                                    159,\n",
            "                                    155,\n",
            "                                    245,\n",
            "                                    105,\n",
            "                                    167,\n",
            "                                    75,\n",
            "                                    67,\n",
            "                                    103,\n",
            "                                    385,\n",
            "                                    159,\n",
            "                                    795,\n",
            "                                    47,\n",
            "                                    31,\n",
            "                                    365,\n",
            "                                    371,\n",
            "                                    49,\n",
            "                                    139,\n",
            "                                    373,\n",
            "                                    201,\n",
            "                                    275,\n",
            "                                    115,\n",
            "                                    267,\n",
            "                                    167,\n",
            "                                    37,\n",
            "                                    325,\n",
            "                                    59,\n",
            "                                    97,\n",
            "                                    105,\n",
            "                                    69,\n",
            "                                    75,\n",
            "                                    201,\n",
            "                                    219,\n",
            "                                    243,\n",
            "                                    167,\n",
            "                                    101,\n",
            "                                    2115,\n",
            "                                    251,\n",
            "                                    503,\n",
            "                                    671,\n",
            "                                    103,\n",
            "                                    113,\n",
            "                                    427,\n",
            "                                    1955],\n",
            "                'episode_reward': [42.7,\n",
            "                                   34.6,\n",
            "                                   9.899999999999999,\n",
            "                                   22.099999999999994,\n",
            "                                   21.999999999999996,\n",
            "                                   14.2,\n",
            "                                   32.3,\n",
            "                                   10.7,\n",
            "                                   18.2,\n",
            "                                   39.49999999999999,\n",
            "                                   80.59999999999997,\n",
            "                                   32.4,\n",
            "                                   21.6,\n",
            "                                   22.2,\n",
            "                                   56.099999999999994,\n",
            "                                   97.50000000000004,\n",
            "                                   24.2,\n",
            "                                   11.5,\n",
            "                                   50.40000000000001,\n",
            "                                   17.599999999999998,\n",
            "                                   42.20000000000001,\n",
            "                                   16.7,\n",
            "                                   13.600000000000003,\n",
            "                                   17.2,\n",
            "                                   31.400000000000002,\n",
            "                                   58.99999999999999,\n",
            "                                   14.299999999999999,\n",
            "                                   14.9,\n",
            "                                   21.900000000000002,\n",
            "                                   22.5,\n",
            "                                   24.8,\n",
            "                                   35.4,\n",
            "                                   46.49999999999998,\n",
            "                                   14.800000000000002,\n",
            "                                   13.3,\n",
            "                                   8.4,\n",
            "                                   18.099999999999998,\n",
            "                                   34.699999999999996,\n",
            "                                   16.0,\n",
            "                                   24.900000000000006,\n",
            "                                   15.0,\n",
            "                                   19.2,\n",
            "                                   13.3,\n",
            "                                   26.9,\n",
            "                                   16.600000000000005,\n",
            "                                   6.6,\n",
            "                                   22.0,\n",
            "                                   28.200000000000003,\n",
            "                                   20.1,\n",
            "                                   43.59999999999999,\n",
            "                                   19.7,\n",
            "                                   132.59999999999994,\n",
            "                                   26.800000000000004,\n",
            "                                   223.39999999999978,\n",
            "                                   58.30000000000001,\n",
            "                                   271.7999999999999,\n",
            "                                   141.70000000000005,\n",
            "                                   96.39999999999999,\n",
            "                                   97.80000000000001,\n",
            "                                   144.9,\n",
            "                                   55.50000000000001,\n",
            "                                   92.0,\n",
            "                                   45.600000000000016,\n",
            "                                   41.800000000000004,\n",
            "                                   52.599999999999966,\n",
            "                                   221.89999999999998,\n",
            "                                   95.80000000000003,\n",
            "                                   479.80000000000035,\n",
            "                                   21.6,\n",
            "                                   15.399999999999999,\n",
            "                                   232.89999999999972,\n",
            "                                   239.59999999999988,\n",
            "                                   28.699999999999996,\n",
            "                                   86.20000000000005,\n",
            "                                   241.29999999999993,\n",
            "                                   119.3,\n",
            "                                   164.59999999999994,\n",
            "                                   70.60000000000002,\n",
            "                                   154.7999999999998,\n",
            "                                   93.6,\n",
            "                                   25.3,\n",
            "                                   202.29999999999993,\n",
            "                                   34.60000000000001,\n",
            "                                   51.7,\n",
            "                                   59.29999999999999,\n",
            "                                   45.300000000000004,\n",
            "                                   42.6,\n",
            "                                   131.09999999999997,\n",
            "                                   135.39999999999992,\n",
            "                                   150.19999999999993,\n",
            "                                   98.40000000000002,\n",
            "                                   66.30000000000003,\n",
            "                                   1303.8000000000043,\n",
            "                                   144.19999999999996,\n",
            "                                   301.1999999999996,\n",
            "                                   421.8000000000002,\n",
            "                                   58.80000000000002,\n",
            "                                   65.69999999999999,\n",
            "                                   271.3999999999996,\n",
            "                                   1225.2000000000078]},\n",
            " 'hostname': '9e348d898222',\n",
            " 'info': {'learner': {'default_policy': {'custom_metrics': {},\n",
            "                                         'diff_num_grad_updates_vs_sampler_policy': 149.5,\n",
            "                                         'learner_stats': {'allreduce_latency': 0.0,\n",
            "                                                           'cur_kl_coeff': 0.19999999999999998,\n",
            "                                                           'cur_lr': 0.0003,\n",
            "                                                           'entropy': 0.5863236266374589,\n",
            "                                                           'entropy_coeff': 0.0,\n",
            "                                                           'grad_gnorm': 11.08043469518423,\n",
            "                                                           'kl': 0.014956535477293009,\n",
            "                                                           'policy_loss': 0.0035589049787571035,\n",
            "                                                           'total_loss': 2.4527933671077093,\n",
            "                                                           'vf_explained_var': 0.006736326018969218,\n",
            "                                                           'vf_loss': 2.44624315738678},\n",
            "                                         'model': {},\n",
            "                                         'num_agent_steps_trained': 256.0,\n",
            "                                         'num_grad_updates_lifetime': 1950.5}},\n",
            "          'num_agent_steps_sampled': 28000,\n",
            "          'num_agent_steps_trained': 28000,\n",
            "          'num_env_steps_sampled': 28000,\n",
            "          'num_env_steps_trained': 28000},\n",
            " 'iterations_since_restore': 7,\n",
            " 'node_ip': '172.28.0.12',\n",
            " 'num_agent_steps_sampled': 28000,\n",
            " 'num_agent_steps_trained': 28000,\n",
            " 'num_env_steps_sampled': 28000,\n",
            " 'num_env_steps_sampled_this_iter': 4000,\n",
            " 'num_env_steps_sampled_throughput_per_sec': 311.82864168954563,\n",
            " 'num_env_steps_trained': 28000,\n",
            " 'num_env_steps_trained_this_iter': 4000,\n",
            " 'num_env_steps_trained_throughput_per_sec': 311.82864168954563,\n",
            " 'num_faulty_episodes': 0,\n",
            " 'num_healthy_workers': 1,\n",
            " 'num_in_flight_async_reqs': 0,\n",
            " 'num_remote_worker_restarts': 0,\n",
            " 'num_steps_trained_this_iter': 4000,\n",
            " 'perf': {'cpu_util_percent': 63.47222222222222, 'ram_util_percent': 18.2},\n",
            " 'pid': 401,\n",
            " 'policy_reward_max': {},\n",
            " 'policy_reward_mean': {},\n",
            " 'policy_reward_min': {},\n",
            " 'sampler_perf': {'mean_action_processing_ms': 0.21318302839090772,\n",
            "                  'mean_env_render_ms': 0.0,\n",
            "                  'mean_env_wait_ms': 0.06347548760539169,\n",
            "                  'mean_inference_ms': 1.7587657250105364,\n",
            "                  'mean_raw_obs_processing_ms': 0.5280497184112107},\n",
            " 'sampler_results': {'connector_metrics': {'ObsPreprocessorConnector_ms': 0.011791706085205078,\n",
            "                                           'StateBufferConnector_ms': 0.0106353759765625,\n",
            "                                           'ViewRequirementAgentConnector_ms': 0.20078730583190918},\n",
            "                     'custom_metrics': {},\n",
            "                     'episode_len_mean': 168.0,\n",
            "                     'episode_media': {},\n",
            "                     'episode_reward_max': 1303.8000000000043,\n",
            "                     'episode_reward_mean': 99.64000000000011,\n",
            "                     'episode_reward_min': 6.6,\n",
            "                     'episodes_this_iter': 2,\n",
            "                     'hist_stats': {'episode_lengths': [113,\n",
            "                                                        75,\n",
            "                                                        21,\n",
            "                                                        33,\n",
            "                                                        39,\n",
            "                                                        35,\n",
            "                                                        65,\n",
            "                                                        25,\n",
            "                                                        31,\n",
            "                                                        65,\n",
            "                                                        151,\n",
            "                                                        79,\n",
            "                                                        35,\n",
            "                                                        47,\n",
            "                                                        109,\n",
            "                                                        185,\n",
            "                                                        43,\n",
            "                                                        17,\n",
            "                                                        87,\n",
            "                                                        39,\n",
            "                                                        79,\n",
            "                                                        33,\n",
            "                                                        31,\n",
            "                                                        35,\n",
            "                                                        59,\n",
            "                                                        99,\n",
            "                                                        29,\n",
            "                                                        29,\n",
            "                                                        41,\n",
            "                                                        61,\n",
            "                                                        59,\n",
            "                                                        71,\n",
            "                                                        65,\n",
            "                                                        23,\n",
            "                                                        25,\n",
            "                                                        35,\n",
            "                                                        29,\n",
            "                                                        69,\n",
            "                                                        35,\n",
            "                                                        49,\n",
            "                                                        27,\n",
            "                                                        35,\n",
            "                                                        29,\n",
            "                                                        61,\n",
            "                                                        27,\n",
            "                                                        11,\n",
            "                                                        43,\n",
            "                                                        55,\n",
            "                                                        37,\n",
            "                                                        71,\n",
            "                                                        37,\n",
            "                                                        227,\n",
            "                                                        47,\n",
            "                                                        363,\n",
            "                                                        117,\n",
            "                                                        475,\n",
            "                                                        237,\n",
            "                                                        159,\n",
            "                                                        155,\n",
            "                                                        245,\n",
            "                                                        105,\n",
            "                                                        167,\n",
            "                                                        75,\n",
            "                                                        67,\n",
            "                                                        103,\n",
            "                                                        385,\n",
            "                                                        159,\n",
            "                                                        795,\n",
            "                                                        47,\n",
            "                                                        31,\n",
            "                                                        365,\n",
            "                                                        371,\n",
            "                                                        49,\n",
            "                                                        139,\n",
            "                                                        373,\n",
            "                                                        201,\n",
            "                                                        275,\n",
            "                                                        115,\n",
            "                                                        267,\n",
            "                                                        167,\n",
            "                                                        37,\n",
            "                                                        325,\n",
            "                                                        59,\n",
            "                                                        97,\n",
            "                                                        105,\n",
            "                                                        69,\n",
            "                                                        75,\n",
            "                                                        201,\n",
            "                                                        219,\n",
            "                                                        243,\n",
            "                                                        167,\n",
            "                                                        101,\n",
            "                                                        2115,\n",
            "                                                        251,\n",
            "                                                        503,\n",
            "                                                        671,\n",
            "                                                        103,\n",
            "                                                        113,\n",
            "                                                        427,\n",
            "                                                        1955],\n",
            "                                    'episode_reward': [42.7,\n",
            "                                                       34.6,\n",
            "                                                       9.899999999999999,\n",
            "                                                       22.099999999999994,\n",
            "                                                       21.999999999999996,\n",
            "                                                       14.2,\n",
            "                                                       32.3,\n",
            "                                                       10.7,\n",
            "                                                       18.2,\n",
            "                                                       39.49999999999999,\n",
            "                                                       80.59999999999997,\n",
            "                                                       32.4,\n",
            "                                                       21.6,\n",
            "                                                       22.2,\n",
            "                                                       56.099999999999994,\n",
            "                                                       97.50000000000004,\n",
            "                                                       24.2,\n",
            "                                                       11.5,\n",
            "                                                       50.40000000000001,\n",
            "                                                       17.599999999999998,\n",
            "                                                       42.20000000000001,\n",
            "                                                       16.7,\n",
            "                                                       13.600000000000003,\n",
            "                                                       17.2,\n",
            "                                                       31.400000000000002,\n",
            "                                                       58.99999999999999,\n",
            "                                                       14.299999999999999,\n",
            "                                                       14.9,\n",
            "                                                       21.900000000000002,\n",
            "                                                       22.5,\n",
            "                                                       24.8,\n",
            "                                                       35.4,\n",
            "                                                       46.49999999999998,\n",
            "                                                       14.800000000000002,\n",
            "                                                       13.3,\n",
            "                                                       8.4,\n",
            "                                                       18.099999999999998,\n",
            "                                                       34.699999999999996,\n",
            "                                                       16.0,\n",
            "                                                       24.900000000000006,\n",
            "                                                       15.0,\n",
            "                                                       19.2,\n",
            "                                                       13.3,\n",
            "                                                       26.9,\n",
            "                                                       16.600000000000005,\n",
            "                                                       6.6,\n",
            "                                                       22.0,\n",
            "                                                       28.200000000000003,\n",
            "                                                       20.1,\n",
            "                                                       43.59999999999999,\n",
            "                                                       19.7,\n",
            "                                                       132.59999999999994,\n",
            "                                                       26.800000000000004,\n",
            "                                                       223.39999999999978,\n",
            "                                                       58.30000000000001,\n",
            "                                                       271.7999999999999,\n",
            "                                                       141.70000000000005,\n",
            "                                                       96.39999999999999,\n",
            "                                                       97.80000000000001,\n",
            "                                                       144.9,\n",
            "                                                       55.50000000000001,\n",
            "                                                       92.0,\n",
            "                                                       45.600000000000016,\n",
            "                                                       41.800000000000004,\n",
            "                                                       52.599999999999966,\n",
            "                                                       221.89999999999998,\n",
            "                                                       95.80000000000003,\n",
            "                                                       479.80000000000035,\n",
            "                                                       21.6,\n",
            "                                                       15.399999999999999,\n",
            "                                                       232.89999999999972,\n",
            "                                                       239.59999999999988,\n",
            "                                                       28.699999999999996,\n",
            "                                                       86.20000000000005,\n",
            "                                                       241.29999999999993,\n",
            "                                                       119.3,\n",
            "                                                       164.59999999999994,\n",
            "                                                       70.60000000000002,\n",
            "                                                       154.7999999999998,\n",
            "                                                       93.6,\n",
            "                                                       25.3,\n",
            "                                                       202.29999999999993,\n",
            "                                                       34.60000000000001,\n",
            "                                                       51.7,\n",
            "                                                       59.29999999999999,\n",
            "                                                       45.300000000000004,\n",
            "                                                       42.6,\n",
            "                                                       131.09999999999997,\n",
            "                                                       135.39999999999992,\n",
            "                                                       150.19999999999993,\n",
            "                                                       98.40000000000002,\n",
            "                                                       66.30000000000003,\n",
            "                                                       1303.8000000000043,\n",
            "                                                       144.19999999999996,\n",
            "                                                       301.1999999999996,\n",
            "                                                       421.8000000000002,\n",
            "                                                       58.80000000000002,\n",
            "                                                       65.69999999999999,\n",
            "                                                       271.3999999999996,\n",
            "                                                       1225.2000000000078]},\n",
            "                     'num_faulty_episodes': 0,\n",
            "                     'policy_reward_max': {},\n",
            "                     'policy_reward_mean': {},\n",
            "                     'policy_reward_min': {},\n",
            "                     'sampler_perf': {'mean_action_processing_ms': 0.21318302839090772,\n",
            "                                      'mean_env_render_ms': 0.0,\n",
            "                                      'mean_env_wait_ms': 0.06347548760539169,\n",
            "                                      'mean_inference_ms': 1.7587657250105364,\n",
            "                                      'mean_raw_obs_processing_ms': 0.5280497184112107}},\n",
            " 'time_since_restore': 101.0770320892334,\n",
            " 'time_this_iter_s': 12.838460683822632,\n",
            " 'time_total_s': 101.0770320892334,\n",
            " 'timers': {'learn_throughput': 817.371,\n",
            "            'learn_time_ms': 4893.74,\n",
            "            'load_throughput': 5279411.643,\n",
            "            'load_time_ms': 0.758,\n",
            "            'sample_time_ms': 9531.148,\n",
            "            'synch_weights_time_ms': 3.51,\n",
            "            'training_iteration_time_ms': 14430.753},\n",
            " 'timestamp': 1693749918,\n",
            " 'timesteps_total': 28000,\n",
            " 'training_iteration': 7,\n",
            " 'trial_id': 'default'}\n",
            "****************************************************************************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the whole algorithm\n",
        "\n",
        "path_to_checkpoint = algo.save()\n",
        "print(\n",
        "    \"An Algorithm checkpoint has been created inside directory:{} \".format(path_to_checkpoint)\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "#save the policy as a torch/tensorflow model\n",
        "#it depends on framework configuration if \"tf\" export .pb  if \"torch\" export .pt\n",
        "# insure the .checkpointing(export_native_model_files=True) is  set\n",
        "policy=algo.get_policy()\n",
        "policy.export_model(\"/content/policy_model\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#export as onnx format from tensorflow model .pp\n",
        "policy.export_model(\"/content/onnx\",onnx=15)\n",
        "\n",
        "\n",
        "\n",
        "#export as onnx format from torch model .pt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "oAkNYvFphIaI",
        "outputId": "c1988c41-541d-455d-b81c-bf0a57e6d60f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An Algorithm checkpoint has been created inside directory:/root/ray_results/PPO_my_2023-09-03_14-00-47jd1jf14o/checkpoint_000001 \n",
            "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-331f5c22797f>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#export as onnx format from torch model .pt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/onnx\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/policy/torch_policy_v2.py\u001b[0m in \u001b[0;36mexport_model\u001b[0;34m(self, export_dir, onnx)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m             \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m             torch.onnx.export(\n\u001b[0m\u001b[1;32m   1135\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mdummy_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \"\"\"\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m     _export(\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1618\u001b[0m                 )\n\u001b[1;32m   1619\u001b[0m             \u001b[0;31m# insert function_proto into model_proto.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m             proto = onnx_proto_utils._add_onnxscript_fn(\n\u001b[0m\u001b[1;32m   1621\u001b[0m                 \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m                 \u001b[0mcustom_opsets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/onnx_proto_utils.py\u001b[0m in \u001b[0;36m_add_onnxscript_fn\u001b[0;34m(model_bytes, custom_opsets)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;31m# serialization anyway in terms of the protobuf limitation. So we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;31m# need to worry about > 2GB model getting here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0mmodel_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;31m# Iterate graph nodes to insert only the included custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'onnx' has no attribute 'load_from_string'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train multi agnet"
      ],
      "metadata": {
        "id": "0f321a6T5heV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMultiAgentEnv(MultiAgentEnv):\n",
        "    def __init__(self):\n",
        "        self.num_agents = 2\n",
        "        self._agent_ids = [\"agent_0\", \"agent_1\"]\n",
        "        self.observation_space = spaces.Discrete(2)\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        self.initial_observations = {\"agent_0\": 0, \"agent_1\": 1}\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        info = {\"agent_0\": {}, \"agent_1\": {}}\n",
        "        return self.initial_observations, info\n",
        "\n",
        "    def step(self, action_dict):\n",
        "        obs = {}\n",
        "        rewards = {}\n",
        "        dones = {}\n",
        "        truncated = {}\n",
        "        info = {}\n",
        "\n",
        "        for key, value in action_dict.items():\n",
        "            obs[key] = random.randint(0, 1)\n",
        "            rewards[key] = 800\n",
        "            if obs[key] >= 1000:\n",
        "                dones[key] = True\n",
        "            else:\n",
        "                dones[key] = False\n",
        "            truncated[key] = False\n",
        "            info[key] = {}\n",
        "\n",
        "        dones[\"__all__\"] = False\n",
        "        truncated[\"__all__\"] = False\n",
        "\n",
        "        return obs, rewards, dones, truncated, info\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRRWMHfN5i9F",
        "outputId": "2d03abd2-0f9b-41e2-d34f-a18a59d3d034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tune.register_env(\"my\",lambda _: SimpleMultiAgentEnv())"
      ],
      "metadata": {
        "id": "wfIEk2Wt5s3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "config=PPOConfig()\n",
        "config.environment(\"my\")\n",
        "\n",
        "policies = { \"policy_1\":PolicySpec(observation_space =spaces.Discrete(15),action_space=spaces.Discrete(15)),\n",
        "            \"policy_2\" :PolicySpec(observation_space =spaces.Discrete(15),action_space=spaces.Discrete(15)),\n",
        "             }\n",
        "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
        "  return \"policy_1\" if agent_id == 'agent_0' else (\"policy_2\" if agent_id == 'agent_1' else \"policy_1\")\n",
        "\n",
        "config.multi_agent(policies=policies,policy_mapping_fn=policy_mapping_fn)\n",
        "config.rollouts(num_rollout_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29u2zW5y5n2d",
        "outputId": "ac5557aa-16f4-4167-9873-0835fd9b8f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-09-02 18:09:50,651\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x78ecb13db3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "algo=config.build()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpytB2ae6DJV",
        "outputId": "97b332ad-b87a-4f80-e8ce-29a1ad387069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-09-02 18:09:52,887\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "`UnifiedLogger` will be removed in Ray 2.7.\n",
            "  return UnifiedLogger(config, logdir, loggers=None)\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "\u001b[2m\u001b[36m(pid=7842)\u001b[0m /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(pid=7842)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[2m\u001b[36m(pid=7841)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=7841)\u001b[0m /usr/local/lib/python3.10/dist-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
            "\u001b[2m\u001b[36m(pid=7841)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "\u001b[2m\u001b[36m(pid=7841)\u001b[0m   pkg_resources.declare_namespace(__name__)\n",
            "\u001b[2m\u001b[36m(pid=7841)\u001b[0m /usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2349: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "\u001b[2m\u001b[36m(pid=7841)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "\u001b[2m\u001b[36m(pid=7841)\u001b[0m   declare_namespace(parent)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=7842)\u001b[0m 2023-09-02 18:10:07,556\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "2023-09-02 18:10:07,662\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=7841)\u001b[0m 2023-09-02 18:10:07,592\tWARNING env.py:298 -- Your MultiAgentEnv <SimpleMultiAgentEnv instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__()` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=7841)\u001b[0m 2023-09-02 18:10:07,616\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=7841)\u001b[0m 2023-09-02 18:10:07,616\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=7841)\u001b[0m 2023-09-02 18:10:07,616\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=7841)\u001b[0m 2023-09-02 18:10:07,616\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
            "2023-09-02 18:10:07,685\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
            "2023-09-02 18:10:07,689\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
            "2023-09-02 18:10:07,692\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
            "2023-09-02 18:10:07,693\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
            "2023-09-02 18:10:07,717\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "2023-09-02 18:10:07,877\tINFO trainable.py:172 -- Trainable.setup took 14.946 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2023-09-02 18:10:07,884\tWARNING util.py:68 -- Install gputil for GPU system monitoring.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(algo.train())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teZSHIV970uC",
        "outputId": "f9eee3d2-5a22-4943-aa24-600ebb05fcb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'__all__': {'num_agent_steps_trained': 256.0, 'num_env_steps_trained': 4000.0, 'total_loss': 19.981413662306537}, 'policy_1': {'total_loss': 19.981413662306537, 'policy_loss': -0.011692280072156887, 'vf_loss': 10.0, 'vf_loss_unclipped': 5924266123.735608, 'vf_explained_var': -0.32959409086689, 'entropy': 2.69141259198504, 'mean_kl_loss': 0.015209753852401593, 'curr_lr': 5e-05, 'curr_entropy_coeff': 0.0, 'curr_kl_coeff': 0.20000000298023224}, 'policy_2': {'total_loss': 9.990064018062437, 'policy_loss': -0.013269568848084095, 'vf_loss': 10.0, 'vf_loss_unclipped': 5924252253.61194, 'vf_explained_var': -0.2924032159197305, 'entropy': 2.6897527950404805, 'mean_kl_loss': 0.016668092678352207, 'curr_lr': 5e-05, 'curr_entropy_coeff': 0.0, 'curr_kl_coeff': 0.20000000298023224}}, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 8000, 'num_agent_steps_trained': 0}, 'sampler_results': {'episode_reward_max': nan, 'episode_reward_min': nan, 'episode_reward_mean': nan, 'episode_len_mean': nan, 'episode_media': {}, 'episodes_this_iter': 0, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [], 'episode_lengths': []}, 'sampler_perf': {}, 'num_faulty_episodes': 0, 'connector_metrics': {}}, 'episode_reward_max': nan, 'episode_reward_min': nan, 'episode_reward_mean': nan, 'episode_len_mean': nan, 'episodes_this_iter': 0, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [], 'episode_lengths': []}, 'sampler_perf': {}, 'num_faulty_episodes': 0, 'connector_metrics': {}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 8000, 'num_agent_steps_trained': 0, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 0, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 0, 'num_env_steps_sampled_throughput_per_sec': 91.7028958223203, 'num_env_steps_trained_throughput_per_sec': 0.0, 'timesteps_total': 4000, 'num_steps_trained_this_iter': 0, 'agent_timesteps_total': 8000, 'timers': {'training_iteration_time_ms': 43619.043, 'sample_time_ms': 22814.688, 'synch_weights_time_ms': 8.437}, 'counters': {'num_env_steps_sampled': 4000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 8000, 'num_agent_steps_trained': 0}, 'done': False, 'episodes_total': 0, 'training_iteration': 1, 'trial_id': 'default', 'date': '2023-09-02_18-10-54', 'timestamp': 1693678254, 'time_this_iter_s': 43.628775119781494, 'time_total_s': 43.628775119781494, 'pid': 6930, 'hostname': 'e577ce7262bd', 'node_ip': '172.28.0.12', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': 'my', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, '_is_atari': None, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': True, 'explore': True, 'exploration_config': {}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function policy_mapping_fn at 0x78edaea85bd0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': SingleAgentRLModuleSpec(module_class=<class 'ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module.PPOTorchRLModule'>, observation_space=None, action_space=None, model_config_dict=None, catalog_class=<class 'ray.rllib.algorithms.ppo.ppo_catalog.PPOCatalog'>, load_state_path=None), '_enable_rl_module_api': True, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'policy_1': (None, Discrete(15), Discrete(15), None), 'policy_2': (None, Discrete(15), Discrete(15), None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 43.628775119781494, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': 82.92537313432835, 'ram_util_percent': 22.516417910447764}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train unity\n"
      ],
      "metadata": {
        "id": "yEA2r3WifVbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Unity3DEnv(MultiAgentEnv):\n",
        "    \"\"\"A MultiAgentEnv representing a single Unity3D game instance.\n",
        "\n",
        "    For an example on how to use this Env with a running Unity3D editor\n",
        "    or with a compiled game, see:\n",
        "    `rllib/examples/unity3d_env_local.py`\n",
        "    For an example on how to use it inside a Unity game client, which\n",
        "    connects to an RLlib Policy server, see:\n",
        "    `rllib/examples/serving/unity3d_[client|server].py`\n",
        "\n",
        "    Supports all Unity3D (MLAgents) examples, multi- or single-agent and\n",
        "    gets converted automatically into an ExternalMultiAgentEnv, when used\n",
        "    inside an RLlib PolicyClient for cloud/distributed training of Unity games.\n",
        "    \"\"\"\n",
        "\n",
        "    # Default base port when connecting directly to the Editor\n",
        "    _BASE_PORT_EDITOR = 5004\n",
        "    # Default base port when connecting to a compiled environment\n",
        "    _BASE_PORT_ENVIRONMENT = 5005\n",
        "    # The worker_id for each environment instance\n",
        "    _WORKER_ID = 0\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        file_name: str = None,\n",
        "        port: Optional[int] = None,\n",
        "        seed: int = 0,\n",
        "        no_graphics: bool = False,\n",
        "        timeout_wait: int = 300,\n",
        "        episode_horizon: int = 1000,\n",
        "    ):\n",
        "        \"\"\"Initializes a Unity3DEnv object.\n",
        "\n",
        "        Args:\n",
        "            file_name (Optional[str]): Name of the Unity game binary.\n",
        "                If None, will assume a locally running Unity3D editor\n",
        "                to be used, instead.\n",
        "            port (Optional[int]): Port number to connect to Unity environment.\n",
        "            seed: A random seed value to use for the Unity3D game.\n",
        "            no_graphics: Whether to run the Unity3D simulator in\n",
        "                no-graphics mode. Default: False.\n",
        "            timeout_wait: Time (in seconds) to wait for connection from\n",
        "                the Unity3D instance.\n",
        "            episode_horizon: A hard horizon to abide to. After at most\n",
        "                this many steps (per-agent episode `step()` calls), the\n",
        "                Unity3D game is reset and will start again (finishing the\n",
        "                multi-agent episode that the game represents).\n",
        "                Note: The game itself may contain its own episode length\n",
        "                limits, which are always obeyed (on top of this value here).\n",
        "        \"\"\"\n",
        "        # Skip env checking as the nature of the agent IDs depends on the game\n",
        "        # running in the connected Unity editor.\n",
        "        self._skip_env_checking = True\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        if file_name is None:\n",
        "            print(\n",
        "                \"No game binary provided, will use a running Unity editor \"\n",
        "                \"instead.\\nMake sure you are pressing the Play (|>) button in \"\n",
        "                \"your editor to start.\"\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "        # Try connecting to the Unity3D game instance. If a port is blocked\n",
        "        port_ = None\n",
        "        while True:\n",
        "            # Sleep for random time to allow for concurrent startup of many\n",
        "            # environments (num_workers >> 1). Otherwise, would lead to port\n",
        "            # conflicts sometimes.\n",
        "            if port_ is not None:\n",
        "                time.sleep(random.randint(1, 10))\n",
        "            port_ = port or (\n",
        "                self._BASE_PORT_ENVIRONMENT if file_name else self._BASE_PORT_EDITOR\n",
        "            )\n",
        "            # cache the worker_id and\n",
        "            # increase it for the next environment\n",
        "            worker_id_ = Unity3DEnv._WORKER_ID if file_name else 0\n",
        "            Unity3DEnv._WORKER_ID += 1\n",
        "            try:\n",
        "                self.unity_env = UnityEnvironment(\n",
        "                    file_name=file_name,\n",
        "                    worker_id=worker_id_,\n",
        "                    base_port=port_,\n",
        "                    seed=seed,\n",
        "                    no_graphics=no_graphics,\n",
        "                    timeout_wait=timeout_wait,\n",
        "                )\n",
        "                print(\"Created UnityEnvironment for port {}\".format(port_ + worker_id_))\n",
        "            except mlagents_envs.exception.UnityWorkerInUseException:\n",
        "                pass\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # ML-Agents API version.\n",
        "        self.api_version = self.unity_env.API_VERSION.split(\".\")\n",
        "        self.api_version = [int(s) for s in self.api_version]\n",
        "\n",
        "        # Reset entire env every this number of step calls.\n",
        "        self.episode_horizon = episode_horizon\n",
        "        # Keep track of how many times we have called `step` so far.\n",
        "        self.episode_timesteps = 0\n",
        "\n",
        "    def step(\n",
        "        self, action_dict\n",
        "    ):\n",
        "        \"\"\"Performs one multi-agent step through the game.\n",
        "\n",
        "        Args:\n",
        "            action_dict: Multi-agent action dict with:\n",
        "                keys=agent identifier consisting of\n",
        "                [MLagents behavior name, e.g. \"Goalie?team=1\"] + \"_\" +\n",
        "                [Agent index, a unique MLAgent-assigned index per single agent]\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - obs: Multi-agent observation dict.\n",
        "                    Only those observations for which to get new actions are\n",
        "                    returned.\n",
        "                - rewards: Rewards dict matching `obs`.\n",
        "                - dones: Done dict with only an __all__ multi-agent entry in\n",
        "                    it. __all__=True, if episode is done for all agents.\n",
        "                - infos: An (empty) info dict.\n",
        "        \"\"\"\n",
        "\n",
        "        # Set only the required actions (from the DecisionSteps) in Unity3D.\n",
        "        all_agents = []\n",
        "        for behavior_name in self.unity_env.behavior_specs:\n",
        "            # New ML-Agents API: Set all agents actions at the same time\n",
        "            # via an ActionTuple. Since API v1.4.0.\n",
        "            if self.api_version[0] > 1 or (\n",
        "                self.api_version[0] == 1 and self.api_version[1] >= 4\n",
        "            ):\n",
        "                actions = []\n",
        "                for agent_id in self.unity_env.get_steps(behavior_name)[0].agent_id:\n",
        "                    key = behavior_name + \"_{}\".format(agent_id)\n",
        "                    all_agents.append(key)\n",
        "                    actions.append(action_dict[key])\n",
        "                if actions:\n",
        "                    if actions[0].dtype == np.float32:\n",
        "                        action_tuple = ActionTuple(continuous=np.array(actions))\n",
        "                    else:\n",
        "                        action_tuple = ActionTuple(discrete=np.array(actions))\n",
        "                    self.unity_env.set_actions(behavior_name, action_tuple)\n",
        "            # Old behavior: Do not use an ActionTuple and set each agent's\n",
        "            # action individually.\n",
        "            else:\n",
        "                for agent_id in self.unity_env.get_steps(behavior_name)[\n",
        "                    0\n",
        "                ].agent_id_to_index.keys():\n",
        "                    key = behavior_name + \"_{}\".format(agent_id)\n",
        "                    all_agents.append(key)\n",
        "                    self.unity_env.set_action_for_agent(\n",
        "                        behavior_name, agent_id, action_dict[key]\n",
        "                    )\n",
        "        # Do the step.\n",
        "        self.unity_env.step()\n",
        "\n",
        "        obs, rewards, terminateds, truncateds, infos = self._get_step_results()\n",
        "\n",
        "        # Global horizon reached? -> Return __all__ truncated=True, so user\n",
        "        # can reset. Set all agents' individual `truncated` to True as well.\n",
        "        self.episode_timesteps += 1\n",
        "        if self.episode_timesteps > self.episode_horizon:\n",
        "            return (\n",
        "                obs,\n",
        "                rewards,\n",
        "                terminateds,\n",
        "                dict({\"__all__\": True}, **{agent_id: True for agent_id in all_agents}),\n",
        "                infos,\n",
        "            )\n",
        "\n",
        "        return obs, rewards, terminateds, truncateds, infos\n",
        "\n",
        "    def reset(\n",
        "        self, *, seed=None, options=None\n",
        "    ):\n",
        "        \"\"\"Resets the entire Unity3D scene (a single multi-agent episode).\"\"\"\n",
        "        self.episode_timesteps = 0\n",
        "        self.unity_env.reset()\n",
        "        obs, _, _, _, infos = self._get_step_results()\n",
        "        return obs, infos\n",
        "\n",
        "    def _get_step_results(self):\n",
        "        \"\"\"Collects those agents' obs/rewards that have to act in next `step`.\n",
        "\n",
        "        Returns:\n",
        "            Tuple:\n",
        "                obs: Multi-agent observation dict.\n",
        "                    Only those observations for which to get new actions are\n",
        "                    returned.\n",
        "                rewards: Rewards dict matching `obs`.\n",
        "                dones: Done dict with only an __all__ multi-agent entry in it.\n",
        "                    __all__=True, if episode is done for all agents.\n",
        "                infos: An (empty) info dict.\n",
        "        \"\"\"\n",
        "        obs = {}\n",
        "        rewards = {}\n",
        "        infos = {}\n",
        "        for behavior_name in self.unity_env.behavior_specs:\n",
        "            decision_steps, terminal_steps = self.unity_env.get_steps(behavior_name)\n",
        "            # Important: Only update those sub-envs that are currently\n",
        "            # available within _env_state.\n",
        "            # Loop through all envs (\"agents\") and fill in, whatever\n",
        "            # information we have.\n",
        "            for agent_id, idx in decision_steps.agent_id_to_index.items():\n",
        "                key = behavior_name + \"_{}\".format(agent_id)\n",
        "                os = tuple(o[idx] for o in decision_steps.obs)\n",
        "                os = os[0] if len(os) == 1 else os\n",
        "                obs[key] = os\n",
        "                rewards[key] = (\n",
        "                    decision_steps.reward[idx] + decision_steps.group_reward[idx]\n",
        "                )\n",
        "            for agent_id, idx in terminal_steps.agent_id_to_index.items():\n",
        "                key = behavior_name + \"_{}\".format(agent_id)\n",
        "                # Only overwrite rewards (last reward in episode), b/c obs\n",
        "                # here is the last obs (which doesn't matter anyways).\n",
        "                # Unless key does not exist in obs.\n",
        "                if key not in obs:\n",
        "                    os = tuple(o[idx] for o in terminal_steps.obs)\n",
        "                    obs[key] = os = os[0] if len(os) == 1 else os\n",
        "                rewards[key] = (\n",
        "                    terminal_steps.reward[idx] + terminal_steps.group_reward[idx]\n",
        "                )\n",
        "\n",
        "        # Only use dones if all agents are done, then we should do a reset.\n",
        "        return obs, rewards, {\"__all__\": False}, {\"__all__\": False}, infos\n"
      ],
      "metadata": {
        "id": "2vzZAfjMbM5Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define funcction to make  myunity environemt\n",
        "tune.register_env(\"unity3d\",lambda _: Unity3DEnv(file_name=\"/content/RL_UNITY/dn_server/dn_linux.x86_64\"))"
      ],
      "metadata": {
        "id": "tNJjRIgLw-yY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policies = { \"policy_1\": PolicySpec(observation_space=spaces.Box(float(\"-inf\"), float(\"inf\"), (6,)),action_space=spaces.Box(float(\"-inf\"), float(\"inf\"), (2,))) }\n",
        "\n",
        "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
        "  return \"policy_1\"\n",
        "\n",
        "#set configs\n",
        "config=(\n",
        "    PPOConfig()\n",
        "    .environment(\"unity3d\")\n",
        "    .multi_agent(policies=policies,policy_mapping_fn=policy_mapping_fn)\n",
        "    .framework(\"torch\")\n",
        "    .rollouts(\n",
        "            num_rollout_workers=1,\n",
        "            rollout_fragment_length=200,\n",
        "            )\n",
        "    .training(\n",
        "            lr=0.0003,\n",
        "            lambda_=0.95,\n",
        "            gamma=0.99,\n",
        "            sgd_minibatch_size=256,\n",
        "            train_batch_size=4000,\n",
        "            num_sgd_iter=20,\n",
        "            clip_param=0.2,\n",
        "            model={\"fcnet_hiddens\": [512, 512]},\n",
        "            )\n",
        "    .resources(num_gpus=0)\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrQ45JnTPabH",
        "outputId": "8d972a22-cf9f-4455-cba0-f70da368c64b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-09-03 07:24:28,766\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "algo=config.build()"
      ],
      "metadata": {
        "id": "b-pFDaFxQor-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "488ad658-b13b-453c-8b11-3e7a2c845556"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-09-03 07:24:31,470\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "`UnifiedLogger` will be removed in Ray 2.7.\n",
            "  return UnifiedLogger(config, logdir, loggers=None)\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "2023-09-03 07:24:34,711\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "\u001b[2m\u001b[36m(pid=10252)\u001b[0m /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(pid=10252)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[2m\u001b[36m(pid=10252)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=10252)\u001b[0m /usr/local/lib/python3.10/dist-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
            "\u001b[2m\u001b[36m(pid=10252)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "\u001b[2m\u001b[36m(pid=10252)\u001b[0m   pkg_resources.declare_namespace(__name__)\n",
            "\u001b[2m\u001b[36m(pid=10252)\u001b[0m /usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2349: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "\u001b[2m\u001b[36m(pid=10252)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "\u001b[2m\u001b[36m(pid=10252)\u001b[0m   declare_namespace(parent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m [UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-bucket-allocator-granularity=16\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-bucket-allocator-bucket-count=8\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-bucket-allocator-block-size=4194304\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-bucket-allocator-block-count=1\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-main-allocator-block-size=16777216\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-thread-allocator-block-size=16777216\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-cache-allocator-block-size=4194304\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-typetree-allocator-block-size=2097152\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-profiler-allocator-block-size=16777216\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-temp-allocator-size-main=4194304\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     \"memorysetup-temp-allocator-size-gfx=262144\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m Loading in SingleInstance mode\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m Mono path[0] = '/content/RL_UNITY/dn_server/dn_linux_Data/Managed'\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m Mono config path = '/content/RL_UNITY/dn_server/dn_linux_Data/MonoBleedingEdge/etc'\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m Found 1 interfaces on host : 0) 172.28.0.12\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m Multi-casting \"[IP] 172.28.0.12 [Port] 55074 [Flags] 2 [Guid] 1105714286 [EditorId] 4135372844 [Version] 1048832 [Id] Unknown(43,172.28.0.12) [Debug] 0 [PackageName] Unknown [ProjectName] UnityEnvironment\" to [225.0.0.222:54997]...\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m Preloaded 'lib_burst_generated.so'\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m Preloaded 'libgrpc_csharp_ext.x64.so'\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m Initialize engine version: 2021.3.9f1 (ad3870b89536)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m [Subsystems] Discovering subsystems at path /content/RL_UNITY/dn_server/dn_linux_Data/UnitySubsystems\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m Forcing GfxDevice: Null\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m GfxDevice: creating device client; threaded=0; jobified=0\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m NullGfxDevice:\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     Version:  NULL 1.0 [1.0]\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     Renderer: Null Device\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m     Vendor:   Unity Technologies\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m Begin MonoManager ReloadAssembly\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m - Completed reload, in  0.248 seconds\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m There is no texture data available to upload.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnloadTime: 0.804701 ms\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m Registered Communicator in Agent.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Debug:Log (object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m Unity.MLAgents.Agent:Awake () (at D:/DARS/code/unity/ml/com.unity.ml-agents/Runtime/Agent.cs:373)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-09-03 07:24:43,854\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "2023-09-03 07:24:43,869\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
            "2023-09-03 07:24:43,870\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
            "2023-09-03 07:24:43,873\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
            "2023-09-03 07:24:43,875\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m /usr/local/lib/python3.10/dist-packages/mlagents_envs/environment.py:94: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m   unity_communicator_version = StrictVersion(unity_com_ver)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m 2023-09-03 07:24:43,822\tWARNING env.py:56 -- Skipping env checking for this experiment\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m 2023-09-03 07:24:43,825\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m 2023-09-03 07:24:43,836\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m 2023-09-03 07:24:43,836\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m 2023-09-03 07:24:43,836\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m 2023-09-03 07:24:43,836\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
            "2023-09-03 07:24:43,986\tINFO trainable.py:172 -- Trainable.setup took 12.482 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2023-09-03 07:24:43,991\tWARNING util.py:68 -- Install gputil for GPU system monitoring.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m Created UnityEnvironment for port 5005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result=algo.train()\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMo0eTg82ZIg",
        "outputId": "3bdca774-dcdf-4577-a002-abadd6f2280a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m colide\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Debug:Log (object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m MyAgent:OnCollisionEnter (UnityEngine.Collision) (at D:/DARS/code/unity/ml/Project/Assets/ML-Agents/Examples/dania/MyAgent.cs:42)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m \n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m colide\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Debug:Log (object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m MyAgent:OnCollisionEnter (UnityEngine.Collision) (at D:/DARS/code/unity/ml/Project/Assets/ML-Agents/Examples/dania/MyAgent.cs:42)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m \n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m colide\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Debug:Log (object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m MyAgent:OnCollisionEnter (UnityEngine.Collision) (at D:/DARS/code/unity/ml/Project/Assets/ML-Agents/Examples/dania/MyAgent.cs:42)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m \n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m colide\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Debug:Log (object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m MyAgent:OnCollisionEnter (UnityEngine.Collision) (at D:/DARS/code/unity/ml/Project/Assets/ML-Agents/Examples/dania/MyAgent.cs:42)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m \n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m colide\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Debug:Log (object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m MyAgent:OnCollisionEnter (UnityEngine.Collision) (at D:/DARS/code/unity/ml/Project/Assets/ML-Agents/Examples/dania/MyAgent.cs:42)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m \n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m colide\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Debug:Log (object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m MyAgent:OnCollisionEnter (UnityEngine.Collision) (at D:/DARS/code/unity/ml/Project/Assets/ML-Agents/Examples/dania/MyAgent.cs:42)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m \n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m colide\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Debug:Log (object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m MyAgent:OnCollisionEnter (UnityEngine.Collision) (at D:/DARS/code/unity/ml/Project/Assets/ML-Agents/Examples/dania/MyAgent.cs:42)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m \n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m colide\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Debug:Log (object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m MyAgent:OnCollisionEnter (UnityEngine.Collision) (at D:/DARS/code/unity/ml/Project/Assets/ML-Agents/Examples/dania/MyAgent.cs:42)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m \n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m colide\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Debug:Log (object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m MyAgent:OnCollisionEnter (UnityEngine.Collision) (at D:/DARS/code/unity/ml/Project/Assets/ML-Agents/Examples/dania/MyAgent.cs:42)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m \n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m colide\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Debug:Log (object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m MyAgent:OnCollisionEnter (UnityEngine.Collision) (at D:/DARS/code/unity/ml/Project/Assets/ML-Agents/Examples/dania/MyAgent.cs:42)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m \n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m colide\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Debug:Log (object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m MyAgent:OnCollisionEnter (UnityEngine.Collision) (at D:/DARS/code/unity/ml/Project/Assets/ML-Agents/Examples/dania/MyAgent.cs:42)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m \n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m colide\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Debug:Log (object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m MyAgent:OnCollisionEnter (UnityEngine.Collision) (at D:/DARS/code/unity/ml/Project/Assets/ML-Agents/Examples/dania/MyAgent.cs:42)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m \n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m colide\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Debug:Log (object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m MyAgent:OnCollisionEnter (UnityEngine.Collision) (at D:/DARS/code/unity/ml/Project/Assets/ML-Agents/Examples/dania/MyAgent.cs:42)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m \n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m colide\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Debug:Log (object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m MyAgent:OnCollisionEnter (UnityEngine.Collision) (at D:/DARS/code/unity/ml/Project/Assets/ML-Agents/Examples/dania/MyAgent.cs:42)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m \n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m colide\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m UnityEngine.Debug:Log (object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m MyAgent:OnCollisionEnter (UnityEngine.Collision) (at D:/DARS/code/unity/ml/Project/Assets/ML-Agents/Examples/dania/MyAgent.cs:42)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=10252)\u001b[0m \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'custom_metrics': {},\n",
              " 'episode_media': {},\n",
              " 'info': {'learner': {'__all__': {'num_agent_steps_trained': 256.0,\n",
              "    'num_env_steps_trained': 4000.0,\n",
              "    'total_loss': 0.06186205271435098},\n",
              "   'policy_1': {'total_loss': 0.06186205271435098,\n",
              "    'policy_loss': 0.002576873893626391,\n",
              "    'vf_loss': 0.04846617842938144,\n",
              "    'vf_loss_unclipped': 0.04846617842938144,\n",
              "    'vf_explained_var': -0.10770912787403923,\n",
              "    'entropy': 2.822500002650788,\n",
              "    'mean_kl_loss': 0.054095001894230915,\n",
              "    'curr_lr': 0.0003,\n",
              "    'curr_entropy_coeff': 0.0,\n",
              "    'curr_kl_coeff': 0.30000001192092896}},\n",
              "  'num_env_steps_sampled': 4000,\n",
              "  'num_env_steps_trained': 0,\n",
              "  'num_agent_steps_sampled': 4000,\n",
              "  'num_agent_steps_trained': 0},\n",
              " 'sampler_results': {'episode_reward_max': 6.0,\n",
              "  'episode_reward_min': 3.0,\n",
              "  'episode_reward_mean': 4.0,\n",
              "  'episode_len_mean': 1001.0,\n",
              "  'episode_media': {},\n",
              "  'episodes_this_iter': 3,\n",
              "  'policy_reward_min': {'policy_1': 3.0},\n",
              "  'policy_reward_max': {'policy_1': 6.0},\n",
              "  'policy_reward_mean': {'policy_1': 4.0},\n",
              "  'custom_metrics': {},\n",
              "  'hist_stats': {'episode_reward': [3.0, 3.0, 6.0],\n",
              "   'episode_lengths': [1001, 1001, 1001],\n",
              "   'policy_policy_1_reward': [3.0, 3.0, 6.0]},\n",
              "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.6888573600542124,\n",
              "   'mean_inference_ms': 1.9958178957114665,\n",
              "   'mean_action_processing_ms': 0.18115444083238832,\n",
              "   'mean_env_wait_ms': 95.39187696390645,\n",
              "   'mean_env_render_ms': 0.0},\n",
              "  'num_faulty_episodes': 0,\n",
              "  'connector_metrics': {'ObsPreprocessorConnector_ms': 0.016657511393229168,\n",
              "   'StateBufferConnector_ms': 0.005054473876953125,\n",
              "   'ViewRequirementAgentConnector_ms': 0.33673445383707684}},\n",
              " 'episode_reward_max': 6.0,\n",
              " 'episode_reward_min': 3.0,\n",
              " 'episode_reward_mean': 4.0,\n",
              " 'episode_len_mean': 1001.0,\n",
              " 'episodes_this_iter': 3,\n",
              " 'policy_reward_min': {'policy_1': 3.0},\n",
              " 'policy_reward_max': {'policy_1': 6.0},\n",
              " 'policy_reward_mean': {'policy_1': 4.0},\n",
              " 'hist_stats': {'episode_reward': [3.0, 3.0, 6.0],\n",
              "  'episode_lengths': [1001, 1001, 1001],\n",
              "  'policy_policy_1_reward': [3.0, 3.0, 6.0]},\n",
              " 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6888573600542124,\n",
              "  'mean_inference_ms': 1.9958178957114665,\n",
              "  'mean_action_processing_ms': 0.18115444083238832,\n",
              "  'mean_env_wait_ms': 95.39187696390645,\n",
              "  'mean_env_render_ms': 0.0},\n",
              " 'num_faulty_episodes': 0,\n",
              " 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.016657511393229168,\n",
              "  'StateBufferConnector_ms': 0.005054473876953125,\n",
              "  'ViewRequirementAgentConnector_ms': 0.33673445383707684},\n",
              " 'num_healthy_workers': 1,\n",
              " 'num_in_flight_async_reqs': 0,\n",
              " 'num_remote_worker_restarts': 0,\n",
              " 'num_agent_steps_sampled': 4000,\n",
              " 'num_agent_steps_trained': 0,\n",
              " 'num_env_steps_sampled': 4000,\n",
              " 'num_env_steps_trained': 0,\n",
              " 'num_env_steps_sampled_this_iter': 4000,\n",
              " 'num_env_steps_trained_this_iter': 0,\n",
              " 'num_env_steps_sampled_throughput_per_sec': 10.002235512886628,\n",
              " 'num_env_steps_trained_throughput_per_sec': 0.0,\n",
              " 'timesteps_total': 4000,\n",
              " 'num_steps_trained_this_iter': 0,\n",
              " 'agent_timesteps_total': 4000,\n",
              " 'timers': {'training_iteration_time_ms': 399910.544,\n",
              "  'sample_time_ms': 393313.324,\n",
              "  'synch_weights_time_ms': 4.638},\n",
              " 'counters': {'num_env_steps_sampled': 4000,\n",
              "  'num_env_steps_trained': 0,\n",
              "  'num_agent_steps_sampled': 4000,\n",
              "  'num_agent_steps_trained': 0},\n",
              " 'done': False,\n",
              " 'episodes_total': 3,\n",
              " 'training_iteration': 1,\n",
              " 'trial_id': 'default',\n",
              " 'date': '2023-09-03_07-31-41',\n",
              " 'timestamp': 1693726301,\n",
              " 'time_this_iter_s': 399.9142119884491,\n",
              " 'time_total_s': 399.9142119884491,\n",
              " 'pid': 9784,\n",
              " 'hostname': '29cbaeae0804',\n",
              " 'node_ip': '172.28.0.12',\n",
              " 'config': {'extra_python_environs_for_driver': {},\n",
              "  'extra_python_environs_for_worker': {},\n",
              "  'num_gpus': 0,\n",
              "  'num_cpus_per_worker': 1,\n",
              "  'num_gpus_per_worker': 0,\n",
              "  '_fake_gpus': False,\n",
              "  'num_learner_workers': 0,\n",
              "  'num_gpus_per_learner_worker': 0,\n",
              "  'num_cpus_per_learner_worker': 1,\n",
              "  'local_gpu_idx': 0,\n",
              "  'custom_resources_per_worker': {},\n",
              "  'placement_strategy': 'PACK',\n",
              "  'eager_tracing': True,\n",
              "  'eager_max_retraces': 20,\n",
              "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
              "   'inter_op_parallelism_threads': 2,\n",
              "   'gpu_options': {'allow_growth': True},\n",
              "   'log_device_placement': False,\n",
              "   'device_count': {'CPU': 1},\n",
              "   'allow_soft_placement': True},\n",
              "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
              "   'inter_op_parallelism_threads': 8},\n",
              "  'torch_compile_learner': False,\n",
              "  'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
              "  'torch_compile_learner_dynamo_backend': 'inductor',\n",
              "  'torch_compile_learner_dynamo_mode': None,\n",
              "  'torch_compile_worker': False,\n",
              "  'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
              "  'torch_compile_worker_dynamo_mode': None,\n",
              "  'env': 'unity3d',\n",
              "  'env_config': {},\n",
              "  'observation_space': None,\n",
              "  'action_space': None,\n",
              "  'env_task_fn': None,\n",
              "  'render_env': False,\n",
              "  'clip_rewards': None,\n",
              "  'normalize_actions': True,\n",
              "  'clip_actions': False,\n",
              "  'disable_env_checking': False,\n",
              "  '_is_atari': None,\n",
              "  'auto_wrap_old_gym_envs': True,\n",
              "  'action_mask_key': 'action_mask',\n",
              "  'env_runner_cls': None,\n",
              "  'num_envs_per_worker': 1,\n",
              "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "  'sample_async': False,\n",
              "  'enable_connectors': True,\n",
              "  'update_worker_filter_stats': True,\n",
              "  'use_worker_filter_stats': True,\n",
              "  'rollout_fragment_length': 200,\n",
              "  'batch_mode': 'truncate_episodes',\n",
              "  'remote_worker_envs': False,\n",
              "  'remote_env_batch_wait_ms': 0,\n",
              "  'validate_workers_after_construction': True,\n",
              "  'preprocessor_pref': 'deepmind',\n",
              "  'observation_filter': 'NoFilter',\n",
              "  'compress_observations': False,\n",
              "  'enable_tf1_exec_eagerly': False,\n",
              "  'sampler_perf_stats_ema_coef': None,\n",
              "  'gamma': 0.99,\n",
              "  'lr': 0.0003,\n",
              "  'grad_clip': None,\n",
              "  'grad_clip_by': 'global_norm',\n",
              "  'train_batch_size': 4000,\n",
              "  'model': {'_disable_preprocessor_api': False,\n",
              "   '_disable_action_flattening': False,\n",
              "   'fcnet_hiddens': [512, 512],\n",
              "   'fcnet_activation': 'tanh',\n",
              "   'conv_filters': None,\n",
              "   'conv_activation': 'relu',\n",
              "   'post_fcnet_hiddens': [],\n",
              "   'post_fcnet_activation': 'relu',\n",
              "   'free_log_std': False,\n",
              "   'no_final_linear': False,\n",
              "   'vf_share_layers': False,\n",
              "   'use_lstm': False,\n",
              "   'max_seq_len': 20,\n",
              "   'lstm_cell_size': 256,\n",
              "   'lstm_use_prev_action': False,\n",
              "   'lstm_use_prev_reward': False,\n",
              "   '_time_major': False,\n",
              "   'use_attention': False,\n",
              "   'attention_num_transformer_units': 1,\n",
              "   'attention_dim': 64,\n",
              "   'attention_num_heads': 1,\n",
              "   'attention_head_dim': 32,\n",
              "   'attention_memory_inference': 50,\n",
              "   'attention_memory_training': 50,\n",
              "   'attention_position_wise_mlp_dim': 32,\n",
              "   'attention_init_gru_gate_bias': 2.0,\n",
              "   'attention_use_n_prev_actions': 0,\n",
              "   'attention_use_n_prev_rewards': 0,\n",
              "   'framestack': True,\n",
              "   'dim': 84,\n",
              "   'grayscale': False,\n",
              "   'zero_mean': True,\n",
              "   'custom_model': None,\n",
              "   'custom_model_config': {},\n",
              "   'custom_action_dist': None,\n",
              "   'custom_preprocessor': None,\n",
              "   'encoder_latent_dim': None,\n",
              "   'always_check_shapes': False,\n",
              "   'lstm_use_prev_action_reward': -1,\n",
              "   '_use_default_native_models': -1},\n",
              "  'optimizer': {},\n",
              "  'max_requests_in_flight_per_sampler_worker': 2,\n",
              "  '_learner_class': None,\n",
              "  '_enable_learner_api': True,\n",
              "  'explore': True,\n",
              "  'exploration_config': {},\n",
              "  'algorithm_config_overrides_per_module': {},\n",
              "  'policy_map_capacity': 100,\n",
              "  'policy_mapping_fn': <function __main__.policy_mapping_fn(agent_id, episode, worker, **kwargs)>,\n",
              "  'policies_to_train': None,\n",
              "  'policy_states_are_swappable': False,\n",
              "  'observation_fn': None,\n",
              "  'count_steps_by': 'env_steps',\n",
              "  'input_config': {},\n",
              "  'actions_in_input_normalized': False,\n",
              "  'postprocess_inputs': False,\n",
              "  'shuffle_buffer_size': 0,\n",
              "  'output': None,\n",
              "  'output_config': {},\n",
              "  'output_compress_columns': ['obs', 'new_obs'],\n",
              "  'output_max_file_size': 67108864,\n",
              "  'offline_sampling': False,\n",
              "  'evaluation_interval': None,\n",
              "  'evaluation_duration': 10,\n",
              "  'evaluation_duration_unit': 'episodes',\n",
              "  'evaluation_sample_timeout_s': 180.0,\n",
              "  'evaluation_parallel_to_training': False,\n",
              "  'evaluation_config': None,\n",
              "  'off_policy_estimation_methods': {},\n",
              "  'ope_split_batch_by_episode': True,\n",
              "  'evaluation_num_workers': 0,\n",
              "  'always_attach_evaluation_results': False,\n",
              "  'enable_async_evaluation': False,\n",
              "  'in_evaluation': False,\n",
              "  'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
              "  'keep_per_episode_custom_metrics': False,\n",
              "  'metrics_episode_collection_timeout_s': 60.0,\n",
              "  'metrics_num_episodes_for_smoothing': 100,\n",
              "  'min_time_s_per_iteration': None,\n",
              "  'min_train_timesteps_per_iteration': 0,\n",
              "  'min_sample_timesteps_per_iteration': 0,\n",
              "  'export_native_model_files': False,\n",
              "  'checkpoint_trainable_policies_only': False,\n",
              "  'logger_creator': None,\n",
              "  'logger_config': None,\n",
              "  'log_level': 'WARN',\n",
              "  'log_sys_usage': True,\n",
              "  'fake_sampler': False,\n",
              "  'seed': None,\n",
              "  'ignore_worker_failures': False,\n",
              "  'recreate_failed_workers': False,\n",
              "  'max_num_worker_restarts': 1000,\n",
              "  'delay_between_worker_restarts_s': 60.0,\n",
              "  'restart_failed_sub_environments': False,\n",
              "  'num_consecutive_worker_failures_tolerance': 100,\n",
              "  'worker_health_probe_timeout_s': 60,\n",
              "  'worker_restore_timeout_s': 1800,\n",
              "  'rl_module_spec': SingleAgentRLModuleSpec(module_class=<class 'ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module.PPOTorchRLModule'>, observation_space=None, action_space=None, model_config_dict=None, catalog_class=<class 'ray.rllib.algorithms.ppo.ppo_catalog.PPOCatalog'>, load_state_path=None),\n",
              "  '_enable_rl_module_api': True,\n",
              "  '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'},\n",
              "  '_tf_policy_handles_more_than_one_loss': False,\n",
              "  '_disable_preprocessor_api': False,\n",
              "  '_disable_action_flattening': False,\n",
              "  '_disable_execution_plan_api': True,\n",
              "  '_disable_initialize_loss_from_dummy_batch': False,\n",
              "  'simple_optimizer': True,\n",
              "  'policy_map_cache': -1,\n",
              "  'worker_cls': -1,\n",
              "  'synchronize_filters': -1,\n",
              "  'replay_sequence_length': None,\n",
              "  'lr_schedule': None,\n",
              "  'use_critic': True,\n",
              "  'use_gae': True,\n",
              "  'use_kl_loss': True,\n",
              "  'kl_coeff': 0.2,\n",
              "  'kl_target': 0.01,\n",
              "  'sgd_minibatch_size': 256,\n",
              "  'num_sgd_iter': 20,\n",
              "  'shuffle_sequences': True,\n",
              "  'vf_loss_coeff': 1.0,\n",
              "  'entropy_coeff': 0.0,\n",
              "  'entropy_coeff_schedule': None,\n",
              "  'clip_param': 0.2,\n",
              "  'vf_clip_param': 10.0,\n",
              "  'vf_share_layers': -1,\n",
              "  'lambda': 0.95,\n",
              "  'input': 'sampler',\n",
              "  'policies': {'policy_1': (None,\n",
              "    Box(-inf, inf, (6,), float32),\n",
              "    Box(-inf, inf, (2,), float32),\n",
              "    None)},\n",
              "  'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
              "  'create_env_on_driver': False,\n",
              "  'custom_eval_function': None,\n",
              "  'framework': 'torch',\n",
              "  'num_cpus_for_driver': 1,\n",
              "  'num_workers': 1},\n",
              " 'time_since_restore': 399.9142119884491,\n",
              " 'iterations_since_restore': 1,\n",
              " 'perf': {'cpu_util_percent': 64.39664429530201,\n",
              "  'ram_util_percent': 18.695973154362417}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test"
      ],
      "metadata": {
        "id": "uE0mBW8irHY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ray.rllib.env.wrappers.unity3d_env import Unity3DEnv\n",
        "\n",
        "\n",
        "tune.register_env(\n",
        "        \"unity3d\",\n",
        "        lambda c: Unity3DEnv(\n",
        "            file_name=c[\"file_name\"],\n",
        "            no_graphics=False,\n",
        "            episode_horizon=c[\"episode_horizon\"],\n",
        "        ),\n",
        "    )\n"
      ],
      "metadata": {
        "id": "W0nGY_UxpENQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "policies = { \"policy_1\": PolicySpec(observation_space=spaces.Box(float(\"-inf\"), float(\"inf\"), (8,)),action_space=spaces.Box(-1.0, 1.0, (2,), dtype=np.float32)) }\n",
        "\n",
        "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
        "  return \"policy_1\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "config = (\n",
        "        PPOConfig()\n",
        "        .environment(\n",
        "            \"unity3d\",\n",
        "            env_config={\n",
        "                \"file_name\":\"/content/RL_UNITY/3dball/dn_linux.x86_64\",\n",
        "                \"episode_horizon\": 3000,\n",
        "            },\n",
        "        )\n",
        "        .framework(\"torch\")\n",
        "        .rollouts(\n",
        "            num_rollout_workers=1,\n",
        "            rollout_fragment_length=200,\n",
        "        )\n",
        "        .training(\n",
        "            lr=0.0003,\n",
        "            lambda_=0.95,\n",
        "            gamma=0.99,\n",
        "            sgd_minibatch_size=256,\n",
        "            train_batch_size=4000,\n",
        "            num_sgd_iter=20,\n",
        "            clip_param=0.2,\n",
        "            model={\"fcnet_hiddens\": [512, 512]},\n",
        "        )\n",
        "        .multi_agent(policies=policies, policy_mapping_fn=policy_mapping_fn)\n",
        "        .resources(num_gpus=0)\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko_5goC5pavP",
        "outputId": "92935375-4907-4f00-8406-2432c0d5d931"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-09-03 06:37:15,122\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "algo=config.build()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8PToWAKsM-o",
        "outputId": "80c9d5b2-cd7a-4c08-b10b-726ea6796446"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-09-03 06:23:48,229\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "`UnifiedLogger` will be removed in Ray 2.7.\n",
            "  return UnifiedLogger(config, logdir, loggers=None)\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "2023-09-03 06:23:51,173\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "\u001b[2m\u001b[36m(pid=1802)\u001b[0m /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(pid=1802)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[2m\u001b[36m(pid=1802)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=1802)\u001b[0m /usr/local/lib/python3.10/dist-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
            "\u001b[2m\u001b[36m(pid=1802)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "\u001b[2m\u001b[36m(pid=1802)\u001b[0m   pkg_resources.declare_namespace(__name__)\n",
            "\u001b[2m\u001b[36m(pid=1802)\u001b[0m /usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2349: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "\u001b[2m\u001b[36m(pid=1802)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "\u001b[2m\u001b[36m(pid=1802)\u001b[0m   declare_namespace(parent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m [UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-bucket-allocator-granularity=16\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-bucket-allocator-bucket-count=8\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-bucket-allocator-block-size=4194304\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-bucket-allocator-block-count=1\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-main-allocator-block-size=16777216\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-thread-allocator-block-size=16777216\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-cache-allocator-block-size=4194304\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-typetree-allocator-block-size=2097152\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-profiler-allocator-block-size=16777216\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-temp-allocator-size-main=4194304\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     \"memorysetup-temp-allocator-size-gfx=262144\"\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m Loading in SingleInstance mode\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m Mono path[0] = '/content/RL_UNITY/3dball/dn_linux_Data/Managed'\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m Mono config path = '/content/RL_UNITY/3dball/dn_linux_Data/MonoBleedingEdge/etc'\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m Found 1 interfaces on host : 0) 172.28.0.12\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m Multi-casting \"[IP] 172.28.0.12 [Port] 55187 [Flags] 2 [Guid] 2577484097 [EditorId] 3376362526 [Version] 1048832 [Id] Unknown(43,172.28.0.12) [Debug] 0 [PackageName] Unknown [ProjectName] UnityEnvironment\" to [225.0.0.222:54997]...\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m Preloaded 'lib_burst_generated.so'\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m Preloaded 'libgrpc_csharp_ext.x64.so'\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m PlayerPrefs - Creating folder: /root/.config/unity3d/Unity Technologies\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m PlayerPrefs - Creating folder: /root/.config/unity3d/Unity Technologies/UnityEnvironment\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m Unable to load player prefs\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m Initialize engine version: 2021.3.9f1 (ad3870b89536)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m [Subsystems] Discovering subsystems at path /content/RL_UNITY/3dball/dn_linux_Data/UnitySubsystems\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m Forcing GfxDevice: Null\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m GfxDevice: creating device client; threaded=0; jobified=0\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m NullGfxDevice:\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     Version:  NULL 1.0 [1.0]\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     Renderer: Null Device\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m     Vendor:   Unity Technologies\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m Begin MonoManager ReloadAssembly\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m - Completed reload, in  0.123 seconds\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m There is no texture data available to upload.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m UnloadTime: 0.728900 ms\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m Registered Communicator in Agent.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m UnityEngine.StackTraceUtility:ExtractStackTrace () (at /home/bokken/buildslave/unity/build/Runtime/Export/Scripting/StackTrace.cs:37)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m UnityEngine.DebugLogHandler:LogFormat (UnityEngine.LogType,UnityEngine.Object,string,object[])\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m UnityEngine.Logger:Log (UnityEngine.LogType,object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m UnityEngine.Debug:Log (object)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m Unity.MLAgents.Agent:Awake () (at D:/DARS/code/unity/ml/com.unity.ml-agents/Runtime/Agent.cs:373)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m \n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m Created UnityEnvironment for port 5005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m /usr/local/lib/python3.10/dist-packages/mlagents_envs/environment.py:94: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m   unity_communicator_version = StrictVersion(unity_com_ver)\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m 2023-09-03 06:24:04,393\tWARNING env.py:56 -- Skipping env checking for this experiment\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m 2023-09-03 06:24:04,401\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m 2023-09-03 06:24:04,456\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m 2023-09-03 06:24:04,456\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m 2023-09-03 06:24:04,456\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1802)\u001b[0m 2023-09-03 06:24:04,456\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
            "2023-09-03 06:24:04,605\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
            "2023-09-03 06:24:04,629\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
            "2023-09-03 06:24:04,631\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
            "2023-09-03 06:24:04,633\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
            "2023-09-03 06:24:04,635\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
            "2023-09-03 06:24:04,808\tINFO trainable.py:172 -- Trainable.setup took 16.547 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2023-09-03 06:24:04,812\tWARNING util.py:68 -- Install gputil for GPU system monitoring.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Gr_kYEJIfRwc",
        "0f321a6T5heV",
        "yEA2r3WifVbl",
        "uE0mBW8irHY9"
      ],
      "authorship_tag": "ABX9TyOQ4GN7BrsQasGrbDcrc4mC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}